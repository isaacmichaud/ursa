{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Universal Research and Scientific Agent (URSA)","text":"<p>The flexible agentic workflow for accelerating scientific tasks. Composes information flow between agents for planning, code writing and execution, and online research to solve complex problems.</p>"},{"location":"Metrics-Guide/","title":"URSA Metrics CLI \u2014 Plotting &amp; Aggregation Guide","text":"<p>This guide covers how to use <code>metrics_cli.py</code> to generate per-run, per-thread, and cross-thread (SUPER) charts from Telemetry JSON. It includes the new model- and agent-level aggregations and the interactive timeline.</p>"},{"location":"Metrics-Guide/#quickstart","title":"Quickstart","text":"<pre><code># Generate thread-level + per-run charts for every thread under a directory\npython ursa/scripts/metrics_cli.py --dir /path/to/metrics --chart all\n</code></pre> <pre><code># Walk all subdirectories, run `all` in each, then build a SUPER rollup at the root\npython ursa/scripts/metrics_cli.py --dir /path/to/workspaces --chart all-recursive\n</code></pre> <pre><code># Single JSON: make a time lollipop, token totals, KDE, or tokens/sec\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart lollipop\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart tokens-bar\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart tokens-kde\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart tokens-rate\n</code></pre>"},{"location":"Metrics-Guide/#what-the-cli-reads-what-it-makes","title":"What the CLI reads &amp; what it makes","text":"<p>Input (per file): - <code>context.agent</code>, <code>context.thread_id</code>, <code>context.run_id</code>, <code>context.started_at</code>, <code>context.ended_at</code> - <code>tables.llm[]</code>, <code>tables.tool[]</code>, <code>tables.runnable[]</code> - <code>llm_events[][].metrics.usage_rollup</code> (for token counts &amp; samples)</p> <p>Outputs (PNG/HTML), depending on mode: - Time lollipop: <code>*_lollipop.png</code> (or <code>thread_&lt;id&gt;_lollipop.png</code>, <code>super_lollipop.png</code>) - Token totals bar: <code>*_tokens_bar.png</code> - Token KDE overlay: <code>*_tokens_kde.png</code> - Tokens per second (two baselines): <code>*_tokens_rate.png</code> - Interactive timeline: <code>thread_&lt;id&gt;_timeline.html</code> - SUPER by-model: <code>super_tokens_bar_by_model.png</code>, <code>super_tokens_rate_by_model.png</code> - Agent rollups (thread or SUPER):   - tokens: <code>thread_&lt;id&gt;_agents_tokens.png</code>, <code>super_agents_tokens.png</code>   - tokens/sec: <code>thread_&lt;id&gt;_agents_tps.png</code>, <code>super_agents_tps.png</code></p> <p>Filenames for per-run charts are derived from the JSON path: <code>path/to/run.json</code> \u2192 <code>path/to/run_breakdown_&lt;chart&gt;.png</code>.</p>"},{"location":"Metrics-Guide/#modes","title":"Modes","text":""},{"location":"Metrics-Guide/#1-single-json-targeted-charts","title":"1) Single JSON (targeted charts)","text":"<pre><code>python ursa/scripts/metrics_cli.py path/to/run.json --chart lollipop\npython ursa/scripts/metrics_cli.py path/to/run.json --chart tokens-bar\npython ursa/scripts/metrics_cli.py path/to/run.json --chart tokens-kde\npython ursa/scripts/metrics_cli.py path/to/run.json --chart tokens-rate\n</code></pre> <p>Use <code>--title</code> and <code>--out</code> to customize:</p> <pre><code>python ursa/scripts/metrics_cli.py run.json \\\n  --chart tokens-rate \\\n  --title \"Model TPS (build #814)\" \\\n  --out out/my_rate.png\n</code></pre>"},{"location":"Metrics-Guide/#2-thread-level-aggregate-all-runs-of-one-thread_id","title":"2) Thread-level (aggregate all runs of one <code>thread_id</code>)","text":"<p>List threads the CLI can see in a directory:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /metrics --list-threads\n</code></pre> <p>Generate a specific thread\u2019s charts:</p> <pre><code># Time lollipop\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-lollipop --thread &lt;thread_id&gt;\n\n# Token totals / KDE / TPS\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-tokens-bar  --thread &lt;thread_id&gt;\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-tokens-kde  --thread &lt;thread_id&gt; --log-x\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-tokens-rate --thread &lt;thread_id&gt;\n\n# Interactive timeline (HTML); y-axis grouped by agent (default) or one row per run\npython ursa/scripts/metrics_cli.py --dir /metrics --chart timeline-html --thread &lt;thread_id&gt; --group-by agent\npython ursa/scripts/metrics_cli.py --dir /metrics --chart timeline-html --thread &lt;thread_id&gt; --group-by run\n</code></pre> <p>Agent rollups for a thread:</p> <pre><code># Tokens stacked by agent\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-agents-tokens --thread &lt;thread_id&gt;\n\n# Tokens/sec by agent (two baselines)\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-agents-tps --thread &lt;thread_id&gt;\n</code></pre> <p>Thread-level TPS uses two denominators: - per LLM-sec (sum): sum of <code>tables.llm[].total_s</code> across all runs in the thread - per thread-sec: <code>max(ended_at) - min(started_at)</code> across the thread\u2019s runs</p>"},{"location":"Metrics-Guide/#3-all-non-recursive-for-a-directory","title":"3) \u201cAll\u201d (non-recursive) for a directory","text":"<p>Run all thread-level charts and per-run charts inside a directory:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /metrics --chart all\n</code></pre> <p>What it produces: - For each thread: lollipop, tokens-bar, tokens-kde, tokens-rate, timeline HTML - For each JSON: lollipop, tokens-bar, tokens-kde, tokens-rate</p> <p>(Use <code>--log-x</code> to log-scale the lollipops &amp; KDE.)</p>"},{"location":"Metrics-Guide/#4-all-recursive-super-cross-thread-rollups","title":"4) \u201cAll-recursive\u201d + SUPER (cross-thread rollups)","text":"<p>Walk subdirectories, run <code>all</code> in each, then build a rollup at the root:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /workspaces --chart all-recursive\n</code></pre> <p>SUPER artifacts at <code>--dir</code>: - <code>super_lollipop.png</code> (time by component across all threads) - <code>super_tokens_bar.png</code> &amp; <code>super_tokens_kde.png</code> (totals &amp; distribution) - <code>super_tokens_rate.png</code> (TPS using \u03a3 thread windows &amp; \u03a3 LLM-sec) - By model: <code>super_tokens_bar_by_model.png</code>, <code>super_tokens_rate_by_model.png</code> - By agent: <code>super_agents_tokens.png</code>, <code>super_agents_tps.png</code> (Use the explicit charts below if you want just the agent rollups without re-running everything.)</p> <p>Build only the SUPER agent rollups for a directory you\u2019ve already processed:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /workspaces --chart super-agents-tokens\npython ursa/scripts/metrics_cli.py --dir /workspaces --chart super-agents-tps\n</code></pre> <p>In SUPER charts, the bottom footer does not show a single start\u2192end \u201cwindow\u201d, since different threads can overlap or run on different machines. Where relevant, the footer shows \u03a3 thread windows and \u03a3 LLM-active seconds instead.</p>"},{"location":"Metrics-Guide/#understanding-the-denominators-for-tps","title":"Understanding the denominators (for TPS)","text":"<ul> <li> <p>LLM-active seconds (sum)   From <code>tables.llm[].total_s</code> (or via event intervals for single runs). If multiple LLM calls overlap, the sum can exceed the wall window; this indicates parallelism.</p> </li> <li> <p>Thread window seconds   For a thread: <code>max(ended_at) - min(started_at)</code>.   For SUPER: sum of per-thread windows (not a single global wall window).</p> </li> </ul> <p>The TPS chart shows both denominators side-by-side to make parallelism visible.</p>"},{"location":"Metrics-Guide/#useful-options","title":"Useful options","text":"Flag Meaning Notes <code>--dir PATH</code> Directory to scan for metrics JSONs. Required for <code>all</code>, thread-level, and SUPER modes. <code>--chart</code> Which artifact(s) to generate. See lists above; default is <code>all</code>. <code>--thread ID</code> Limit to one thread for thread-level charts. Use with <code>--chart thread-*</code> or <code>timeline-html</code>. <code>--list-threads</code> Print discovered threads in <code>--dir</code>. Great to copy/paste a <code>--thread</code> ID. <code>--group-llm</code> Group all LLM rows into <code>llm:total</code> in time charts. Affects lollipop/pie/bar (time). <code>--group-by {agent,run}</code> Timeline y-axis grouping. <code>agent</code> is compact; <code>run</code> gives one lane per run. <code>--log-x</code> Log-scale for lollipop &amp; KDE. Helpful when components vary by orders of magnitude. <code>--min-label-pct FLOAT</code> Hide dot labels below this percent in lollipop. Default <code>0.0</code> (show all). <code>--title TEXT</code> Custom chart title. For targeted modes. <code>--out PATH</code> Custom output file path. For targeted modes. <code>--check</code> Print attribution totals for a single JSON and exit. Verifies <code>llm+tool+other \u2248 graph:graph</code>. <code>--epsilon FLOAT</code> Tolerance for <code>--check</code>. Default <code>0.050</code> seconds."},{"location":"Metrics-Guide/#examples-copypaste","title":"Examples (copy/paste)","text":"<pre><code># See what threads are in a directory\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun --list-threads\n</code></pre> <pre><code># Thread-level bundle for a single thread (PNG + HTML)\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart thread-tokens-rate --thread modsim_predict_final_mild-orange\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart timeline-html --thread modsim_predict_final_mild-orange --group-by agent\n</code></pre> <pre><code># Agent breakdowns for one thread (stacked tokens + TPS)\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart thread-agents-tokens --thread &lt;thread_id&gt;\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart thread-agents-tps --thread &lt;thread_id&gt;\n</code></pre> <pre><code># Directory-wide (non-recursive) batch\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun --chart all\n</code></pre> <pre><code># Recursive batch + SUPER rollups at the root\npython ursa/scripts/metrics_cli.py --dir ./workspaces --chart all-recursive\n</code></pre> <pre><code># Only the SUPER agent charts (when you already have per-thread results)\npython ursa/scripts/metrics_cli.py --dir ./workspaces --chart super-agents-tokens\npython ursa/scripts/metrics_cli.py --dir ./workspaces --chart super-agents-tps\n</code></pre> <pre><code># Single JSON \u2013 compare denominators in TPS\npython ursa/scripts/metrics_cli.py ./workspaces/t1/run_0007.json --chart tokens-rate --title \"step 7 TPS\"\n</code></pre>"},{"location":"Metrics-Guide/#output-naming-where-to-find-things","title":"Output naming &amp; where to find things","text":"<p>Per run (single JSON):</p> <pre><code>&lt;path&gt;/run_breakdown_lollipop.png\n&lt;path&gt;/run_breakdown_tokens_bar.png\n&lt;path&gt;/run_breakdown_tokens_kde.png\n&lt;path&gt;/run_breakdown_tokens_rate.png\n</code></pre> <p>Thread-level (in --dir):</p> <pre><code>thread_&lt;thread_id&gt;_lollipop.png\nthread_&lt;thread_id&gt;_tokens_bar.png\nthread_&lt;thread_id&gt;_tokens_kde.png\nthread_&lt;thread_id&gt;_tokens_rate.png\nthread_&lt;thread_id&gt;_timeline.html\nthread_&lt;thread_id&gt;_agents_tokens.png\nthread_&lt;thread_id&gt;_agents_tps.png\n</code></pre> <p>SUPER (at the root --dir for all-recursive):</p> <pre><code>super_lollipop.png\nsuper_tokens_bar.png\nsuper_tokens_kde.png\nsuper_tokens_rate.png\nsuper_tokens_bar_by_model.png\nsuper_tokens_rate_by_model.png\nsuper_agents_tokens.png\nsuper_agents_tps.png\n</code></pre>"},{"location":"Metrics-Guide/#tips","title":"Tips","text":"<ul> <li>Use <code>--log-x</code> for lollipop &amp; KDE when a few components dominate.</li> <li>Use <code>--group-llm</code> to collapse many LLM rows into a single \u201cllm:total\u201d bar for readability.</li> <li>The interactive timeline (<code>timeline-html</code>) is ideal for human inspection of overlaps; set <code>--group-by run</code> to see one lane per run.</li> <li>In SUPER TPS, the footer reports \u03a3 thread windows and \u03a3 LLM-active seconds rather than a single start\u2192end time.</li> </ul>"},{"location":"Metrics-Guide/#troubleshooting","title":"Troubleshooting","text":"<p>No thread IDs found - Ensure <code>--dir</code> points at a directory containing Telemetry JSON files. - JSON must include <code>context.thread_id</code>, <code>context.agent</code>, <code>context.run_id</code>, <code>context.started_at</code>, <code>context.ended_at</code>.</p> <p>Tokens charts look empty (all zeros) - Check that <code>llm_events[].metrics.usage_rollup</code> has <code>input_tokens</code> / <code>output_tokens</code> (or <code>prompt_tokens</code> / <code>completion_tokens</code>) fields. - If a provider omits <code>total_tokens</code>, the CLI computes <code>max(total, input+output)</code>.</p> <p>TPS \u201cLLM sum exceeds window \u2192 parallel LLM work\u201d - Expected when multiple LLM calls overlap. The note is helpful, not an error.</p> <p>Attribution check fails (<code>--check</code>) - The CLI prints: <code>graph:graph</code>, <code>LLM total_s</code>, <code>Tool total_s</code>, <code>Unattributed</code>, and any overage.   Small residuals under <code>--epsilon</code> are tolerated.</p> <p>Agent plots still zero - The agent aggregators depend on <code>context.agent</code> and <code>llm_events</code> being present per run. If your pipeline writes tokens only at the model level without <code>llm_events</code>, the totals per agent will be zero.</p>"},{"location":"Metrics-Guide/#version-notes","title":"Version notes","text":"<ul> <li>SUPER \u201cby model\u201d and \u201cby agent\u201d charts are additive across all discovered threads.  </li> <li>SUPER footers avoid a single run-window timestamp (threads can overlap and run elsewhere); they report sums instead.</li> </ul> <p>Happy charting!</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/","title":"Plan\u2013Execute Runner \u2014 Checkpointing &amp; Resume Guide","text":"<p>This guide shows how to run <code>plan_execute_from_yaml.py</code>, what files it creates, and how to resume from exactly the point you want \u2014 in both single and hierarchical planning modes.</p> <p>Example config to try: <code>examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml</code></p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#quickstart","title":"Quickstart","text":"<pre><code># Fresh run (prompts auto-default after the countdown)\npython examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml\n</code></pre> <p>Resume from a specific checkpoint: This example assumes the randomly generated workspace name is <code>FOOBAR</code>.  Change it to the one generated when you ran it yourself.  It also assumes a checkpoint number you want to resume.</p> <p>It's important to understand that what this does is look in <code>workspace</code> for a checkpoint to resume from.  You can set that <code>workspace</code> yourself by name or use the name generated by URSA if you didn't choose one yourself.</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_FOOBAR \\\n  --resume-from executor_checkpoint_5.db\n</code></pre> <p>Headless / HPC-friendly (no waiting at prompts):</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_batchrun \\\n  --interactive-timeout 0\n</code></pre>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#what-the-script-does","title":"What the script does","text":"<ul> <li>Plans steps from your YAML \u201cproblem.\u201d</li> <li>Executes those steps while checkpointing to SQLite.</li> <li>Lets you pause/resume from either:</li> <li>the live checkpoint (<code>executor_checkpoint.db</code>), or</li> <li>any snapshot (<code>executor_checkpoint_*.db</code>) created after each step/sub-step.</li> </ul> <p>All interactive prompts include a countdown and then pick a safe default. Use <code>--interactive-timeout 0</code> to default immediately (great for clusters).</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#cli-options","title":"CLI Options","text":"Flag What it does When to use it <code>--config PATH</code> Path to your YAML problem config. Required. Try the example <code>pi_multiple_ways.yaml</code>. <code>--workspace NAME</code> Directory for checkpoints &amp; outputs. If omitted, a new name is generated and printed. Use a stable name to resume later. <code>--planning-mode {single,hierarchical}</code> Force planning mode. If omitted, you\u2019ll be prompted. First run locks the workspace to that mode. <code>single</code> = plan once, run top-level steps. <code>hierarchical</code> = plan top-level, then re-plan sub-steps per main step. <code>--stepwise-exit</code> Demo mode: exits after each plan/step checkpoint. Good for demonstrating checkpointing; not needed normally. <code>--resume-from FILE</code> Restore executor state from a checkpoint file (e.g., <code>executor_checkpoint_5.db</code> or <code>executor_checkpoint_3_2.db</code>). Jump back to an earlier step or to a precise sub-step. <code>--interactive-timeout SECONDS</code> How long prompts (model/mode/checkpoint) wait before defaulting. <code>0</code> = default immediately. Set to <code>0</code> for HPC/headless. Otherwise default is <code>60</code>."},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#files-youll-see-in-a-workspace","title":"Files you\u2019ll see in a workspace","text":"<pre><code>&lt;workspace&gt;/\n\u251c\u2500\u2500 executor_checkpoint.db          # live executor DB (current run)\n\u251c\u2500\u2500 executor_checkpoint_1.db        # snapshot after step 1 (single mode)\n\u251c\u2500\u2500 executor_checkpoint_2.db        # snapshot after step 2\n\u251c\u2500\u2500 executor_checkpoint_3_2.db      # snapshot after MAIN=3, SUB=2 (hierarchical)\n\u251c\u2500\u2500 executor_progress.json          # single-mode progress (next_index, plan_hash, last_summary)\n\u251c\u2500\u2500 hier_progress.json              # hierarchical progress (main index + per-step sub-progress)\n\u251c\u2500\u2500 planner_checkpoint.db           # planner\u2019s DB\n\u251c\u2500\u2500 run_meta.json                   # metadata (planning_mode lock, plan hash, model, etc.)\n\u2514\u2500\u2500 ...                             # artifacts produced by steps (csv, plots, code, html, etc.)\n</code></pre> <p>Snapshot naming: - Single mode: <code>executor_checkpoint_&lt;STEP&gt;.db</code>   Example for 6 steps: <code>executor_checkpoint_1.db</code> . . . <code>executor_checkpoint_6.db</code>. - Hierarchical mode: <code>executor_checkpoint_&lt;MAIN&gt;_&lt;SUB&gt;.db</code> <code>&lt;MAIN&gt;</code> is the 1-based top-level step; <code>&lt;SUB&gt;</code> is the 1-based sub-step just finished.</p> <p>You can resume from any of these snapshot files.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#single-vs-hierarchical","title":"Single vs. Hierarchical","text":""},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#single-mode","title":"Single mode","text":"<ul> <li>Planner creates one list of top-level steps.</li> <li>Execution proceeds linearly.</li> <li>After each top-level step completes, a snapshot is saved:</li> <li><code>executor_checkpoint_1.db</code>, <code>executor_checkpoint_2.db</code>, . . . </li> </ul> <p>Resuming (single): - <code>--resume-from executor_checkpoint_5.db</code> sets <code>executor_progress.json</code> so the next step to run is step 6. - Without <code>--resume-from</code>, you\u2019ll get an interactive chooser (with countdown). Default is <code>executor_checkpoint.db</code> (live).</p> <p>Example tree (6 steps planned):</p> <pre><code>workspace/\n\u251c\u2500\u2500 executor_checkpoint.db\n\u251c\u2500\u2500 executor_checkpoint_1.db\n\u251c\u2500\u2500 executor_checkpoint_2.db\n\u251c\u2500\u2500 executor_checkpoint_3.db\n\u251c\u2500\u2500 executor_checkpoint_4.db\n\u251c\u2500\u2500 executor_checkpoint_5.db\n\u251c\u2500\u2500 executor_checkpoint_6.db\n\u2514\u2500\u2500 executor_progress.json\n</code></pre> <p>To redo from step 3, run with: <code>--resume-from executor_checkpoint_2.db</code>.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#hierarchical-mode","title":"Hierarchical mode","text":"<ul> <li>Planner creates top-level steps.</li> <li>For each main step, it re-plans concrete sub-steps.</li> <li>A snapshot is saved after each sub-step:</li> <li><code>executor_checkpoint_3_2.db</code> = finished main step 3, sub-step 2.</li> </ul> <p>Resuming (hierarchical): - <code>--resume-from executor_checkpoint_3_2.db</code> resumes within main step 3 at sub-step 3 (if it exists). - <code>executor_checkpoint_4_1.db</code> would continue with main 4, sub-step 2, etc.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#typical-flows","title":"Typical flows","text":""},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#fresh-run-default-everything","title":"Fresh run, default everything","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml\n</code></pre> <ul> <li>Prompts choose defaults after the countdown.</li> <li>A new workspace is created if <code>--workspace</code> isn\u2019t supplied.</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#resume-from-the-latest-run-no-prompts","title":"Resume from the latest run (no prompts)","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_myws \\\n  --interactive-timeout 0\n</code></pre> <ul> <li>Uses the default <code>executor_checkpoint.db</code>.  </li> <li>If no checkpoints exist yet, it starts fresh.</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#jump-back-to-an-earlier-point","title":"Jump back to an earlier point","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_myws \\\n  --resume-from executor_checkpoint_5.db\n</code></pre> <ul> <li>Great for re-running from step 6 in single mode, or for precise sub-step resumes in hierarchical mode using <code>&lt;MAIN&gt;_&lt;SUB&gt;</code>.</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#planning-mode-lock-per-workspace","title":"Planning-mode lock per workspace","text":"<p>On the first run in a workspace, the chosen planning mode is locked in <code>run_meta.json</code>. If you later pass a different <code>--planning-mode</code> for the same workspace, the script warns and keeps the original mode. Use a new <code>--workspace</code> to switch modes for a project.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#when-the-plan-changes","title":"When the plan changes","text":"<p>The runner hashes your plan (<code>plan_sig</code>). If the plan changes:</p> <ul> <li>Single mode: <code>executor_progress.json</code> resets to step 0 for the new plan.</li> <li>Hierarchical mode: top-level or sub-plan changes reset the affected progress segments.</li> </ul> <p>If you want a clean re-run, either: - remove <code>executor_progress.json</code> (single), and/or - remove <code>hier_progress.json</code> (hierarchical), or - choose a new workspace.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#example-run-the-provided-yaml","title":"Example: run the provided YAML","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --planning-mode single\n</code></pre> <p>You\u2019ll see messages like:</p> <pre><code>[checkpoint] saved step snapshot: executor_checkpoint_1.db\n[checkpoint] saved step snapshot: executor_checkpoint_2.db\n...\n</code></pre> <p>Now resume from after step 4:</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_&lt;your-workspace&gt; \\\n  --resume-from executor_checkpoint_4.db\n</code></pre> <p>Or try hierarchical mode:</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --planning-mode hierarchical\n</code></pre> <p>You\u2019ll see sub-step snapshots like <code>executor_checkpoint_3_2.db</code>.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#tips-for-clusters-headless","title":"Tips for clusters / headless","text":"<ul> <li>Use <code>--interactive-timeout 0</code> so prompts immediately pick defaults.</li> <li>Always pass <code>--workspace</code> so outputs land where you expect.</li> <li>Use <code>--resume-from</code> to pin the exact snapshot to restore (no prompt).</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>\u201cAll steps already executed\u201d (single):   You\u2019re at the end per <code>executor_progress.json</code>. To re-run, delete it or resume from an earlier snapshot (e.g., <code>--resume-from executor_checkpoint_2.db</code>).</p> </li> <li> <p>\u201cTop-level plan changed \u2014 resetting . . .\u201d (hierarchical):   The plan hash changed; progress resets for safety.</p> </li> <li> <p>Mode mismatch warning:   The workspace is locked to the first-run mode. Use a new <code>--workspace</code> to change modes.</p> </li> </ul> <p>Happy checkpointing &amp; resuming!</p>"},{"location":"arxiv_agent/","title":"ArxivAgent Documentation","text":"<p><code>ArxivAgent</code> is a class that helps fetch, process, and summarize scientific papers from arXiv. It uses LLMs to generate summaries of papers relevant to a given query and context.</p>"},{"location":"arxiv_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import ArxivAgent\n\n# Initialize the agent\nagent = ArxivAgent()\n\n# Run a query\nresult = agent.invoke(\n    arxiv_search_query=\"Experimental Constraints on neutron star radius\", \n    context=\"What are the constraints on the neutron star radius and what uncertainties are there on the constraints?\"\n)\n\n# Print the summary\nprint(result)\n</code></pre>"},{"location":"arxiv_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>ArxivAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> <code>init_chat_model(\"openai:gpt-5-mini\")</code> The LLM model to use for summarization <code>summarize</code> bool True Whether to summarize the papers or just fetch them <code>process_images</code> bool True Whether to extract and describe images from papers <code>max_results</code> int 3 Maximum number of papers to fetch from arXiv <code>database_path</code> str 'arxiv_papers' Directory to store downloaded PDFs <code>summaries_path</code> str 'arxiv_generated_summaries' Directory to store paper summaries <code>vectorstore_path</code> str 'arxiv_vectorstores' Directory to store vector embeddings <code>download_papers</code> bool True Whether to download papers or use existing ones"},{"location":"arxiv_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"arxiv_agent/#customizing-the-agent","title":"Customizing the Agent","text":"<pre><code>from langchain.chat_models import init_chat_model\nfrom ursa.agents import ArxivAgent\n\nagent = ArxivAgent(\n    llm=init_chat_model(\"openai:gpt-5-mini\"),  # Use a more powerful model\n    max_results=5,       # Fetch more papers\n    process_images=False,  # Skip image processing to save time\n    download_papers=False  # Use only papers already in database_path\n)\n</code></pre>"},{"location":"arxiv_agent/#running-multiple-queries","title":"Running Multiple Queries","text":"<pre><code># First query\nresult1 = agent.invoke(\n    arxiv_search_query=\"quantum computing error correction\", \n    context=\"Summarize recent advances in quantum error correction techniques\"\n)\n\n# Second query (will reuse downloaded papers if applicable)\nresult2 = agent.invoke(\n    arxiv_search_query=\"quantum computing algorithms\", \n    context=\"What are the most promising quantum algorithms for near-term devices?\"\n)\n</code></pre>"},{"location":"arxiv_agent/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Fetching Papers: The agent searches arXiv for papers matching your query and downloads them as PDFs.</p> </li> <li> <p>Processing: If <code>summarize=True</code>, each paper is:</p> </li> <li>Converted to text</li> <li>Split into chunks</li> <li>Embedded into a vector database</li> <li> <p>If <code>process_images=True</code>, images are extracted and described using GPT-4 Vision</p> </li> <li> <p>Summarization: The agent:</p> </li> <li>Retrieves the most relevant chunks based on your context</li> <li>Generates a summary for each paper</li> <li> <p>Creates a final summary addressing your specific context</p> </li> <li> <p>Output: Returns a comprehensive summary that synthesizes information from all relevant papers.</p> </li> </ol>"},{"location":"arxiv_agent/#notes","title":"Notes","text":"<ul> <li>Summaries and vector stores are cached, making subsequent queries faster.</li> <li>The agent uses a ThreadPoolExecutor to process papers in parallel.</li> <li>You can find the combined summaries in 'summaries_combined.txt' and the final summary in 'final_summary.txt'.</li> </ul>"},{"location":"chatollama_setup/","title":"Running with ChatOllama","text":"<p>Disable OPENAI_API_KEY by setting the following two env variales: (without both of these env vars ollama complains about auth)</p> <pre><code>$ export OPENAI_API_KEY=ollama\n$ export OPENAI_BASE_URL=&lt;YOUR OLLAMA ENDPOINT&gt;\n</code></pre> <p>Example Auth Error</p> <pre><code>openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: ollama. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n</code></pre>"},{"location":"combining_arxiv_and_execution/","title":"Using ArxivAgent and ExecutionAgent Together","text":"<p>This guide demonstrates how to combine the <code>ArxivAgent</code> and <code>ExecutionAgent</code> for comprehensive research and analysis workflows.</p>"},{"location":"combining_arxiv_and_execution/#overview","title":"Overview","text":"<p>This workflow enables you to: 1. Search and analyze papers from arXiv 2. Process the research findings 3. Generate executable code to analyze and visualize the data</p>"},{"location":"combining_arxiv_and_execution/#basic-usage","title":"Basic Usage","text":"<pre><code>from langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage\nfrom ursa.agents import ArxivAgent, ExecutionAgent\n\n# Initialize the language model\nmodel = init_chat_model(\n    model=\"openai:gpt-5-mini\",\n    max_tokens=50000,\n)\n\n# Initialize the ArxivAgent\narxiv_agent = ArxivAgent(\n    llm=model,\n    summarize=True,\n    process_images=False,\n    max_results=20,\n    database_path=\"arxiv_papers_materials1\",\n    summaries_path=\"arxiv_summaries_materials1\",\n    vectorstore_path=\"arxiv_vectorstores_materials1\",\n    download_papers=True,\n)\n\n# Run a search and analysis\nresearch_results = arxiv_agent.invoke(\n    arxiv_search_query=\"high entropy alloy hardness\",\n    context=\"What data and uncertainties are reported for hardness of the high entropy alloy and how that that compare to other alloys?\",\n)\n\n# Initialize the ExecutionAgent\nexecutor = ExecutionAgent(llm=model)\n\n# Create a task for the ExecutionAgent\nexecution_plan = f\"\"\"\nThe following is the summaries of research papers on the high entropy alloy hardness: \n{research_results}\n\nSummarize the results in a markdown document. Include a plot of the data extracted from the papers. This \nwill be reviewed by experts in the field so technical accuracy and clarity is critical.\n\"\"\"\n\n# Prepare input for the ExecutionAgent\ninit = {\"messages\": [HumanMessage(content=execution_plan)]}\n\n# Execute the plan\nfinal_results = executor.invoke(init)\n\n# Display results\nfor message in final_results[\"messages\"]:\n    print(message.content)\n</code></pre>"},{"location":"combining_arxiv_and_execution/#parameters","title":"Parameters","text":""},{"location":"combining_arxiv_and_execution/#arxivagent","title":"ArxivAgent","text":"Parameter Description <code>llm</code> Language model to use for analysis <code>summarize</code> Whether to summarize papers (boolean) <code>process_images</code> Whether to process images in papers (boolean) <code>max_results</code> Maximum number of papers to retrieve <code>database_path</code> Path to store downloaded papers <code>summaries_path</code> Path to store paper summaries <code>vectorstore_path</code> Path to store vector embeddings <code>download_papers</code> Whether to download full papers (boolean)"},{"location":"combining_arxiv_and_execution/#executionagent","title":"ExecutionAgent","text":"Parameter Description <code>llm</code> Language model to use for code generation and execution"},{"location":"combining_arxiv_and_execution/#workflow-steps","title":"Workflow Steps","text":"<ol> <li> <p>Research Phase: ArxivAgent searches arXiv for relevant papers based on your query, downloads them, and analyzes their content according to your context.</p> </li> <li> <p>Analysis Phase: ExecutionAgent takes the research results and generates code to analyze and visualize the data.</p> </li> <li> <p>Output: The ExecutionAgent produces a markdown document with analysis, visualizations, and insights from the research.</p> </li> </ol>"},{"location":"combining_arxiv_and_execution/#use-cases","title":"Use Cases","text":"<ul> <li>Literature reviews on scientific topics</li> <li>Data extraction and visualization from research papers</li> <li>Comparative analysis across multiple publications</li> <li>Technical report generation</li> </ul>"},{"location":"combining_arxiv_and_execution/#notes","title":"Notes","text":"<ul> <li>Ensure you have sufficient disk space for paper storage</li> <li>Processing a large number of papers may take significant time</li> <li>The quality of analysis depends on the capabilities of the chosen language model</li> </ul>"},{"location":"combining_arxiv_and_execution_neutronStar/","title":"Using ArxivAgent and ExecutionAgent for Astrophysics Research","text":"<p>This guide demonstrates how to use the <code>ArxivAgent</code> and <code>ExecutionAgent</code> together to research neutron star properties and generate a comprehensive analysis.</p>"},{"location":"combining_arxiv_and_execution_neutronStar/#overview","title":"Overview","text":"<p>This workflow allows you to: 1. Search arXiv for papers on neutron star radius constraints 2. Process and summarize the research findings 3. Generate a markdown document with data visualization</p>"},{"location":"combining_arxiv_and_execution_neutronStar/#basic-usage","title":"Basic Usage","text":"<pre><code>from langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage\nfrom ursa.agents import ArxivAgent, ExecutionAgent\n\n# Initialize the language model\nmodel = init_chat_model(\n    model=\"openai:gpt-5-mini\",\n    max_tokens=50000,\n)\n\n# Initialize the ArxivAgent\narxiv_agent = ArxivAgent(\n    llm=model,\n    summarize=True,\n    process_images=False,\n    max_results=5,\n    database_path=\"arxiv_papers_neutron_star\",\n    summaries_path=\"arxiv_summaries_neutron_star\",\n    vectorstore_path=\"arxiv_vectorstores_neutron_star\",\n    download_papers=True,\n)\n\n# Run a search on neutron star radius constraints\nresearch_results = arxiv_agent.invoke(\n    arxiv_search_query=\"Experimental Constraints on neutron star radius\",\n    context=\"What are the constraints on the neutron star radius and what uncertainties are there on the constraints?\",\n)\n\n# Initialize the ExecutionAgent\nexecutor = ExecutionAgent(llm=model)\n\n# Create a task for the ExecutionAgent\nexecution_plan = f\"\"\"\nThe following is the summaries of research papers on the contraints on neutron\nstar radius: \n{research_results}\n\nSummarize the results in a markdown document. Include a plot of the data extracted from the papers. This \nwill be reviewed by experts in the field so technical accuracy and clarity is \ncritical.\n\"\"\"\n\n# Prepare input for the ExecutionAgent\ninit = {\"messages\": [HumanMessage(content=execution_plan)]}\n\n# Execute the plan\nfinal_results = executor.invoke(init)\n\n# Display results\nfor message in final_results[\"messages\"]:\n    print(message.content)\n</code></pre>"},{"location":"combining_arxiv_and_execution_neutronStar/#parameters","title":"Parameters","text":""},{"location":"combining_arxiv_and_execution_neutronStar/#arxivagent","title":"ArxivAgent","text":"Parameter Description <code>llm</code> Language model to use for analysis <code>summarize</code> Whether to summarize papers (boolean) <code>process_images</code> Whether to process images in papers (boolean) <code>max_results</code> Maximum number of papers to retrieve (5 in this example) <code>database_path</code> Path to store downloaded papers <code>summaries_path</code> Path to store paper summaries <code>vectorstore_path</code> Path to store vector embeddings <code>download_papers</code> Whether to download full papers (boolean)"},{"location":"combining_arxiv_and_execution_neutronStar/#executionagent","title":"ExecutionAgent","text":"Parameter Description <code>llm</code> Language model to use for code generation and execution"},{"location":"combining_arxiv_and_execution_neutronStar/#workflow-steps","title":"Workflow Steps","text":"<ol> <li> <p>Research Phase: ArxivAgent searches arXiv for papers on neutron star radius constraints, downloads them, and analyzes their content.</p> </li> <li> <p>Analysis Phase: ExecutionAgent processes the research summaries and generates code to visualize the constraints and uncertainties.</p> </li> <li> <p>Output: The ExecutionAgent produces a markdown document with analysis, visualizations, and insights about neutron star radius constraints.</p> </li> </ol>"},{"location":"combining_arxiv_and_execution_neutronStar/#use-cases","title":"Use Cases","text":"<ul> <li>Astrophysics literature reviews</li> <li>Compilation of experimental constraints on astronomical objects</li> <li>Visualization of scientific data from multiple sources</li> <li>Creation of technical reports for expert audiences</li> </ul>"},{"location":"combining_arxiv_and_execution_neutronStar/#notes","title":"Notes","text":"<ul> <li>The quality of analysis depends on the available papers and the capabilities of the language model</li> <li>Consider adjusting <code>max_results</code> based on the breadth of research needed</li> <li>Ensure proper storage paths are set to avoid conflicts with other research projects</li> </ul>"},{"location":"execution_agent/","title":"ExecutionAgent Documentation","text":"<p><code>ExecutionAgent</code> is a class that enables AI-powered code execution, writing, and editing. It uses a state machine architecture to safely execute commands, write code files, and search for information.</p>"},{"location":"execution_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import ExecutionAgent\n\n# Initialize the agent\nagent = ExecutionAgent()\n\n# Run a prompt\nresult = agent(\"Write and execute a python script to print the first 10 integers.\")\n\n# Access the final response\nprint(result[\"messages\"][-1].content)\n</code></pre>"},{"location":"execution_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>ExecutionAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> <code>init_chat_model(\"openai:gpt-5-mini\")</code> The LLM model to use <code>extra_tools</code> <code>Optional[list[Callable[..., Any]]]</code> <code>None</code> Additional tools for the execution agent"},{"location":"execution_agent/#features","title":"Features","text":""},{"location":"execution_agent/#code-execution","title":"Code Execution","text":"<p>The agent can safely execute shell commands in a controlled environment:</p> <pre><code>result = agent(\"Install numpy and create a script that uses it to calculate the mean of [1, 2, 3, 4, 5]\")\n</code></pre>"},{"location":"execution_agent/#code-writing","title":"Code Writing","text":"<p>The agent can write code files to a workspace directory:</p> <pre><code>result = agent(\"Create a Flask web application that displays 'Hello World'\")\n</code></pre>"},{"location":"execution_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"execution_agent/#customizing-the-workspace","title":"Customizing the Workspace","text":"<p>The agent creates a workspace folder with a randomly generated name for each run. You can access this workspace path from the result:</p> <pre><code>result = agent(\"Create a Python script\"))\nworkspace_path = result[\"workspace\"]\nprint(f\"Files were created in: {workspace_path}\")\n</code></pre>"},{"location":"execution_agent/#setting-a-recursion-limit","title":"Setting a Recursion Limit","text":"<p>For complex tasks, you may need to adjust the recursion limit:</p> <pre><code>result = agent.invoke(\n    \"Create a complex project with multiple files and tests\", \n    recursion_limit=2000\n)\n</code></pre>"},{"location":"execution_agent/#safety-features","title":"Safety Features","text":"<p>The agent includes built-in safety checks for shell commands:</p> <ol> <li>Commands are evaluated for safety before execution</li> <li>Unsafe commands are blocked with explanations</li> <li>The agent suggests safer alternatives when appropriate</li> </ol>"},{"location":"execution_agent/#how-it-works","title":"How It Works","text":"<ol> <li>State Machine: The agent uses a directed graph to manage its workflow:</li> <li><code>agent</code> node: Processes user requests and generates responses</li> <li><code>safety_check</code> node: Evaluates command safety</li> <li> <p><code>action</code> node: Executes tools (<code>run_cmd</code>, <code>write_code</code>, <code>edit_code</code>, <code>search</code>)</p> <ul> <li>extra tools can be provided to the agent as follows:    ```py    from langchain.tools import tool</li> </ul> <p>@tool    def do_magic(a: int, b: int) -&gt; float:        \"\"\"Do magic with integers a and b.</p> <pre><code>   Args:\n       a: first integer\n       b: second integer\n   \"\"\"\n   return sqrt(a**2 + b**2)\n</code></pre> <p>agent = ExecutionAgent(extra_tools=[do_magic])    <code>``    -</code>summarize` node: Creates a final summary when complete</p> </li> <li> <p>Tools:</p> </li> <li><code>run_cmd</code>: Executes shell commands in the workspace directory</li> <li><code>write_code</code>: Creates new code files with syntax highlighting</li> <li><code>edit_code</code>: Modifies existing code files with diff preview</li> <li> <p><code>search_tool</code>: Performs web searches via DuckDuckGo</p> </li> <li> <p>Visualization:</p> </li> <li>Code changes are displayed with syntax highlighting</li> <li>File edits show detailed diffs</li> <li>Command execution shows stdout and stderr</li> </ol>"},{"location":"execution_agent/#notes","title":"Notes","text":"<ul> <li>The agent creates a new workspace directory for each run</li> <li>Files are written to and executed from this workspace</li> <li>Shell commands have a 60000-second timeout by default</li> <li>The agent can handle keyboard interrupts during command execution</li> </ul>"},{"location":"humanInTheLoop_example/","title":"URSA Human-in-the-Loop Agent Interface Documentation","text":""},{"location":"humanInTheLoop_example/#overview","title":"Overview","text":"<p>This module implements a human-in-the-loop (HITL) interface for the URSA agent framework, allowing users to directly interact with different specialized AI agents through a command-line interface. The system maintains context between agent interactions and provides persistent storage for agent states.</p> <p>The file can be run with:</p> <p><code>python /path/to/hitl_basic.py</code></p>"},{"location":"humanInTheLoop_example/#setup-and-initialization","title":"Setup and Initialization","text":"<ol> <li>Creates a workspace directory for storing agent data and checkpoints</li> <li>Initializes SQLite databases and checkpointers for various agents:</li> <li>Executor agent</li> <li>Planner agent</li> <li>WebSearcher agent</li> <li>Configures the language model and embedding model</li> <li>Instantiates the following agents:</li> <li>ArxivAgent</li> <li>ExecutionAgent</li> <li>PlanningAgent</li> <li>WebSearchAgent</li> <li>RecallAgent</li> </ol>"},{"location":"humanInTheLoop_example/#user-interaction-loop","title":"User Interaction Loop","text":"<p>The function runs a continuous interaction loop until the user enters \"[USER DONE]\":</p> <ol> <li>Displays a header explaining how to use the interface</li> <li>Prompts the user for input</li> <li>Parses the input to determine which agent to invoke:</li> <li><code>[Arxiver]</code>: Searches ArXiv for academic papers</li> <li><code>[Executor]</code>: Executes code or commands</li> <li><code>[Planner]</code>: Creates plans or strategies</li> <li><code>[WebSearcher]</code>: Performs web searches</li> <li><code>[Rememberer]</code>: Retrieves information from memory</li> <li><code>[Chatter]</code>: Has a general conversation using the language model</li> <li>Importantly, the output from the previous agent interaction is automatically included in the prompt to the next agent. This creates a continuous context flow where each agent has access to what the previous agent produced.</li> <li>Invokes the appropriate agent with the user's query and context from previous interactions</li> <li>Displays the agent's response</li> <li>Continues the loop until the user indicates they're done</li> </ol>"},{"location":"humanInTheLoop_example/#agent-specific-behavior","title":"Agent-Specific Behavior","text":"<ul> <li>ArxivAgent: Converts user query into a search query, retrieves relevant papers, and summarizes results</li> <li>ExecutionAgent: Processes user instructions in the context of previous outputs, can execute code</li> <li>PlanningAgent: Creates plans based on user requirements and previous context</li> <li>WebSearchAgent: Performs web searches based on user queries</li> <li>RecallAgent: Retrieves relevant information from persistent memory</li> <li>Chatter: Provides direct access to the language model for general conversation</li> </ul>"},{"location":"humanInTheLoop_example/#usage","title":"Usage","text":"<p>Run the script and interact with agents by prefixing your queries with the appropriate agent tag:</p> <pre><code>[Arxiver] Find recent papers on transformer architecture improvements\n[Executor] Write a Python function to calculate Fibonacci numbers\n[Planner] Create a research plan for analyzing climate data\n[WebSearcher] What are the latest developments in quantum computing?\n[Rememberer] What did we discuss about neural networks earlier?\n[Chatter] Explain the concept of attention mechanisms in simple terms\n</code></pre>"},{"location":"humanInTheLoop_example/#context-continuity","title":"Context Continuity","text":"<p>A key feature of this interface is that each agent receives both your current query AND the output from the previous agent interaction. This allows for natural follow-up queries and building on previous results. For example:</p> <ol> <li><code>[WebSearcher] Find information about large language models</code></li> <li><code>[Planner] Create a research plan based on this information</code></li> </ol> <p>In this sequence, the Planner would receive both your planning request AND the search results about large language models from the WebSearcher, enabling it to create a more informed and contextually relevant plan.</p> <p>Use \"[USER DONE]\" to exit the interface.</p>"},{"location":"hypothesizer_agent/","title":"HypothesizerAgent Documentation","text":"<p><code>HypothesizerAgent</code> is a multi-agent system that iteratively refines solutions to complex problems through a structured debate process. It employs three specialized agents that work together to generate, critique, and provide alternative perspectives on potential solutions.</p>"},{"location":"hypothesizer_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import HypothesizerAgent\n\n# Initialize the agent\nagent = HypothesizerAgent()\n\n# Run the agent with a question\nsolution = agent.invoke(\n    prompt=\"Find a city with at least 10 vowels in its name.\",\n    max_iter=3\n)\n\n# Print the final solution\nprint(solution)\n</code></pre>"},{"location":"hypothesizer_agent/#parameters","title":"Parameters","text":""},{"location":"hypothesizer_agent/#initialization","title":"Initialization","text":"Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> <code>init_chat_model(\"openai:gpt-5-mini\")</code> The language model to use for all agents <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters passed to the base agent"},{"location":"hypothesizer_agent/#run-method","title":"Run Method","text":"Parameter Type Default Description <code>prompt</code> str Required The question or problem to solve <code>max_iter</code> int 3 Maximum number of refinement iterations <code>recursion_limit</code> int 99999 Maximum recursion depth for the graph"},{"location":"hypothesizer_agent/#how-it-works","title":"How It Works","text":"<p>The system uses a three-agent debate process:</p> <ol> <li>Agent 1 (Hypothesizer): Generates initial solutions and refines them based on feedback</li> <li>Agent 2 (Critic): Identifies flaws, assumptions, and areas for improvement</li> <li>Agent 3 (Competitor/Stakeholder): Provides alternative perspectives from different stakeholders</li> </ol> <p>The process iterates through these agents multiple times, with each iteration building on the feedback from previous rounds.</p>"},{"location":"hypothesizer_agent/#features","title":"Features","text":"<ul> <li>Web Search Integration: Uses DuckDuckGo to gather information for each agent</li> <li>Iterative Refinement: Solutions improve through multiple rounds of critique and revision</li> <li>LaTeX Report Generation: Creates a comprehensive LaTeX document summarizing the process</li> <li>URL Tracking: Records all websites visited during the research process</li> </ul>"},{"location":"hypothesizer_agent/#output","title":"Output","text":"<p>The agent produces:</p> <ol> <li>A final refined solution (returned by the <code>run</code> method)</li> <li>A LaTeX document with:</li> <li>Executive summary of the iterative process</li> <li>Final solution in full</li> <li>Detailed appendix of all iterations</li> <li>List of websites visited during research</li> <li>A text file containing the full history of all iterations</li> </ol>"},{"location":"hypothesizer_agent/#example","title":"Example","text":"<pre><code>from ursa.agents import HypothesizerAgent\nfrom langchain.chat_models import init_chat_model\n\n# Initialize with a specific LLM\nagent = HypothesizerAgent(llm=init_chat_model(\"openai:gpt-5-mini\"))\n\n# Run with 5 iterations\nresult = agent.invoke(\n    prompt=\"What strategies could a small local bookstore use to compete with large online retailers?\",\n    max_iter=5\n)\n\nprint(result)\n</code></pre>"},{"location":"hypothesizer_agent/#notes","title":"Notes","text":"<ul> <li>Each iteration includes a solution, critique, and competitor perspective</li> <li>The agent performs web searches to gather information at each step</li> <li>The final LaTeX document provides a comprehensive record of the reasoning process</li> <li>Higher <code>max_iter</code> values produce more refined solutions but take longer to complete</li> </ul>"},{"location":"planning_agent/","title":"PlanningAgent Documentation","text":"<p><code>PlanningAgent</code> is a class that implements a multi-step planning approach for complex problem solving. It uses a state machine architecture to generate plans, reflect on them, and formalize the final solution.</p>"},{"location":"planning_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import PlanningAgent\n\n# Initialize the agent\nagent = PlanningAgent()\n\n# Run a planning task\nresult = agent.invoke(\"Find a city with at least 10 vowels in its name.\")\n\n# Access the final plan\nplan_steps = result[\"plan_steps\"]\n</code></pre>"},{"location":"planning_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>PlanningAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> BaseChatModel <code>init_chat_model(\"openai:gpt-5-mini\")</code> The LLM model to use for planning <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters passed to the base agent"},{"location":"planning_agent/#features","title":"Features","text":""},{"location":"planning_agent/#multi-step-planning","title":"Multi-step Planning","text":"<p>The agent follows a three-stage planning process:</p> <ol> <li>Generation: Creates an initial plan to solve the problem</li> <li>Reflection: Critically evaluates and improves the plan</li> <li>Formalization: Structures the final plan as a JSON object</li> </ol>"},{"location":"planning_agent/#structured-output","title":"Structured Output","text":"<p>The final output includes:</p> <ul> <li><code>messages</code>: The conversation history</li> <li><code>plan_steps</code>: A structured list of steps to solve the problem</li> </ul>"},{"location":"planning_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"planning_agent/#customizing-reflection-steps","title":"Customizing Reflection Steps","text":"<p>You can adjust how many reflection iterations the agent performs:</p> <pre><code># Initialize with custom reflection steps\ninitial_state = {\n    \"messages\": [HumanMessage(content=\"Your complex problem here\")],\n    \"reflection_steps\": 5  # Default is 3\n}\n\nresult = agent.invoke(initial_state, {\"configurable\": {\"thread_id\": agent.thread_id}})\n</code></pre>"},{"location":"planning_agent/#streaming-results","title":"Streaming Results","text":"<p>You can stream the agent's thinking process:</p> <pre><code>for event in agent.stream(\n    {\"messages\": [HumanMessage(content=\"Your problem here\")]},\n    {\"configurable\": {\"thread_id\": agent.thread_id}}\n):\n    print(event[list(event.keys())[0]][\"messages\"][-1].content)\n</code></pre>"},{"location":"planning_agent/#setting-a-recursion-limit","title":"Setting a Recursion Limit","text":"<p>For complex planning tasks, you may need to adjust the recursion limit:</p> <pre><code>result = agent.invoke(\n    \"Solve this complex problem...\", \n    recursion_limit=200  # Default is 100\n)\n</code></pre>"},{"location":"planning_agent/#how-it-works","title":"How It Works","text":"<ol> <li>State Machine: The agent uses a directed graph to manage its workflow:</li> <li><code>generate</code> node: Creates or improves the plan</li> <li><code>reflect</code> node: Evaluates the plan for improvements</li> <li> <p><code>formalize</code> node: Structures the final plan as JSON</p> </li> <li> <p>Termination Conditions: The planning process ends when either:</p> </li> <li>The agent has completed the specified number of reflection steps</li> <li> <p>The agent explicitly marks the plan as \"[APPROVED]\"</p> </li> <li> <p>JSON Output: The final plan is structured as a JSON array of steps, each containing:</p> </li> <li>A description of the step</li> <li>Any relevant details for executing that step</li> </ol>"},{"location":"planning_agent/#notes","title":"Notes","text":"<ul> <li>The agent continues to refine its plan through multiple reflection cycles</li> <li>The final output is a structured JSON representation of the solution steps</li> <li>You can access the complete conversation history in the <code>messages</code> field of the result</li> </ul>"},{"location":"web_search_agent/","title":"WebSearchAgent Documentation","text":"<p><code>WebSearchAgent</code> is a powerful tool for conducting internet-based research on any topic. It leverages language models and web search capabilities to gather, process, and summarize information from online sources.</p>"},{"location":"web_search_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import WebSearchAgent\nfrom langchain_openai import ChatOpenAI\n\n# Initialize with default model (gpt-5-mini)\nwebsearcher = WebSearchAgent()\n\n# Or initialize with a custom model\nmodel = ChatOpenAI(model=\"gpt-5-mini\", max_completion_tokens=10000)\nwebsearcher = WebSearchAgent(llm=model)\n\n# Run a web search query\nresult = websearcher.invoke(\"Who are the 2025 Detroit Tigers top 10 prospects and what year were they born?\")\n\n# Access the web search results\nfinal_summary = result[\"messages\"][-1].content\nsources = result[\"urls_visited\"]\n\nprint(\"Web Search Summary:\")\nprint(final_summary)\nprint(\"Sources:\", sources)\n</code></pre>"},{"location":"web_search_agent/#parameters","title":"Parameters","text":""},{"location":"web_search_agent/#initialization","title":"Initialization","text":"Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> init_chat_model(\"openai:gpt-5-mini\") The language model to use for web search <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters passed to the base agent"},{"location":"web_search_agent/#run-method","title":"Run Method","text":"Parameter Type Default Description <code>prompt</code> str Required The web search question or topic <code>recursion_limit</code> int 100 Maximum recursion depth for the web search process"},{"location":"web_search_agent/#features","title":"Features","text":"<ul> <li>Automated Web Search: Uses DuckDuckGo to find relevant information</li> <li>Content Processing: Extracts and summarizes content from web pages</li> <li>Iterative Web Search: Continues researching until sufficient information is gathered</li> <li>Source Tracking: Records all URLs visited during research</li> <li>Internet Connectivity Check: Verifies internet access before attempting research</li> </ul>"},{"location":"web_search_agent/#output","title":"Output","text":"<p>The agent returns a dictionary containing:</p> <ul> <li><code>messages</code>: A list of message objects, with the final message containing the comprehensive web search summary</li> <li><code>urls_visited</code>: A list of all sources consulted during the web search process</li> </ul>"},{"location":"web_search_agent/#advanced-usage","title":"Advanced Usage","text":"<pre><code>from langchain.chat_model import init_chat_model\nfrom ursa.agents import WebSearchAgent\n\n# Initialize with custom parameters\nwebsearcher = WebSearchAgent(\n    llm=init_chat_model(\"openai:gpt-5-mini\"),\n    url=\"https://www.example.com\"  # Custom URL for internet connectivity check\n)\n\n# Run with higher recursion limit for complex topics\nresult = websearcher.invoke(\n    \"What are the latest developments in quantum computing? Summarize in markdown format.\",\n    recursion_limit=200\n)\n</code></pre>"},{"location":"web_search_agent/#notes","title":"Notes","text":"<ul> <li>The agent requires internet connectivity to function properly</li> <li>Rate limiting is implemented to avoid overwhelming search services</li> <li>For networks with SSL inspection, you may need to set the <code>CERT_FILE</code> environment variable</li> <li>The websearch process includes multiple steps: search, content processing, review, and final summarization</li> </ul>"},{"location":"api_reference/agents/","title":"agents","text":""},{"location":"api_reference/agents/#ursa.agents.acquisition_agents","title":"<code>acquisition_agents</code>","text":""},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.ArxivAgent","title":"<code>ArxivAgent</code>","text":"<p>               Bases: <code>BaseAcquisitionAgent</code></p> <p>Drop-in replacement for your existing ArxivAgent that reuses the generic flow. Keeps the same behaviors (download PDFs, image processing, summarization/RAG).</p> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class ArxivAgent(BaseAcquisitionAgent):\n    \"\"\"\n    Drop-in replacement for your existing ArxivAgent that reuses the generic flow.\n    Keeps the same behaviors (download PDFs, image processing, summarization/RAG).\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        *,\n        process_images: bool = True,\n        max_results: int = 3,\n        download: bool = True,\n        rag_embedding=None,\n        database_path=\"arxiv_papers\",\n        summaries_path=\"arxiv_generated_summaries\",\n        vectorstore_path=\"arxiv_vectorstores\",\n        **kwargs,\n    ):\n        super().__init__(\n            llm,\n            rag_embedding=rag_embedding,\n            process_images=process_images,\n            max_results=max_results,\n            database_path=database_path,\n            summaries_path=summaries_path,\n            vectorstore_path=vectorstore_path,\n            download=download,\n            **kwargs,\n        )\n\n    def _id(self, hit_or_item: Dict[str, Any]) -&gt; str:\n        # hits from arXiv feed have 'id' like \".../abs/XXXX.YYYY\"\n        arxiv_id = hit_or_item.get(\"arxiv_id\")\n        if arxiv_id:\n            return arxiv_id\n        feed_id = hit_or_item.get(\"id\", \"\")\n        if \"/abs/\" in feed_id:\n            return feed_id.split(\"/abs/\")[-1]\n        return _hash(json.dumps(hit_or_item))\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        return f\"ArXiv ID: {item.get('id', '?')}\"\n\n    def _search(self, query: str) -&gt; List[Dict[str, Any]]:\n        enc = quote(query)\n        url = f\"http://export.arxiv.org/api/query?search_query=all:{enc}&amp;start=0&amp;max_results={self.max_results}\"\n        try:\n            resp = requests.get(url, timeout=15)\n            resp.raise_for_status()\n            feed = feedparser.parse(resp.content)\n            entries = feed.entries if hasattr(feed, \"entries\") else []\n            hits = []\n            for e in entries:\n                full_id = e.id.split(\"/abs/\")[-1]\n                hits.append({\n                    \"id\": e.id,\n                    \"title\": e.title.strip(),\n                    \"arxiv_id\": full_id.split(\"/\")[-1],\n                })\n            return hits\n        except Exception as e:\n            return [\n                {\n                    \"id\": _hash(query + str(time.time())),\n                    \"title\": \"Search error\",\n                    \"error\": str(e),\n                }\n            ]\n\n    def _materialize(self, hit: Dict[str, Any]) -&gt; ItemMetadata:\n        arxiv_id = self._id(hit)\n        title = hit.get(\"title\", \"\")\n        pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n        local_path = os.path.join(self.database_path, f\"{arxiv_id}.pdf\")\n        full_text = \"\"\n        try:\n            _download(pdf_url, local_path)\n            full_text = _load_pdf_text(local_path)\n        except Exception as e:\n            full_text = f\"[Error loading ArXiv {arxiv_id}: {e}]\"\n        full_text = self._postprocess_text(full_text, local_path)\n        return {\n            \"id\": arxiv_id,\n            \"title\": title,\n            \"url\": pdf_url,\n            \"local_path\": local_path,\n            \"full_text\": full_text,\n        }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.BaseAcquisitionAgent","title":"<code>BaseAcquisitionAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>A generic \"acquire-then-summarize-or-RAG\" agent.</p> Subclasses must implement <ul> <li>_search(self, query) -&gt; List[dict-like]: lightweight hits</li> <li>_materialize(self, hit) -&gt; ItemMetadata: download or scrape and return populated item</li> <li>_id(self, hit_or_item) -&gt; str: stable id for caching/file naming</li> <li>_citation(self, item) -&gt; str: human-readable citation string</li> </ul> Optional hooks <ul> <li>_postprocess_text(self, text, local_path) -&gt; str (e.g., image interpretation)</li> <li>_filter_hit(self, hit) -&gt; bool</li> </ul> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class BaseAcquisitionAgent(BaseAgent):\n    \"\"\"\n    A generic \"acquire-then-summarize-or-RAG\" agent.\n\n    Subclasses must implement:\n      - _search(self, query) -&gt; List[dict-like]: lightweight hits\n      - _materialize(self, hit) -&gt; ItemMetadata: download or scrape and return populated item\n      - _id(self, hit_or_item) -&gt; str: stable id for caching/file naming\n      - _citation(self, item) -&gt; str: human-readable citation string\n\n    Optional hooks:\n      - _postprocess_text(self, text, local_path) -&gt; str (e.g., image interpretation)\n      - _filter_hit(self, hit) -&gt; bool\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        *,\n        summarize: bool = True,\n        rag_embedding=None,\n        process_images: bool = True,\n        max_results: int = 5,\n        database_path: str = \"acq_db\",\n        summaries_path: str = \"acq_summaries\",\n        vectorstore_path: str = \"acq_vectorstores\",\n        download: bool = True,\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.summarize = summarize\n        self.rag_embedding = rag_embedding\n        self.process_images = process_images\n        self.max_results = max_results\n        self.database_path = database_path\n        self.summaries_path = summaries_path\n        self.vectorstore_path = vectorstore_path\n        self.download = download\n\n        os.makedirs(self.database_path, exist_ok=True)\n        os.makedirs(self.summaries_path, exist_ok=True)\n\n        self._action = self._build_graph()\n\n    # ---- abstract-ish methods ----\n    def _search(self, query: str) -&gt; List[Dict[str, Any]]:\n        raise NotImplementedError\n\n    def _materialize(self, hit: Dict[str, Any]) -&gt; ItemMetadata:\n        raise NotImplementedError\n\n    def _id(self, hit_or_item: Dict[str, Any]) -&gt; str:\n        raise NotImplementedError\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        # Subclass should format its ideal citation; fallback is ID or URL.\n        return item.get(\"id\") or item.get(\"url\", \"Unknown Source\")\n\n    # ---- optional hooks ----\n    def _filter_hit(self, hit: Dict[str, Any]) -&gt; bool:\n        return True\n\n    def _postprocess_text(self, text: str, local_path: Optional[str]) -&gt; str:\n        # Default: optionally add image descriptions for PDFs\n        if (\n            self.process_images\n            and local_path\n            and local_path.lower().endswith(\".pdf\")\n        ):\n            try:\n                descs = extract_and_describe_images(local_path)\n                if any(descs):\n                    text += \"\\n\\n[Image Interpretations]\\n\" + \"\\n\".join(descs)\n            except Exception:\n                pass\n        return text\n\n    # ---- shared nodes ----\n    def _fetch_items(self, query: str) -&gt; List[ItemMetadata]:\n        hits = self._search(query)[: self.max_results] if self.download else []\n        items: List[ItemMetadata] = []\n\n        # If not downloading/scraping, try to load whatever is cached in database_path.\n        if not self.download:\n            for fname in os.listdir(self.database_path):\n                if fname.lower().endswith((\".pdf\", \".txt\", \".html\")):\n                    item_id = os.path.splitext(fname)[0]\n                    local_path = os.path.join(self.database_path, fname)\n                    full_text = \"\"\n                    try:\n                        if fname.lower().endswith(\".pdf\"):\n                            full_text = _load_pdf_text(local_path)\n                        else:\n                            with open(\n                                local_path,\n                                \"r\",\n                                encoding=\"utf-8\",\n                                errors=\"ignore\",\n                            ) as f:\n                                full_text = f.read()\n                    except Exception as e:\n                        full_text = f\"[Error reading cached file: {e}]\"\n                    full_text = self._postprocess_text(full_text, local_path)\n                    items.append({\n                        \"id\": item_id,\n                        \"local_path\": local_path,\n                        \"full_text\": full_text,\n                    })\n            return items\n\n        # Normal path: search \u2192 materialize each\n        with ThreadPoolExecutor(max_workers=min(32, max(1, len(hits)))) as ex:\n            futures = [\n                ex.submit(self._materialize, h)\n                for h in hits\n                if self._filter_hit(h)\n            ]\n            for fut in as_completed(futures):\n                try:\n                    item = fut.result()\n                    items.append(item)\n                except Exception as e:\n                    items.append({\n                        \"id\": _hash(str(time.time())),\n                        \"full_text\": f\"[Error: {e}]\",\n                    })\n        return items\n\n    def _fetch_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        items = self._fetch_items(state[\"query\"])\n        return {**state, \"items\": items}\n\n    def _summarize_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an assistant responsible for summarizing retrieved content in the context of this task: {context}\n\n        Summarize the content below:\n\n        {retrieved_content}\n        \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n\n        if \"items\" not in state or not state[\"items\"]:\n            return {**state, \"summaries\": None}\n\n        summaries: List[Optional[str]] = [None] * len(state[\"items\"])\n\n        def process(i: int, item: ItemMetadata):\n            item_id = item.get(\"id\", f\"item_{i}\")\n            out_path = os.path.join(\n                self.summaries_path, f\"{_safe_filename(item_id)}_summary.txt\"\n            )\n            try:\n                cleaned = remove_surrogates(item.get(\"full_text\", \"\"))\n                summary = chain.invoke(\n                    {\"retrieved_content\": cleaned, \"context\": state[\"context\"]},\n                    config=self.build_config(tags=[\"acq\", \"summarize_each\"]),\n                )\n            except Exception as e:\n                summary = f\"[Error summarizing item {item_id}: {e}]\"\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(summary)\n            return i, summary\n\n        with ThreadPoolExecutor(max_workers=min(32, len(state[\"items\"]))) as ex:\n            futures = [\n                ex.submit(process, i, it) for i, it in enumerate(state[\"items\"])\n            ]\n            for fut in as_completed(futures):\n                i, s = fut.result()\n                summaries[i] = s\n\n        return {**state, \"summaries\": summaries}  # type: ignore\n\n    def _rag_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        new_state = state.copy()\n        rag_agent = RAGAgent(\n            llm=self.llm,\n            embedding=self.rag_embedding,\n            database_path=self.database_path,\n        )\n        new_state[\"final_summary\"] = rag_agent.invoke(context=state[\"context\"])[\n            \"summary\"\n        ]\n        return new_state\n\n    def _aggregate_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        if not state.get(\"summaries\") or not state.get(\"items\"):\n            return {**state, \"final_summary\": None}\n\n        blocks: List[str] = []\n        for idx, (item, summ) in enumerate(\n            zip(state[\"items\"], state[\"summaries\"])\n        ):  # type: ignore\n            cite = self._citation(item)\n            blocks.append(f\"[{idx + 1}] {cite}\\n\\nSummary:\\n{summ}\")\n\n        combined = \"\\n\\n\" + (\"\\n\\n\" + \"-\" * 40 + \"\\n\\n\").join(blocks)\n        with open(\n            os.path.join(self.summaries_path, \"summaries_combined.txt\"),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            f.write(combined)\n\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a scientific assistant extracting insights from multiple summaries.\n\n        Here are the summaries:\n\n        {Summaries}\n\n        Your task is to read all the summaries and provide a response to this task: {context}\n        \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n\n        final_summary = chain.invoke(\n            {\"Summaries\": combined, \"context\": state[\"context\"]},\n            config=self.build_config(tags=[\"acq\", \"aggregate\"]),\n        )\n        with open(\n            os.path.join(self.summaries_path, \"final_summary.txt\"),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            f.write(final_summary)\n\n        return {**state, \"final_summary\": final_summary}\n\n    def _build_graph(self):\n        graph = StateGraph(AcquisitionState)\n        self.add_node(graph, self._fetch_node)\n\n        if self.summarize:\n            if self.rag_embedding:\n                self.add_node(graph, self._rag_node)\n                graph.set_entry_point(\"_fetch_node\")\n                graph.add_edge(\"_fetch_node\", \"_rag_node\")\n                graph.set_finish_point(\"_rag_node\")\n            else:\n                self.add_node(graph, self._summarize_node)\n                self.add_node(graph, self._aggregate_node)\n\n                graph.set_entry_point(\"_fetch_node\")\n                graph.add_edge(\"_fetch_node\", \"_summarize_node\")\n                graph.add_edge(\"_summarize_node\", \"_aggregate_node\")\n                graph.set_finish_point(\"_aggregate_node\")\n        else:\n            graph.set_entry_point(\"_fetch_node\")\n            graph.set_finish_point(\"_fetch_node\")\n\n        return graph.compile(checkpointer=self.checkpointer)\n\n    def _invoke(\n        self,\n        inputs: Mapping[str, Any],\n        *,\n        summarize: bool | None = None,\n        recursion_limit: int = 1000,\n        **_,\n    ) -&gt; str:\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n\n        # alias support like your ArxivAgent\n        if \"query\" not in inputs:\n            if \"arxiv_search_query\" in inputs:\n                inputs = dict(inputs)\n                inputs[\"query\"] = inputs.pop(\"arxiv_search_query\")\n            else:\n                raise KeyError(\n                    \"Missing 'query' in inputs (alias 'arxiv_search_query' also accepted).\"\n                )\n\n        result = self._action.invoke(inputs, config)\n        use_summary = self.summarize if summarize is None else summarize\n        return (\n            result.get(\"final_summary\", \"No summary generated.\")\n            if use_summary\n            else \"\\n\\nFinished fetching items!\"\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.OSTIAgent","title":"<code>OSTIAgent</code>","text":"<p>               Bases: <code>BaseAcquisitionAgent</code></p> <p>Minimal OSTI.gov acquisition agent.</p> NOTE <ul> <li>OSTI provides search endpoints that can return metadata including full-text links.</li> <li>Depending on your environment, you may prefer the public API or site scraping.</li> <li>Here we assume a JSON API that yields results with keys like:       {'osti_id': '12345', 'title': '...', 'pdf_url': 'https://...pdf', 'landing_page': 'https://...'}   Adapt field names if your OSTI integration differs.</li> </ul> <p>Customize <code>_search</code> and <code>_materialize</code> to match your OSTI access path.</p> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class OSTIAgent(BaseAcquisitionAgent):\n    \"\"\"\n    Minimal OSTI.gov acquisition agent.\n\n    NOTE:\n      - OSTI provides search endpoints that can return metadata including full-text links.\n      - Depending on your environment, you may prefer the public API or site scraping.\n      - Here we assume a JSON API that yields results with keys like:\n            {'osti_id': '12345', 'title': '...', 'pdf_url': 'https://...pdf', 'landing_page': 'https://...'}\n        Adapt field names if your OSTI integration differs.\n\n    Customize `_search` and `_materialize` to match your OSTI access path.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        api_base: str = \"https://www.osti.gov/api/v1/records\",\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.api_base = api_base\n\n    def _id(self, hit_or_item: Dict[str, Any]) -&gt; str:\n        if \"osti_id\" in hit_or_item:\n            return str(hit_or_item[\"osti_id\"])\n        if \"id\" in hit_or_item:\n            return str(hit_or_item[\"id\"])\n        if \"landing_page\" in hit_or_item:\n            return _hash(hit_or_item[\"landing_page\"])\n        return _hash(json.dumps(hit_or_item))\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        t = item.get(\"title\", \"\") or \"\"\n        oid = item.get(\"id\", \"\")\n        return f\"OSTI {oid}: {t}\" if t else f\"OSTI {oid}\"\n\n    def _search(self, query: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Adjust params to your OSTI setup. This call is intentionally simple;\n        add paging/auth as needed.\n        \"\"\"\n        params = {\n            \"q\": query,\n            \"size\": self.max_results,\n        }\n        try:\n            r = requests.get(self.api_base, params=params, timeout=25)\n            r.raise_for_status()\n            data = r.json()\n            # Normalize to a list of hits; adapt key if your API differs.\n            if isinstance(data, dict) and \"records\" in data:\n                hits = data[\"records\"]\n            elif isinstance(data, list):\n                hits = data\n            else:\n                hits = []\n            return hits[: self.max_results]\n        except Exception as e:\n            return [\n                {\n                    \"id\": _hash(query + str(time.time())),\n                    \"title\": \"Search error\",\n                    \"error\": str(e),\n                }\n            ]\n\n    def _materialize(self, hit: Dict[str, Any]) -&gt; ItemMetadata:\n        item_id = self._id(hit)\n        title = hit.get(\"title\") or hit.get(\"title_public\", \"\") or \"\"\n        landing = None\n        local_path = \"\"\n        full_text = \"\"\n\n        try:\n            pdf_url, landing_used, _ = resolve_pdf_from_osti_record(\n                hit,\n                headers={\"User-Agent\": \"Mozilla/5.0\"},\n                unpaywall_email=os.environ.get(\"UNPAYWALL_EMAIL\"),  # optional\n            )\n\n            if pdf_url:\n                # Try to download as PDF (validate headers)\n                with requests.get(\n                    pdf_url,\n                    headers={\"User-Agent\": \"Mozilla/5.0\"},\n                    timeout=25,\n                    allow_redirects=True,\n                    stream=True,\n                ) as r:\n                    r.raise_for_status()\n                    if _is_pdf_response(r):\n                        fname = _derive_filename_from_cd_or_url(\n                            r, f\"osti_{item_id}.pdf\"\n                        )\n                        local_path = os.path.join(self.database_path, fname)\n                        _download_stream_to(local_path, r)\n                        # Extract PDF text\n                        try:\n                            from langchain_community.document_loaders import (\n                                PyPDFLoader,\n                            )\n\n                            loader = PyPDFLoader(local_path)\n                            pages = loader.load()\n                            full_text = \"\\n\".join(p.page_content for p in pages)\n                        except Exception as e:\n                            full_text = (\n                                f\"[Downloaded but text extraction failed: {e}]\"\n                            )\n                    else:\n                        # Not a PDF; treat as HTML landing and parse text\n                        landing = r.url\n                        r.close()\n            # If we still have no text, try scraping the DOE PAGES landing or citation page\n            if not full_text:\n                # Prefer DOE PAGES landing if present, else OSTI biblio\n                landing = (\n                    landing\n                    or landing_used\n                    or next(\n                        (\n                            link.get(\"href\")\n                            for link in hit.get(\"links\", [])\n                            if link.get(\"rel\")\n                            in (\"citation_doe_pages\", \"citation\")\n                        ),\n                        None,\n                    )\n                )\n                if landing:\n                    soup = _get_soup(\n                        landing,\n                        timeout=25,\n                        headers={\"User-Agent\": \"Mozilla/5.0\"},\n                    )\n                    html_text = soup.get_text(\" \", strip=True)\n                    full_text = html_text[:1_000_000]  # keep it bounded\n                    # Save raw HTML for cache/inspection\n                    local_path = os.path.join(\n                        self.database_path, f\"{item_id}.html\"\n                    )\n                    with open(local_path, \"w\", encoding=\"utf-8\") as f:\n                        f.write(str(soup))\n                else:\n                    full_text = \"[No PDF or landing page text available.]\"\n\n        except Exception as e:\n            full_text = f\"[Error materializing OSTI {item_id}: {e}]\"\n\n        full_text = self._postprocess_text(full_text, local_path)\n        return {\n            \"id\": item_id,\n            \"title\": title,\n            \"url\": landing,\n            \"local_path\": local_path,\n            \"full_text\": full_text,\n            \"extra\": {\"raw_hit\": hit},\n        }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.WebSearchAgent","title":"<code>WebSearchAgent</code>","text":"<p>               Bases: <code>BaseAcquisitionAgent</code></p> <p>Uses DuckDuckGo Search (ddgs) to find pages, downloads HTML or PDFs, extracts text, and then follows the same summarize/RAG path.</p> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class WebSearchAgent(BaseAcquisitionAgent):\n    \"\"\"\n    Uses DuckDuckGo Search (ddgs) to find pages, downloads HTML or PDFs,\n    extracts text, and then follows the same summarize/RAG path.\n    \"\"\"\n\n    def __init__(self, *args, user_agent: str = \"Mozilla/5.0\", **kwargs):\n        super().__init__(*args, **kwargs)\n        self.user_agent = user_agent\n        if DDGS is None:\n            raise ImportError(\n                \"duckduckgo-search (DDGS) is required for WebSearchAgentGeneric.\"\n            )\n\n    def _id(self, hit_or_item: Dict[str, Any]) -&gt; str:\n        url = hit_or_item.get(\"href\") or hit_or_item.get(\"url\") or \"\"\n        return (\n            _hash(url)\n            if url\n            else hit_or_item.get(\"id\", _hash(json.dumps(hit_or_item)))\n        )\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        t = item.get(\"title\", \"\") or \"\"\n        u = item.get(\"url\", \"\") or \"\"\n        return f\"{t} ({u})\" if t else (u or item.get(\"id\", \"Web result\"))\n\n    def _search(self, query: str) -&gt; List[Dict[str, Any]]:\n        results: List[Dict[str, Any]] = []\n        with DDGS() as ddgs:\n            for r in ddgs.text(\n                query, max_results=self.max_results, backend=\"duckduckgo\"\n            ):\n                # r keys typically: title, href, body\n                results.append(r)\n        return results\n\n    def _materialize(self, hit: Dict[str, Any]) -&gt; ItemMetadata:\n        url = hit.get(\"href\") or hit.get(\"url\")\n        title = hit.get(\"title\", \"\")\n        if not url:\n            return {\"id\": self._id(hit), \"title\": title, \"full_text\": \"\"}\n\n        headers = {\"User-Agent\": self.user_agent}\n        local_path = \"\"\n        full_text = \"\"\n        item_id = self._id(hit)\n\n        try:\n            if _looks_like_pdf_url(url):\n                local_path = os.path.join(\n                    self.database_path, _safe_filename(item_id) + \".pdf\"\n                )\n                _download(url, local_path)\n                full_text = _load_pdf_text(local_path)\n            else:\n                r = requests.get(url, headers=headers, timeout=20)\n                r.raise_for_status()\n                html = r.text\n                local_path = os.path.join(\n                    self.database_path, _safe_filename(item_id) + \".html\"\n                )\n                with open(local_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html)\n                full_text = extract_main_text_only(html)\n                # full_text = _basic_readable_text_from_html(html)\n        except Exception as e:\n            full_text = f\"[Error retrieving {url}: {e}]\"\n\n        full_text = self._postprocess_text(full_text, local_path)\n        return {\n            \"id\": item_id,\n            \"title\": title,\n            \"url\": url,\n            \"local_path\": local_path,\n            \"full_text\": full_text,\n            \"extra\": {\"snippet\": hit.get(\"body\", \"\")},\n        }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base","title":"<code>base</code>","text":"<p>Base agent class providing telemetry, configuration, and execution abstractions.</p> <p>This module defines the BaseAgent abstract class, which serves as the foundation for all agent implementations in the Ursa framework. It provides:</p> <ul> <li>Standardized initialization with LLM configuration</li> <li>Telemetry and metrics collection</li> <li>Thread and checkpoint management</li> <li>Input normalization and validation</li> <li>Execution flow control with invoke/stream methods</li> <li>Graph integration utilities for LangGraph compatibility</li> <li>Runtime enforcement of the agent interface contract</li> </ul> <p>Agents built on this base class benefit from consistent behavior, observability, and integration capabilities while only needing to implement the core _invoke method.</p>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent","title":"<code>BaseAgent</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all agent implementations in the Ursa framework.</p> <p>BaseAgent provides a standardized foundation for building LLM-powered agents with built-in telemetry, configuration management, and execution flow control. It handles common tasks like input normalization, thread management, metrics collection, and LangGraph integration.</p> <p>Subclasses only need to implement the _invoke method to define their core functionality, while inheriting standardized invocation patterns, telemetry, and graph integration capabilities. The class enforces a consistent interface through runtime checks that prevent subclasses from overriding critical methods like invoke().</p> <p>The agent supports both direct invocation with inputs and streaming responses, with automatic tracking of token usage, execution time, and other metrics. It also provides utilities for integrating with LangGraph through node wrapping and configuration.</p> Subclass Inheritance Guidelines <ul> <li>Must Override: _invoke() - Define your agent's core functionality</li> <li>Can Override: _stream() - Enable streaming support                 _normalize_inputs() - Customize input handling                 Various helper methods (_default_node_tags, _as_runnable, etc.)</li> <li>Never Override: invoke() - Final method with runtime enforcement                   stream() - Handles telemetry and delegates to _stream                   call() - Delegates to invoke                   Other public methods (build_config, write_state, add_node)</li> </ul> <p>To create a custom agent, inherit from this class and implement the _invoke method:</p> <pre><code>class MyAgent(BaseAgent):\n    def _invoke(self, inputs: Mapping[str, Any], **config: Any) -&gt; Any:\n        # Process inputs and return results\n        ...\n</code></pre> Source code in <code>src/ursa/agents/base.py</code> <pre><code>class BaseAgent(ABC):\n    \"\"\"Abstract base class for all agent implementations in the Ursa framework.\n\n    BaseAgent provides a standardized foundation for building LLM-powered agents with\n    built-in telemetry, configuration management, and execution flow control. It handles\n    common tasks like input normalization, thread management, metrics collection, and\n    LangGraph integration.\n\n    Subclasses only need to implement the _invoke method to define their core\n    functionality, while inheriting standardized invocation patterns, telemetry, and\n    graph integration capabilities. The class enforces a consistent interface through\n    runtime checks that prevent subclasses from overriding critical methods like\n    invoke().\n\n    The agent supports both direct invocation with inputs and streaming responses, with\n    automatic tracking of token usage, execution time, and other metrics. It also\n    provides utilities for integrating with LangGraph through node wrapping and\n    configuration.\n\n    Subclass Inheritance Guidelines:\n        - Must Override: _invoke() - Define your agent's core functionality\n        - Can Override: _stream() - Enable streaming support\n                        _normalize_inputs() - Customize input handling\n                        Various helper methods (_default_node_tags, _as_runnable, etc.)\n        - Never Override: invoke() - Final method with runtime enforcement\n                          stream() - Handles telemetry and delegates to _stream\n                          __call__() - Delegates to invoke\n                          Other public methods (build_config, write_state, add_node)\n\n    To create a custom agent, inherit from this class and implement the _invoke method:\n\n    ```python\n    class MyAgent(BaseAgent):\n        def _invoke(self, inputs: Mapping[str, Any], **config: Any) -&gt; Any:\n            # Process inputs and return results\n            ...\n    ```\n    \"\"\"\n\n    # This will be shared across all BaseAgent instances.\n    _invoke_depth: int = 0\n\n    _TELEMETRY_KW = {\n        \"raw_debug\",\n        \"save_json\",\n        \"metrics_path\",\n        \"save_raw_snapshot\",\n        \"save_raw_records\",\n    }\n\n    _CONTROL_KW = {\"config\", \"recursion_limit\", \"tags\", \"metadata\", \"callbacks\"}\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        checkpointer: Optional[BaseCheckpointSaver] = None,\n        enable_metrics: bool = True,\n        metrics_dir: str = \"ursa_metrics\",  # dir to save metrics, with a default\n        autosave_metrics: bool = True,\n        thread_id: Optional[str] = None,\n    ):\n        self.llm = llm\n        \"\"\"Initializes the base agent with a language model and optional configurations.\n\n        Args:\n            llm: a BaseChatModel instance.\n            checkpointer: Optional checkpoint saver for persisting agent state.\n            enable_metrics: Whether to collect performance and usage metrics.\n            metrics_dir: Directory path where metrics will be saved.\n            autosave_metrics: Whether to automatically save metrics to disk.\n            thread_id: Unique identifier for this agent instance. Generated if not\n                       provided.\n        \"\"\"\n        self.thread_id = thread_id or uuid4().hex\n        self.checkpointer = checkpointer\n        self.telemetry = Telemetry(\n            enable=enable_metrics,\n            output_dir=metrics_dir,\n            save_json_default=autosave_metrics,\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Agent name.\"\"\"\n        return self.__class__.__name__\n\n    def add_node(\n        self,\n        graph: StateGraph,\n        f: Callable[..., Mapping[str, Any]],\n        node_name: Optional[str] = None,\n        agent_name: Optional[str] = None,\n    ) -&gt; StateGraph:\n        \"\"\"Add a node to the state graph with token usage tracking.\n\n        This method adds a function as a node to the state graph, wrapping it to track\n        token usage during execution. The node is identified by either the provided\n        node_name or the function's name.\n\n        Args:\n            graph: The StateGraph to add the node to.\n            f: The function to add as a node. Should return a mapping of string keys to\n                any values.\n            node_name: Optional name for the node. If not provided, the function's name\n                will be used.\n            agent_name: Optional agent name for tracking. If not provided, the agent's\n                name in snake_case will be used.\n\n        Returns:\n            The updated StateGraph with the new node added.\n        \"\"\"\n        _node_name = node_name or f.__name__\n        _agent_name = agent_name or _to_snake(self.name)\n        wrapped_node = self._wrap_node(f, _node_name, _agent_name)\n\n        return graph.add_node(_node_name, wrapped_node)\n\n    def write_state(self, filename: str, state: dict) -&gt; None:\n        \"\"\"Writes agent state to a JSON file.\n\n        Serializes the provided state dictionary to JSON format and writes it to the\n        specified file. The JSON is written with non-ASCII characters preserved.\n\n        Args:\n            filename: Path to the file where state will be written.\n            state: Dictionary containing the agent state to be serialized.\n        \"\"\"\n        json_state = dumps(state, ensure_ascii=False)\n        with open(filename, \"w\") as f:\n            f.write(json_state)\n\n    def build_config(self, **overrides) -&gt; dict:\n        \"\"\"Constructs a config dictionary for agent operations with telemetry support.\n\n        This method creates a standardized configuration dictionary that includes thread\n        identification, telemetry callbacks, and other metadata needed for agent\n        operations. The configuration can be customized through override parameters.\n\n        Args:\n            **overrides: Optional configuration overrides that can include keys like\n                'recursion_limit', 'configurable', 'metadata', 'tags', etc.\n\n        Returns:\n            dict: A complete configuration dictionary with all necessary parameters.\n        \"\"\"\n        # Create the base configuration with essential fields.\n        base = {\n            \"configurable\": {\"thread_id\": self.thread_id},\n            \"metadata\": {\n                \"thread_id\": self.thread_id,\n                \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n            },\n            \"tags\": [self.name],\n            \"callbacks\": self.telemetry.callbacks,\n        }\n\n        # Try to determine the model name from either direct or nested attributes\n        model_name = getattr(self, \"llm_model\", None) or getattr(\n            getattr(self, \"llm\", None), \"model\", None\n        )\n\n        # Add model name to metadata if available\n        if model_name:\n            base[\"metadata\"][\"model\"] = model_name\n\n        # Handle configurable dictionary overrides by merging with base configurable\n        if \"configurable\" in overrides and isinstance(\n            overrides[\"configurable\"], dict\n        ):\n            base[\"configurable\"].update(overrides.pop(\"configurable\"))\n\n        # Handle metadata dictionary overrides by merging with base metadata\n        if \"metadata\" in overrides and isinstance(overrides[\"metadata\"], dict):\n            base[\"metadata\"].update(overrides.pop(\"metadata\"))\n\n        # Merge tags from caller-provided overrides, avoid duplicates\n        if \"tags\" in overrides and isinstance(overrides[\"tags\"], list):\n            base[\"tags\"] = base[\"tags\"] + [\n                t for t in overrides.pop(\"tags\") if t not in base[\"tags\"]\n            ]\n\n        # Apply any remaining overrides directly to the base configuration\n        base.update(overrides)\n\n        return base\n\n    def _invoke_engine(\n        self,\n        invoke_method,\n        inputs: Optional[InputLike] = None,\n        raw_debug: bool = False,\n        save_json: Optional[bool] = None,\n        metrics_path: Optional[str] = None,\n        save_raw_snapshot: Optional[bool] = None,\n        save_raw_records: Optional[bool] = None,\n        config: Optional[dict] = None,\n        **kwargs: Any,\n    ):\n        BaseAgent._invoke_depth += 1\n\n        try:\n            # Start telemetry tracking for the top-level invocation\n            if BaseAgent._invoke_depth == 1:\n                self.telemetry.begin_run(\n                    agent=self.name, thread_id=self.thread_id\n                )\n\n            # Handle the case where inputs are provided as keyword arguments\n            if inputs is None:\n                # Separate kwargs into input parameters and control parameters\n                kw_inputs: dict[str, Any] = {}\n                control_kwargs: dict[str, Any] = {}\n                for k, v in kwargs.items():\n                    if k in self._TELEMETRY_KW or k in self._CONTROL_KW:\n                        control_kwargs[k] = v\n                    else:\n                        kw_inputs[k] = v\n                inputs = kw_inputs\n\n                # Only control kwargs remain for further processing\n                kwargs = control_kwargs\n\n            # Handle the case where inputs are provided as a positional argument\n            else:\n                # Ensure no ambiguous keyword arguments are present\n                for k in kwargs.keys():\n                    if not (k in self._TELEMETRY_KW or k in self._CONTROL_KW):\n                        raise TypeError(\n                            f\"Unexpected keyword argument '{k}'. \"\n                            \"Pass inputs as a single mapping or omit the positional \"\n                            \"inputs and pass them as keyword arguments.\"\n                        )\n\n            # Allow subclasses to normalize or transform the input format\n            normalized = self._normalize_inputs(inputs)\n\n            # Delegate to the subclass implementation with the normalized inputs\n            # and any control parameters\n            return invoke_method(normalized, config=config, **kwargs)\n\n        finally:\n            # Clean up the invocation depth tracking\n            BaseAgent._invoke_depth -= 1\n\n            # For the top-level invocation, finalize telemetry and generate outputs\n            if BaseAgent._invoke_depth == 0:\n                self.telemetry.render(\n                    raw=raw_debug,\n                    save_json=save_json,\n                    filepath=metrics_path,\n                    save_raw_snapshot=save_raw_snapshot,\n                    save_raw_records=save_raw_records,\n                )\n\n    # NOTE: The `invoke` method uses the PEP 570 `/,*` notation to explicitly state which\n    # arguments can and cannot be passed as positional or keyword arguments.\n    @final\n    def invoke(\n        self,\n        inputs: Optional[InputLike] = None,\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: Optional[bool] = None,\n        metrics_path: Optional[str] = None,\n        save_raw_snapshot: Optional[bool] = None,\n        save_raw_records: Optional[bool] = None,\n        config: Optional[dict] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Executes the agent with the provided inputs and configuration.\n\n        This is the main entry point for agent execution. It handles input normalization,\n        telemetry tracking, and proper execution context management. The method supports\n        flexible input formats - either as a positional argument or as keyword arguments.\n\n        Args:\n            inputs: Optional positional input to the agent. If provided, all non-control\n                keyword arguments will be rejected to avoid ambiguity.\n            raw_debug: If True, displays raw telemetry data for debugging purposes.\n            save_json: If True, saves telemetry data as JSON.\n            metrics_path: Optional file path where telemetry metrics should be saved.\n            save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n            save_raw_records: If True, saves raw telemetry records.\n            config: Optional configuration dictionary to override default settings.\n            **kwargs: Additional keyword arguments that can be either:\n                - Input parameters (when no positional input is provided)\n                - Control parameters recognized by the agent\n\n        Returns:\n            The result of the agent's execution.\n\n        Raises:\n            TypeError: If both positional inputs and non-control keyword arguments are\n                provided simultaneously.\n        \"\"\"\n        return self._invoke_engine(\n            invoke_method=self._invoke,\n            inputs=inputs,\n            raw_debug=raw_debug,\n            save_json=save_json,\n            metrics_path=metrics_path,\n            save_raw_snapshot=save_raw_snapshot,\n            save_raw_records=save_raw_records,\n            config=config,\n            **kwargs,\n        )\n\n    # NOTE: The `ainvoke` method uses the PEP 570 `/,*` notation to explicitly state which\n    # arguments can and cannot be passed as positional or keyword arguments.\n    @final\n    def ainvoke(\n        self,\n        inputs: Optional[InputLike] = None,\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: Optional[bool] = None,\n        metrics_path: Optional[str] = None,\n        save_raw_snapshot: Optional[bool] = None,\n        save_raw_records: Optional[bool] = None,\n        config: Optional[dict] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Asynchrnously executes the agent with the provided inputs and configuration.\n\n        (Async version of `invoke`.)\n\n        This is the main entry point for agent execution. It handles input normalization,\n        telemetry tracking, and proper execution context management. The method supports\n        flexible input formats - either as a positional argument or as keyword arguments.\n\n        Args:\n            inputs: Optional positional input to the agent. If provided, all non-control\n                keyword arguments will be rejected to avoid ambiguity.\n            raw_debug: If True, displays raw telemetry data for debugging purposes.\n            save_json: If True, saves telemetry data as JSON.\n            metrics_path: Optional file path where telemetry metrics should be saved.\n            save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n            save_raw_records: If True, saves raw telemetry records.\n            config: Optional configuration dictionary to override default settings.\n            **kwargs: Additional keyword arguments that can be either:\n                - Input parameters (when no positional input is provided)\n                - Control parameters recognized by the agent\n\n        Returns:\n            The result of the agent's execution.\n\n        Raises:\n            TypeError: If both positional inputs and non-control keyword arguments are\n                provided simultaneously.\n        \"\"\"\n        return self._invoke_engine(\n            invoke_method=self._ainvoke,\n            inputs=inputs,\n            raw_debug=raw_debug,\n            save_json=save_json,\n            metrics_path=metrics_path,\n            save_raw_snapshot=save_raw_snapshot,\n            save_raw_records=save_raw_records,\n            config=config,\n            **kwargs,\n        )\n\n    def _normalize_inputs(self, inputs: InputLike) -&gt; Mapping[str, Any]:\n        \"\"\"Normalizes various input formats into a standardized mapping.\n\n        This method converts different input types into a consistent dictionary format\n        that can be processed by the agent. String inputs are wrapped as messages, while\n        mappings are passed through unchanged.\n\n        Args:\n            inputs: The input to normalize. Can be a string (which will be converted to a\n                message) or a mapping (which will be returned as-is).\n\n        Returns:\n            A mapping containing the normalized inputs, with keys appropriate for agent\n            processing.\n\n        Raises:\n            TypeError: If the input type is not supported (neither string nor mapping).\n        \"\"\"\n        if isinstance(inputs, str):\n            # Adjust to your message type\n            return {\"messages\": [HumanMessage(content=inputs)]}\n        if isinstance(inputs, Mapping):\n            return inputs\n        raise TypeError(f\"Unsupported input type: {type(inputs)}\")\n\n    @abstractmethod\n    def _invoke(self, inputs: Mapping[str, Any], **config: Any) -&gt; Any:\n        \"\"\"Subclasses implement the actual work against normalized inputs.\"\"\"\n        ...\n\n    def _ainvoke(self, inputs: Mapping[str, Any], **config: Any) -&gt; Any:\n        \"\"\"Subclasses implement the actual work against normalized inputs.\"\"\"\n        ...\n\n    def __call__(self, inputs: InputLike, /, **kwargs: Any) -&gt; Any:\n        \"\"\"Specify calling behavior for class instance.\"\"\"\n        return self.invoke(inputs, **kwargs)\n\n    # Runtime enforcement: forbid subclasses from overriding invoke\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Ensure subclass does not override key method.\"\"\"\n        super().__init_subclass__(**kwargs)\n        if \"invoke\" in cls.__dict__:\n            err_msg = (\n                f\"{cls.__name__} must not override BaseAgent.invoke(); \"\n                \"implement _invoke() only.\"\n            )\n            raise TypeError(err_msg)\n\n    def stream(\n        self,\n        inputs: InputLike,\n        config: Any | None = None,  # allow positional/keyword like LangGraph\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: bool | None = None,\n        metrics_path: str | None = None,\n        save_raw_snapshot: bool | None = None,\n        save_raw_records: bool | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[Any]:\n        \"\"\"Streams agent responses with telemetry tracking.\n\n        This method serves as the public streaming entry point for agent interactions.\n        It wraps the actual streaming implementation with telemetry tracking to capture\n        metrics and debugging information.\n\n        Args:\n            inputs: The input to process, which will be normalized internally.\n            config: Optional configuration for the agent, compatible with LangGraph\n                positional/keyword argument style.\n            raw_debug: If True, renders raw debug information in telemetry output.\n            save_json: If True, saves telemetry data as JSON.\n            metrics_path: Optional file path where metrics should be saved.\n            save_raw_snapshot: If True, saves raw snapshot data in telemetry.\n            save_raw_records: If True, saves raw record data in telemetry.\n            **kwargs: Additional keyword arguments passed to the streaming\n                implementation.\n\n        Returns:\n            An iterator yielding the agent's responses.\n\n        Note:\n            This method tracks invocation depth to properly handle nested agent calls\n            and ensure telemetry is only rendered once at the top level.\n        \"\"\"\n        # Track invocation depth to handle nested agent calls\n        BaseAgent._invoke_depth += 1\n\n        try:\n            # Start telemetry tracking for top-level invocations only\n            if BaseAgent._invoke_depth == 1:\n                self.telemetry.begin_run(\n                    agent=self.name, thread_id=self.thread_id\n                )\n\n            # Normalize inputs and delegate to the actual streaming implementation\n            normalized = self._normalize_inputs(inputs)\n            yield from self._stream(normalized, config=config, **kwargs)\n\n        finally:\n            # Decrement invocation depth when exiting\n            BaseAgent._invoke_depth -= 1\n\n            # Render telemetry data only for top-level invocations\n            if BaseAgent._invoke_depth == 0:\n                self.telemetry.render(\n                    raw=raw_debug,\n                    save_json=save_json,\n                    filepath=metrics_path,\n                    save_raw_snapshot=save_raw_snapshot,\n                    save_raw_records=save_raw_records,\n                )\n\n    def _stream(\n        self,\n        inputs: Mapping[str, Any],\n        *,\n        config: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[Any]:\n        \"\"\"Subclass method to be overwritten for streaming implementation.\"\"\"\n        raise NotImplementedError(\n            f\"{self.name} does not support streaming. \"\n            \"Override _stream(...) in your agent to enable it.\"\n        )\n\n    def _default_node_tags(\n        self, name: str, extra: Sequence[str] | None = None\n    ) -&gt; list[str]:\n        \"\"\"Generate default tags for a graph node.\n\n        Args:\n            name: The name of the node.\n            extra: Optional sequence of additional tags to include.\n\n        Returns:\n            list[str]: A list of tags for the node, including the agent name, 'graph',\n                the node name, and any extra tags provided.\n        \"\"\"\n        # Start with standard tags: agent name, graph indicator, and node name\n        tags = [self.name, \"graph\", name]\n\n        # Add any extra tags if provided\n        if extra:\n            tags.extend(extra)\n\n        return tags\n\n    def _as_runnable(self, fn: Any):\n        \"\"\"Convert a function to a runnable if it isn't already.\n\n        Args:\n            fn: The function or object to convert to a runnable.\n\n        Returns:\n            A runnable object that can be used in the graph. If the input is already\n            runnable (has .with_config and .invoke methods), it's returned as is.\n            Otherwise, it's wrapped in a RunnableLambda.\n        \"\"\"\n        # Check if the function already has the required runnable interface\n        # If so, return it as is; otherwise wrap it in a RunnableLambda\n        return (\n            fn\n            if hasattr(fn, \"with_config\") and hasattr(fn, \"invoke\")\n            else RunnableLambda(fn)\n        )\n\n    def _node_cfg(self, name: str, *extra_tags: str) -&gt; dict:\n        \"\"\"Build a consistent configuration for a node/runnable.\n\n        Creates a configuration dict that can be reapplied after operations like\n        .map(), subgraph compile, etc.\n\n        Args:\n            name: The name of the node.\n            *extra_tags: Additional tags to include in the node configuration.\n\n        Returns:\n            dict: A configuration dictionary with run_name, tags, and metadata.\n        \"\"\"\n        # Determine the namespace - use first extra tag if available, otherwise\n        # convert agent name to snake_case\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n\n        # Combine all tags: agent name, graph indicator, node name, and any extra tags\n        tags = [self.name, \"graph\", name, *extra_tags]\n\n        # Return the complete configuration dictionary\n        return dict(\n            run_name=\"node\",  # keep \"node:\" prefixing in the timer\n            tags=tags,\n            metadata={\n                \"langgraph_node\": name,\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n\n    def ns(self, runnable_or_fn, name: str, *extra_tags: str):\n        \"\"\"Return a runnable with node configuration applied.\n\n        Applies the agent's node configuration to a runnable or callable. This method\n        should be called again after operations like .map() or subgraph .compile() as\n        these operations may drop configuration.\n\n        Args:\n            runnable_or_fn: A runnable or callable to configure.\n            name: The name to assign to this node.\n            *extra_tags: Additional tags to apply to the node.\n\n        Returns:\n            A configured runnable with the agent's node configuration applied.\n        \"\"\"\n        # Convert input to a runnable if it's not already one\n        r = self._as_runnable(runnable_or_fn)\n        # Apply node configuration and return the configured runnable\n        return r.with_config(**self._node_cfg(name, *extra_tags))\n\n    def _wrap_node(self, fn_or_runnable, name: str, *extra_tags: str):\n        \"\"\"Wrap a function or runnable as a node in the graph.\n\n        This is a convenience wrapper around the ns() method.\n\n        Args:\n            fn_or_runnable: A function or runnable to wrap as a node.\n            name: The name to assign to this node.\n            *extra_tags: Additional tags to apply to the node.\n\n        Returns:\n            A configured runnable with the agent's node configuration applied.\n        \"\"\"\n        return self.ns(fn_or_runnable, name, *extra_tags)\n\n    def _wrap_cond(self, fn: Any, name: str, *extra_tags: str):\n        \"\"\"Wrap a conditional function as a routing node in the graph.\n\n        Creates a runnable lambda with routing-specific configuration.\n\n        Args:\n            fn: The conditional function to wrap.\n            name: The name of the routing node.\n            *extra_tags: Additional tags to apply to the node.\n\n        Returns:\n            A configured RunnableLambda with routing-specific metadata.\n        \"\"\"\n        # Use the first extra tag as namespace, or fall back to agent name in snake_case\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n\n        # Create and return a configured RunnableLambda for routing\n        return RunnableLambda(fn).with_config(\n            run_name=\"node\",\n            tags=[\n                self.name,\n                \"graph\",\n                f\"route:{name}\",\n                *extra_tags,\n            ],\n            metadata={\n                \"langgraph_node\": f\"route:{name}\",\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n\n    def _named(self, runnable: Any, name: str, *extra_tags: str):\n        \"\"\"Apply a specific name and configuration to a runnable.\n\n        Configures a runnable with a specific name and the agent's metadata.\n\n        Args:\n            runnable: The runnable to configure.\n            name: The name to assign to this runnable.\n            *extra_tags: Additional tags to apply to the runnable.\n\n        Returns:\n            A configured runnable with the specified name and agent metadata.\n        \"\"\"\n        # Use the first extra tag as namespace, or fall back to agent name in snake_case\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n\n        # Apply configuration and return the configured runnable\n        return runnable.with_config(\n            run_name=name,\n            tags=[self.name, \"graph\", name, *extra_tags],\n            metadata={\n                \"langgraph_node\": name,\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.llm","title":"<code>llm = llm</code>  <code>instance-attribute</code>","text":"<p>Initializes the base agent with a language model and optional configurations.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <p>a BaseChatModel instance.</p> required <code>checkpointer</code> <p>Optional checkpoint saver for persisting agent state.</p> required <code>enable_metrics</code> <p>Whether to collect performance and usage metrics.</p> required <code>metrics_dir</code> <p>Directory path where metrics will be saved.</p> required <code>autosave_metrics</code> <p>Whether to automatically save metrics to disk.</p> required <code>thread_id</code> <p>Unique identifier for this agent instance. Generated if not        provided.</p> required"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.name","title":"<code>name</code>  <code>property</code>","text":"<p>Agent name.</p>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.__call__","title":"<code>__call__(inputs, /, **kwargs)</code>","text":"<p>Specify calling behavior for class instance.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def __call__(self, inputs: InputLike, /, **kwargs: Any) -&gt; Any:\n    \"\"\"Specify calling behavior for class instance.\"\"\"\n    return self.invoke(inputs, **kwargs)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Ensure subclass does not override key method.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Ensure subclass does not override key method.\"\"\"\n    super().__init_subclass__(**kwargs)\n    if \"invoke\" in cls.__dict__:\n        err_msg = (\n            f\"{cls.__name__} must not override BaseAgent.invoke(); \"\n            \"implement _invoke() only.\"\n        )\n        raise TypeError(err_msg)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.add_node","title":"<code>add_node(graph, f, node_name=None, agent_name=None)</code>","text":"<p>Add a node to the state graph with token usage tracking.</p> <p>This method adds a function as a node to the state graph, wrapping it to track token usage during execution. The node is identified by either the provided node_name or the function's name.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>StateGraph</code> <p>The StateGraph to add the node to.</p> required <code>f</code> <code>Callable[..., Mapping[str, Any]]</code> <p>The function to add as a node. Should return a mapping of string keys to any values.</p> required <code>node_name</code> <code>Optional[str]</code> <p>Optional name for the node. If not provided, the function's name will be used.</p> <code>None</code> <code>agent_name</code> <code>Optional[str]</code> <p>Optional agent name for tracking. If not provided, the agent's name in snake_case will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>StateGraph</code> <p>The updated StateGraph with the new node added.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def add_node(\n    self,\n    graph: StateGraph,\n    f: Callable[..., Mapping[str, Any]],\n    node_name: Optional[str] = None,\n    agent_name: Optional[str] = None,\n) -&gt; StateGraph:\n    \"\"\"Add a node to the state graph with token usage tracking.\n\n    This method adds a function as a node to the state graph, wrapping it to track\n    token usage during execution. The node is identified by either the provided\n    node_name or the function's name.\n\n    Args:\n        graph: The StateGraph to add the node to.\n        f: The function to add as a node. Should return a mapping of string keys to\n            any values.\n        node_name: Optional name for the node. If not provided, the function's name\n            will be used.\n        agent_name: Optional agent name for tracking. If not provided, the agent's\n            name in snake_case will be used.\n\n    Returns:\n        The updated StateGraph with the new node added.\n    \"\"\"\n    _node_name = node_name or f.__name__\n    _agent_name = agent_name or _to_snake(self.name)\n    wrapped_node = self._wrap_node(f, _node_name, _agent_name)\n\n    return graph.add_node(_node_name, wrapped_node)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.ainvoke","title":"<code>ainvoke(inputs=None, /, *, raw_debug=False, save_json=None, metrics_path=None, save_raw_snapshot=None, save_raw_records=None, config=None, **kwargs)</code>","text":"<p>Asynchrnously executes the agent with the provided inputs and configuration.</p> <p>(Async version of <code>invoke</code>.)</p> <p>This is the main entry point for agent execution. It handles input normalization, telemetry tracking, and proper execution context management. The method supports flexible input formats - either as a positional argument or as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Optional[InputLike]</code> <p>Optional positional input to the agent. If provided, all non-control keyword arguments will be rejected to avoid ambiguity.</p> <code>None</code> <code>raw_debug</code> <code>bool</code> <p>If True, displays raw telemetry data for debugging purposes.</p> <code>False</code> <code>save_json</code> <code>Optional[bool]</code> <p>If True, saves telemetry data as JSON.</p> <code>None</code> <code>metrics_path</code> <code>Optional[str]</code> <p>Optional file path where telemetry metrics should be saved.</p> <code>None</code> <code>save_raw_snapshot</code> <code>Optional[bool]</code> <p>If True, saves a raw snapshot of the telemetry data.</p> <code>None</code> <code>save_raw_records</code> <code>Optional[bool]</code> <p>If True, saves raw telemetry records.</p> <code>None</code> <code>config</code> <code>Optional[dict]</code> <p>Optional configuration dictionary to override default settings.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that can be either: - Input parameters (when no positional input is provided) - Control parameters recognized by the agent</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the agent's execution.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If both positional inputs and non-control keyword arguments are provided simultaneously.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>@final\ndef ainvoke(\n    self,\n    inputs: Optional[InputLike] = None,\n    /,\n    *,\n    raw_debug: bool = False,\n    save_json: Optional[bool] = None,\n    metrics_path: Optional[str] = None,\n    save_raw_snapshot: Optional[bool] = None,\n    save_raw_records: Optional[bool] = None,\n    config: Optional[dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Asynchrnously executes the agent with the provided inputs and configuration.\n\n    (Async version of `invoke`.)\n\n    This is the main entry point for agent execution. It handles input normalization,\n    telemetry tracking, and proper execution context management. The method supports\n    flexible input formats - either as a positional argument or as keyword arguments.\n\n    Args:\n        inputs: Optional positional input to the agent. If provided, all non-control\n            keyword arguments will be rejected to avoid ambiguity.\n        raw_debug: If True, displays raw telemetry data for debugging purposes.\n        save_json: If True, saves telemetry data as JSON.\n        metrics_path: Optional file path where telemetry metrics should be saved.\n        save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n        save_raw_records: If True, saves raw telemetry records.\n        config: Optional configuration dictionary to override default settings.\n        **kwargs: Additional keyword arguments that can be either:\n            - Input parameters (when no positional input is provided)\n            - Control parameters recognized by the agent\n\n    Returns:\n        The result of the agent's execution.\n\n    Raises:\n        TypeError: If both positional inputs and non-control keyword arguments are\n            provided simultaneously.\n    \"\"\"\n    return self._invoke_engine(\n        invoke_method=self._ainvoke,\n        inputs=inputs,\n        raw_debug=raw_debug,\n        save_json=save_json,\n        metrics_path=metrics_path,\n        save_raw_snapshot=save_raw_snapshot,\n        save_raw_records=save_raw_records,\n        config=config,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.build_config","title":"<code>build_config(**overrides)</code>","text":"<p>Constructs a config dictionary for agent operations with telemetry support.</p> <p>This method creates a standardized configuration dictionary that includes thread identification, telemetry callbacks, and other metadata needed for agent operations. The configuration can be customized through override parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**overrides</code> <p>Optional configuration overrides that can include keys like 'recursion_limit', 'configurable', 'metadata', 'tags', etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A complete configuration dictionary with all necessary parameters.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def build_config(self, **overrides) -&gt; dict:\n    \"\"\"Constructs a config dictionary for agent operations with telemetry support.\n\n    This method creates a standardized configuration dictionary that includes thread\n    identification, telemetry callbacks, and other metadata needed for agent\n    operations. The configuration can be customized through override parameters.\n\n    Args:\n        **overrides: Optional configuration overrides that can include keys like\n            'recursion_limit', 'configurable', 'metadata', 'tags', etc.\n\n    Returns:\n        dict: A complete configuration dictionary with all necessary parameters.\n    \"\"\"\n    # Create the base configuration with essential fields.\n    base = {\n        \"configurable\": {\"thread_id\": self.thread_id},\n        \"metadata\": {\n            \"thread_id\": self.thread_id,\n            \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n        },\n        \"tags\": [self.name],\n        \"callbacks\": self.telemetry.callbacks,\n    }\n\n    # Try to determine the model name from either direct or nested attributes\n    model_name = getattr(self, \"llm_model\", None) or getattr(\n        getattr(self, \"llm\", None), \"model\", None\n    )\n\n    # Add model name to metadata if available\n    if model_name:\n        base[\"metadata\"][\"model\"] = model_name\n\n    # Handle configurable dictionary overrides by merging with base configurable\n    if \"configurable\" in overrides and isinstance(\n        overrides[\"configurable\"], dict\n    ):\n        base[\"configurable\"].update(overrides.pop(\"configurable\"))\n\n    # Handle metadata dictionary overrides by merging with base metadata\n    if \"metadata\" in overrides and isinstance(overrides[\"metadata\"], dict):\n        base[\"metadata\"].update(overrides.pop(\"metadata\"))\n\n    # Merge tags from caller-provided overrides, avoid duplicates\n    if \"tags\" in overrides and isinstance(overrides[\"tags\"], list):\n        base[\"tags\"] = base[\"tags\"] + [\n            t for t in overrides.pop(\"tags\") if t not in base[\"tags\"]\n        ]\n\n    # Apply any remaining overrides directly to the base configuration\n    base.update(overrides)\n\n    return base\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.invoke","title":"<code>invoke(inputs=None, /, *, raw_debug=False, save_json=None, metrics_path=None, save_raw_snapshot=None, save_raw_records=None, config=None, **kwargs)</code>","text":"<p>Executes the agent with the provided inputs and configuration.</p> <p>This is the main entry point for agent execution. It handles input normalization, telemetry tracking, and proper execution context management. The method supports flexible input formats - either as a positional argument or as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Optional[InputLike]</code> <p>Optional positional input to the agent. If provided, all non-control keyword arguments will be rejected to avoid ambiguity.</p> <code>None</code> <code>raw_debug</code> <code>bool</code> <p>If True, displays raw telemetry data for debugging purposes.</p> <code>False</code> <code>save_json</code> <code>Optional[bool]</code> <p>If True, saves telemetry data as JSON.</p> <code>None</code> <code>metrics_path</code> <code>Optional[str]</code> <p>Optional file path where telemetry metrics should be saved.</p> <code>None</code> <code>save_raw_snapshot</code> <code>Optional[bool]</code> <p>If True, saves a raw snapshot of the telemetry data.</p> <code>None</code> <code>save_raw_records</code> <code>Optional[bool]</code> <p>If True, saves raw telemetry records.</p> <code>None</code> <code>config</code> <code>Optional[dict]</code> <p>Optional configuration dictionary to override default settings.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that can be either: - Input parameters (when no positional input is provided) - Control parameters recognized by the agent</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the agent's execution.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If both positional inputs and non-control keyword arguments are provided simultaneously.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>@final\ndef invoke(\n    self,\n    inputs: Optional[InputLike] = None,\n    /,\n    *,\n    raw_debug: bool = False,\n    save_json: Optional[bool] = None,\n    metrics_path: Optional[str] = None,\n    save_raw_snapshot: Optional[bool] = None,\n    save_raw_records: Optional[bool] = None,\n    config: Optional[dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Executes the agent with the provided inputs and configuration.\n\n    This is the main entry point for agent execution. It handles input normalization,\n    telemetry tracking, and proper execution context management. The method supports\n    flexible input formats - either as a positional argument or as keyword arguments.\n\n    Args:\n        inputs: Optional positional input to the agent. If provided, all non-control\n            keyword arguments will be rejected to avoid ambiguity.\n        raw_debug: If True, displays raw telemetry data for debugging purposes.\n        save_json: If True, saves telemetry data as JSON.\n        metrics_path: Optional file path where telemetry metrics should be saved.\n        save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n        save_raw_records: If True, saves raw telemetry records.\n        config: Optional configuration dictionary to override default settings.\n        **kwargs: Additional keyword arguments that can be either:\n            - Input parameters (when no positional input is provided)\n            - Control parameters recognized by the agent\n\n    Returns:\n        The result of the agent's execution.\n\n    Raises:\n        TypeError: If both positional inputs and non-control keyword arguments are\n            provided simultaneously.\n    \"\"\"\n    return self._invoke_engine(\n        invoke_method=self._invoke,\n        inputs=inputs,\n        raw_debug=raw_debug,\n        save_json=save_json,\n        metrics_path=metrics_path,\n        save_raw_snapshot=save_raw_snapshot,\n        save_raw_records=save_raw_records,\n        config=config,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.ns","title":"<code>ns(runnable_or_fn, name, *extra_tags)</code>","text":"<p>Return a runnable with node configuration applied.</p> <p>Applies the agent's node configuration to a runnable or callable. This method should be called again after operations like .map() or subgraph .compile() as these operations may drop configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runnable_or_fn</code> <p>A runnable or callable to configure.</p> required <code>name</code> <code>str</code> <p>The name to assign to this node.</p> required <code>*extra_tags</code> <code>str</code> <p>Additional tags to apply to the node.</p> <code>()</code> <p>Returns:</p> Type Description <p>A configured runnable with the agent's node configuration applied.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def ns(self, runnable_or_fn, name: str, *extra_tags: str):\n    \"\"\"Return a runnable with node configuration applied.\n\n    Applies the agent's node configuration to a runnable or callable. This method\n    should be called again after operations like .map() or subgraph .compile() as\n    these operations may drop configuration.\n\n    Args:\n        runnable_or_fn: A runnable or callable to configure.\n        name: The name to assign to this node.\n        *extra_tags: Additional tags to apply to the node.\n\n    Returns:\n        A configured runnable with the agent's node configuration applied.\n    \"\"\"\n    # Convert input to a runnable if it's not already one\n    r = self._as_runnable(runnable_or_fn)\n    # Apply node configuration and return the configured runnable\n    return r.with_config(**self._node_cfg(name, *extra_tags))\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.stream","title":"<code>stream(inputs, config=None, /, *, raw_debug=False, save_json=None, metrics_path=None, save_raw_snapshot=None, save_raw_records=None, **kwargs)</code>","text":"<p>Streams agent responses with telemetry tracking.</p> <p>This method serves as the public streaming entry point for agent interactions. It wraps the actual streaming implementation with telemetry tracking to capture metrics and debugging information.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>InputLike</code> <p>The input to process, which will be normalized internally.</p> required <code>config</code> <code>Any | None</code> <p>Optional configuration for the agent, compatible with LangGraph positional/keyword argument style.</p> <code>None</code> <code>raw_debug</code> <code>bool</code> <p>If True, renders raw debug information in telemetry output.</p> <code>False</code> <code>save_json</code> <code>bool | None</code> <p>If True, saves telemetry data as JSON.</p> <code>None</code> <code>metrics_path</code> <code>str | None</code> <p>Optional file path where metrics should be saved.</p> <code>None</code> <code>save_raw_snapshot</code> <code>bool | None</code> <p>If True, saves raw snapshot data in telemetry.</p> <code>None</code> <code>save_raw_records</code> <code>bool | None</code> <p>If True, saves raw record data in telemetry.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the streaming implementation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>An iterator yielding the agent's responses.</p> Note <p>This method tracks invocation depth to properly handle nested agent calls and ensure telemetry is only rendered once at the top level.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def stream(\n    self,\n    inputs: InputLike,\n    config: Any | None = None,  # allow positional/keyword like LangGraph\n    /,\n    *,\n    raw_debug: bool = False,\n    save_json: bool | None = None,\n    metrics_path: str | None = None,\n    save_raw_snapshot: bool | None = None,\n    save_raw_records: bool | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[Any]:\n    \"\"\"Streams agent responses with telemetry tracking.\n\n    This method serves as the public streaming entry point for agent interactions.\n    It wraps the actual streaming implementation with telemetry tracking to capture\n    metrics and debugging information.\n\n    Args:\n        inputs: The input to process, which will be normalized internally.\n        config: Optional configuration for the agent, compatible with LangGraph\n            positional/keyword argument style.\n        raw_debug: If True, renders raw debug information in telemetry output.\n        save_json: If True, saves telemetry data as JSON.\n        metrics_path: Optional file path where metrics should be saved.\n        save_raw_snapshot: If True, saves raw snapshot data in telemetry.\n        save_raw_records: If True, saves raw record data in telemetry.\n        **kwargs: Additional keyword arguments passed to the streaming\n            implementation.\n\n    Returns:\n        An iterator yielding the agent's responses.\n\n    Note:\n        This method tracks invocation depth to properly handle nested agent calls\n        and ensure telemetry is only rendered once at the top level.\n    \"\"\"\n    # Track invocation depth to handle nested agent calls\n    BaseAgent._invoke_depth += 1\n\n    try:\n        # Start telemetry tracking for top-level invocations only\n        if BaseAgent._invoke_depth == 1:\n            self.telemetry.begin_run(\n                agent=self.name, thread_id=self.thread_id\n            )\n\n        # Normalize inputs and delegate to the actual streaming implementation\n        normalized = self._normalize_inputs(inputs)\n        yield from self._stream(normalized, config=config, **kwargs)\n\n    finally:\n        # Decrement invocation depth when exiting\n        BaseAgent._invoke_depth -= 1\n\n        # Render telemetry data only for top-level invocations\n        if BaseAgent._invoke_depth == 0:\n            self.telemetry.render(\n                raw=raw_debug,\n                save_json=save_json,\n                filepath=metrics_path,\n                save_raw_snapshot=save_raw_snapshot,\n                save_raw_records=save_raw_records,\n            )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.write_state","title":"<code>write_state(filename, state)</code>","text":"<p>Writes agent state to a JSON file.</p> <p>Serializes the provided state dictionary to JSON format and writes it to the specified file. The JSON is written with non-ASCII characters preserved.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file where state will be written.</p> required <code>state</code> <code>dict</code> <p>Dictionary containing the agent state to be serialized.</p> required Source code in <code>src/ursa/agents/base.py</code> <pre><code>def write_state(self, filename: str, state: dict) -&gt; None:\n    \"\"\"Writes agent state to a JSON file.\n\n    Serializes the provided state dictionary to JSON format and writes it to the\n    specified file. The JSON is written with non-ASCII characters preserved.\n\n    Args:\n        filename: Path to the file where state will be written.\n        state: Dictionary containing the agent state to be serialized.\n    \"\"\"\n    json_state = dumps(state, ensure_ascii=False)\n    with open(filename, \"w\") as f:\n        f.write(json_state)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent","title":"<code>code_review_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.code_review_agent.read_file","title":"<code>read_file(filename, state)</code>","text":"<p>Reads in a file with a given filename into a string</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>string filename to read in</p> required Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef read_file(filename: str, state: Annotated[dict, InjectedState]):\n    \"\"\"\n    Reads in a file with a given filename into a string\n\n    Args:\n        filename: string filename to read in\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    full_filename = os.path.join(workspace_dir, filename)\n\n    print(\"[READING]: \", full_filename)\n    with open(full_filename, \"r\") as file:\n        file_contents = file.read()\n    return file_contents\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Run command from commandline</p> Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"Run command from commandline\"\"\"\n    workspace_dir = state[\"workspace\"]\n\n    print(\"RUNNING: \", query)\n    process = subprocess.Popen(\n        query.split(\" \"),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        cwd=workspace_dir,\n    )\n\n    stdout, stderr = process.communicate(timeout=600)\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent.write_file","title":"<code>write_file(code, filename, state)</code>","text":"<p>Writes text to a file in the given workspace as requested.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Text to write to a file</p> required <code>filename</code> <code>str</code> <p>the filename to write to</p> required <p>Returns:</p> Type Description <code>str</code> <p>Execution results</p> Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef write_file(\n    code: str, filename: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Writes text to a file in the given workspace as requested.\n\n    Args:\n        code: Text to write to a file\n        filename: the filename to write to\n\n    Returns:\n        Execution results\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n\n    print(\"[WRITING]: \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent","title":"<code>execution_agent</code>","text":"<p>Execution agent that builds a tool-enabled state graph to autonomously run tasks.</p> <p>This module implements ExecutionAgent, a LangGraph-based agent that executes user instructions by invoking LLM tool calls and coordinating a controlled workflow.</p> <p>Key features: - Workspace management with optional symlinking for external sources. - Safety-checked shell execution via run_cmd with output size budgeting. - Code authoring and edits through write_code and edit_code with rich previews. - Web search capability through DuckDuckGoSearchResults. - Summarization of the session and optional memory logging. - Configurable graph with nodes for agent, safety_check, action, and summarize.</p> <p>Implementation notes: - LLM prompts are sourced from prompt_library.execution_prompts. - Outputs from subprocess are trimmed under MAX_TOOL_MSG_CHARS to fit tool messages. - The agent uses ToolNode and LangGraph StateGraph to loop until no tool calls remain. - Safety gates block unsafe shell commands and surface the rationale to the user.</p> <p>Environment: - MAX_TOOL_MSG_CHARS caps combined stdout/stderr in tool responses.</p> <p>Entry points: - ExecutionAgent._invoke(...) runs the compiled graph. - main() shows a minimal demo that writes and runs a script.</p>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent","title":"<code>ExecutionAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>Orchestrates model-driven code execution, tool calls, and state management.</p> <p>Orchestrates model-driven code execution, tool calls, and state management for iterative program synthesis and shell interaction.</p> <p>This agent wraps an LLM with a small execution graph that alternates between issuing model queries, invoking tools (run, write, edit, search), performing safety checks, and summarizing progress. It manages a workspace on disk, optional symlinks, and an optional memory backend to persist summaries.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseChatModel</code> <p>Model identifier or bound chat model instance. If a string is provided, the BaseAgent initializer will resolve it.</p> required <code>agent_memory</code> <code>Any | AgentMemory</code> <p>Memory backend used to store summarized agent interactions. If provided, summaries are saved here.</p> <code>None</code> <code>log_state</code> <code>bool</code> <p>When True, the agent writes intermediate json state to disk for debugging and auditability.</p> <code>False</code> <code>**kwargs</code> <p>Passed through to the BaseAgent constructor (e.g., model configuration, checkpointer).</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>safe_codes</code> <code>list[str]</code> <p>List of trusted programming languages for the agent. Defaults to python and julia</p> <code>executor_prompt</code> <code>str</code> <p>Prompt used when invoking the executor LLM loop.</p> <code>summarize_prompt</code> <code>str</code> <p>Prompt used to request concise summaries for memory or final output.</p> <code>tools</code> <code>list[Tool]</code> <p>Tools available to the agent (run_cmd, write_code, edit_code, search_tool).</p> <code>tool_node</code> <code>ToolNode</code> <p>Graph node that dispatches tool calls.</p> <code>llm</code> <code>BaseChatModel</code> <p>LLM instance bound to the available tools.</p> <code>_action</code> <code>StateGraph</code> <p>Compiled execution graph that implements the main loop and branching logic.</p> <p>Methods:</p> Name Description <code>query_executor</code> <p>Send messages to the executor LLM, ensure workspace exists, and handle symlink setup before returning the model response.</p> <code>summarize</code> <p>Produce and optionally persist a summary of recent interactions to the memory backend.</p> <code>safety_check</code> <p>Validate pending run_cmd calls via the safety prompt and append ToolMessages for unsafe commands.</p> <code>get_safety_prompt</code> <p>Get the LLM prompt for safety_check that includes an editable list of available programming languages and gets the context of files that the agent has generated and can trust.</p> <code>_build_graph</code> <p>Construct and compile the StateGraph for the agent loop.</p> <code>_invoke</code> <p>Internal entry that invokes the compiled graph with a given recursion limit.</p> <code>action </code> <p>Disabled; direct access is not supported. Use invoke or stream entry points instead.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>Accessing the .action attribute raises to encourage using .stream(...) or .invoke(...).</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>class ExecutionAgent(BaseAgent):\n    \"\"\"Orchestrates model-driven code execution, tool calls, and state management.\n\n    Orchestrates model-driven code execution, tool calls, and state management for\n    iterative program synthesis and shell interaction.\n\n    This agent wraps an LLM with a small execution graph that alternates\n    between issuing model queries, invoking tools (run, write, edit, search),\n    performing safety checks, and summarizing progress. It manages a\n    workspace on disk, optional symlinks, and an optional memory backend to\n    persist summaries.\n\n    Args:\n        llm (BaseChatModel): Model identifier or bound chat model\n            instance. If a string is provided, the BaseAgent initializer will\n            resolve it.\n        agent_memory (Any | AgentMemory, optional): Memory backend used to\n            store summarized agent interactions. If provided, summaries are\n            saved here.\n        log_state (bool): When True, the agent writes intermediate json state\n            to disk for debugging and auditability.\n        **kwargs: Passed through to the BaseAgent constructor (e.g., model\n            configuration, checkpointer).\n\n    Attributes:\n        safe_codes (list[str]): List of trusted programming languages for the\n            agent. Defaults to python and julia\n        executor_prompt (str): Prompt used when invoking the executor LLM\n            loop.\n        summarize_prompt (str): Prompt used to request concise summaries for\n            memory or final output.\n        tools (list[Tool]): Tools available to the agent (run_cmd, write_code,\n            edit_code, search_tool).\n        tool_node (ToolNode): Graph node that dispatches tool calls.\n        llm (BaseChatModel): LLM instance bound to the available tools.\n        _action (StateGraph): Compiled execution graph that implements the\n            main loop and branching logic.\n\n    Methods:\n        query_executor(state): Send messages to the executor LLM, ensure\n            workspace exists, and handle symlink setup before returning the\n            model response.\n        summarize(state): Produce and optionally persist a summary of recent\n            interactions to the memory backend.\n        safety_check(state): Validate pending run_cmd calls via the safety\n            prompt and append ToolMessages for unsafe commands.\n        get_safety_prompt(query, safe_codes, created_files): Get the LLM prompt for safety_check\n            that includes an editable list of available programming languages and gets the context\n            of files that the agent has generated and can trust.\n        _build_graph(): Construct and compile the StateGraph for the agent\n            loop.\n        _invoke(inputs, recursion_limit=...): Internal entry that invokes the\n            compiled graph with a given recursion limit.\n        action (property): Disabled; direct access is not supported. Use\n            invoke or stream entry points instead.\n\n    Raises:\n        AttributeError: Accessing the .action attribute raises to encourage\n            using .stream(...) or .invoke(...).\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        agent_memory: Optional[Any | AgentMemory] = None,\n        log_state: bool = False,\n        extra_tools: Optional[list[Callable[..., Any]]] = None,\n        tokens_before_summarize: int = 50000,\n        messages_to_keep: int = 20,\n        safe_codes: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        \"\"\"ExecutionAgent class initialization.\"\"\"\n        super().__init__(llm, **kwargs)\n        self.agent_memory = agent_memory\n        self.safe_codes = safe_codes or [\"python\", \"julia\"]\n        self.get_safety_prompt = get_safety_prompt\n        self.executor_prompt = executor_prompt\n        self.summarize_prompt = summarize_prompt\n        self.tools = [run_cmd, write_code, edit_code, search_tool]\n        self.extra_tools = extra_tools\n        if self.extra_tools is not None:\n            self.tools.extend(self.extra_tools)\n        self.tool_node = ToolNode(self.tools)\n        self.llm = self.llm.bind_tools(self.tools)\n        self.log_state = log_state\n        self._action = self._build_graph()\n        self.context_summarizer = SummarizationMiddleware(\n            model=self.llm,\n            max_tokens_before_summary=tokens_before_summarize,\n            messages_to_keep=messages_to_keep,\n        )\n\n    # Check message history length and summarize to shorten the token usage:\n    def _summarize_context(self, state: ExecutionState) -&gt; ExecutionState:\n        summarized_messages = self.context_summarizer.before_model(state, None)\n        if summarized_messages:\n            tokens_before_summarize = self.context_summarizer.token_counter(\n                state[\"messages\"]\n            )\n            state[\"messages\"] = summarized_messages[\"messages\"]\n            tokens_after_summarize = self.context_summarizer.token_counter(\n                state[\"messages\"][1:]\n            )\n            console.print(\n                Panel(\n                    (\n                        f\"Summarized Conversation History:\\n\"\n                        f\"Approximate tokens before: {tokens_before_summarize}\\n\"\n                        f\"Approximate tokens after: {tokens_after_summarize}\\n\"\n                    ),\n                    title=\"[bold yellow1 on black]:clipboard: Plan\",\n                    border_style=\"yellow1\",\n                    style=\"bold yellow1 on black\",\n                )\n            )\n        else:\n            tokens_after_summarize = self.context_summarizer.token_counter(\n                state[\"messages\"]\n            )\n        return state\n\n    # Define the function that calls the model\n    def query_executor(self, state: ExecutionState) -&gt; ExecutionState:\n        \"\"\"Prepare workspace, handle optional symlinks, and invoke the executor LLM.\n\n        This method copies the incoming state, ensures a workspace directory exists\n        (creating one with a random name when absent), optionally creates a symlink\n        described by state[\"symlinkdir\"], sets or injects the executor system prompt\n        as the first message, and invokes the bound LLM. When logging is enabled,\n        it persists the pre-invocation state to disk.\n\n        Args:\n            state: The current execution state. Expected keys include:\n                - \"messages\": Ordered list of System/Human/AI/Tool messages.\n                - \"workspace\": Optional path to the working directory.\n                - \"symlinkdir\": Optional dict with \"source\" and \"dest\" keys.\n\n        Returns:\n            ExecutionState: Partial state update containing:\n                - \"messages\": A list with the model's response as the latest entry.\n                - \"workspace\": The resolved workspace path.\n        \"\"\"\n        new_state = state.copy()\n\n        # 1) Ensure a workspace directory exists, creating a named one if absent.\n        if \"workspace\" not in new_state.keys():\n            new_state[\"workspace\"] = randomname.get_name()\n            print(\n                f\"{RED}Creating the folder \"\n                f\"{BLUE}{BOLD}{new_state['workspace']}{RESET}{RED} \"\n                f\"for this project.{RESET}\"\n            )\n        os.makedirs(new_state[\"workspace\"], exist_ok=True)\n\n        # 1.5) Check message history length and summarize to shorten the token usage:\n        new_state = self._summarize_context(new_state)\n\n        # 2) Optionally create a symlink if symlinkdir is provided and not yet linked.\n        sd = new_state.get(\"symlinkdir\")\n        if isinstance(sd, dict) and \"is_linked\" not in sd:\n            # symlinkdir structure: {\"source\": \"/path/to/src\", \"dest\": \"link/name\"}\n            symlinkdir = sd\n\n            src = Path(symlinkdir[\"source\"]).expanduser().resolve()\n            workspace_root = Path(new_state[\"workspace\"]).expanduser().resolve()\n            dst = (\n                workspace_root / symlinkdir[\"dest\"]\n            )  # Link lives inside workspace.\n\n            # If a file/link already exists at the destination, replace it.\n            if dst.exists() or dst.is_symlink():\n                dst.unlink()\n\n            # Ensure parent directories for the link exist.\n            dst.parent.mkdir(parents=True, exist_ok=True)\n\n            # Create the symlink (tell pathlib if the target is a directory).\n            dst.symlink_to(src, target_is_directory=src.is_dir())\n            print(f\"{RED}Symlinked {src} (source) --&gt; {dst} (dest)\")\n            new_state[\"symlinkdir\"][\"is_linked\"] = True\n\n        # 3) Ensure the executor prompt is the first SystemMessage.\n        if isinstance(new_state[\"messages\"][0], SystemMessage):\n            new_state[\"messages\"][0] = SystemMessage(\n                content=self.executor_prompt\n            )\n        else:\n            new_state[\"messages\"] = [\n                SystemMessage(content=self.executor_prompt)\n            ] + state[\"messages\"]\n\n        # 4) Invoke the LLM with the prepared message sequence.\n        try:\n            response = self.llm.invoke(\n                new_state[\"messages\"], self.build_config(tags=[\"agent\"])\n            )\n            new_state[\"messages\"].append(response)\n        except Exception as e:\n            print(\"Error: \", e, \" \", new_state[\"messages\"][-1].content)\n            new_state[\"messages\"].append(\n                AIMessage(content=f\"Response error {e}\")\n            )\n\n        # 5) Optionally persist the pre-invocation state for audit/debugging.\n        if self.log_state:\n            self.write_state(\"execution_agent.json\", new_state)\n\n        # Return the model's response and the workspace path as a partial state update.\n        return new_state\n\n    def summarize(self, state: ExecutionState) -&gt; ExecutionState:\n        \"\"\"Produce a concise summary of the conversation and optionally persist memory.\n\n        This method builds a summarization prompt, invokes the LLM to obtain a compact\n        summary of recent interactions, optionally logs salient details to the agent\n        memory backend, and writes debug state when logging is enabled.\n\n        Args:\n            state (ExecutionState): The execution state containing message history.\n\n        Returns:\n            ExecutionState: A partial update with a single string message containing\n                the summary.\n        \"\"\"\n        new_state = state.copy()\n\n        # 0) Check message history length and summarize to shorten the token usage:\n        new_state = self._summarize_context(new_state)\n\n        # 1) Construct the summarization message list (system prompt + prior messages).\n        messages = (\n            new_state[\"messages\"]\n            if isinstance(new_state[\"messages\"][0], SystemMessage)\n            else [SystemMessage(content=summarize_prompt)]\n            + new_state[\"messages\"]\n        )\n\n        # 2) Invoke the LLM to generate a summary; capture content even on failure.\n        response_content = \"\"\n        try:\n            response = self.llm.invoke(\n                messages, self.build_config(tags=[\"summarize\"])\n            )\n            response_content = response.content\n            new_state[\"messages\"].append(response)\n        except Exception as e:\n            print(\"Error: \", e, \" \", messages[-1].content)\n            new_state[\"messages\"].append(\n                AIMessage(content=f\"Response error {e}\")\n            )\n\n        # 3) Optionally persist salient details to the memory backend.\n        if self.agent_memory:\n            memories: list[str] = []\n            # Collect human/system/tool message content; for AI tool calls, store args.\n            for msg in new_state[\"messages\"]:\n                if not isinstance(msg, AIMessage):\n                    memories.append(msg.content)\n                elif not msg.tool_calls:\n                    memories.append(msg.content)\n                else:\n                    tool_strings = []\n                    for tool in msg.tool_calls:\n                        tool_strings.append(\"Tool Name: \" + tool[\"name\"])\n                        for arg_name in tool[\"args\"]:\n                            tool_strings.append(\n                                f\"Arg: {str(arg_name)}\\nValue: \"\n                                f\"{str(tool['args'][arg_name])}\"\n                            )\n                    memories.append(\"\\n\".join(tool_strings))\n            memories.append(response_content)\n            self.agent_memory.add_memories(memories)\n\n        # 4) Optionally write state to disk for debugging/auditing.\n        if self.log_state:\n            self.write_state(\"execution_agent.json\", new_state)\n\n        # 5) Return a partial state update with only the summary content.\n        return new_state\n\n    def safety_check(self, state: ExecutionState) -&gt; ExecutionState:\n        \"\"\"Assess pending shell commands for safety and inject ToolMessages with results.\n\n        This method inspects the most recent AI tool calls, evaluates any run_cmd\n        queries against the safety prompt, and constructs ToolMessages that either\n        flag unsafe commands with reasons or confirm safe execution. If any command\n        is unsafe, the generated ToolMessages are appended to the state so the agent\n        can react without executing the command.\n\n        Args:\n            state (ExecutionState): Current execution state.\n\n        Returns:\n            ExecutionState: Either the unchanged state (all safe) or a copy with one\n                or more ToolMessages appended when unsafe commands are detected.\n        \"\"\"\n        # 1) Work on a shallow copy; inspect the most recent model message.\n        new_state = state.copy()\n        last_msg = new_state[\"messages\"][-1]\n\n        # 1.5) Check message history length and summarize to shorten the token usage:\n        new_state = self._summarize_context(new_state)\n\n        # 2) Evaluate any pending run_cmd tool calls for safety.\n        tool_responses: list[ToolMessage] = []\n        any_unsafe = False\n        for tool_call in last_msg.tool_calls:\n            if tool_call[\"name\"] != \"run_cmd\":\n                continue\n\n            query = tool_call[\"args\"][\"query\"]\n            safety_result = self.llm.invoke(\n                self.get_safety_prompt(\n                    query, self.safe_codes, new_state.get(\"code_files\", [])\n                ),\n                self.build_config(tags=[\"safety_check\"]),\n            )\n\n            if \"[NO]\" in safety_result.content:\n                any_unsafe = True\n                tool_response = (\n                    \"[UNSAFE] That command `{q}` was deemed unsafe and cannot be run.\\n\"\n                    \"For reason: {r}\"\n                ).format(q=query, r=safety_result.content)\n                console.print(\n                    \"[bold red][WARNING][/bold red] Command deemed unsafe:\",\n                    query,\n                )\n                # Also surface the model's rationale for transparency.\n                console.print(\n                    \"[bold red][WARNING][/bold red] REASON:\", tool_response\n                )\n            else:\n                tool_response = f\"Command `{query}` passed safety check.\"\n                console.print(\n                    f\"[green]Command passed safety check:[/green] {query}\"\n                )\n\n            tool_responses.append(\n                ToolMessage(\n                    content=tool_response,\n                    tool_call_id=tool_call[\"id\"],\n                )\n            )\n\n        # 3) If any command is unsafe, append all tool responses; otherwise keep state.\n        if any_unsafe:\n            new_state[\"messages\"].extend(tool_responses)\n\n        return new_state\n\n    def _build_graph(self):\n        \"\"\"Construct and compile the agent's LangGraph state machine.\"\"\"\n        # Create a graph over the agent's execution state.\n        graph = StateGraph(ExecutionState)\n\n        # Register nodes:\n        # - \"agent\": LLM planning/execution step\n        # - \"action\": tool dispatch (run_cmd, write_code, etc.)\n        # - \"summarize\": summary/finalization step\n        # - \"safety_check\": gate for shell command safety\n        self.add_node(graph, self.query_executor, \"agent\")\n        self.add_node(graph, self.tool_node, \"action\")\n        self.add_node(graph, self.summarize, \"summarize\")\n        self.add_node(graph, self.safety_check, \"safety_check\")\n\n        # Set entrypoint: execution starts with the \"agent\" node.\n        graph.set_entry_point(\"agent\")\n\n        # From \"agent\", either continue (tools) or finish (summarize),\n        # based on presence of tool calls in the last message.\n        graph.add_conditional_edges(\n            \"agent\",\n            self._wrap_cond(should_continue, \"should_continue\", \"execution\"),\n            {\"continue\": \"safety_check\", \"summarize\": \"summarize\"},\n        )\n\n        # From \"safety_check\", route to tools if safe, otherwise back to agent\n        # to revise the plan without executing unsafe commands.\n        graph.add_conditional_edges(\n            \"safety_check\",\n            self._wrap_cond(command_safe, \"command_safe\", \"execution\"),\n            {\"safe\": \"action\", \"unsafe\": \"agent\"},\n        )\n\n        # After tools run, return control to the agent for the next step.\n        graph.add_edge(\"action\", \"agent\")\n\n        # The graph completes at the \"summarize\" node.\n        graph.set_finish_point(\"summarize\")\n\n        # Compile and return the executable graph (optionally with a checkpointer).\n        return graph.compile(checkpointer=self.checkpointer)\n\n    async def add_mcp_tool(\n        self, mcp_tools: Callable[..., Any] | list[Callable[..., Any]]\n    ) -&gt; None:\n        client = MultiServerMCPClient(mcp_tools)\n        tools = await client.get_tools()\n        self.add_tool(tools)\n\n    def add_tool(\n        self, new_tools: Callable[..., Any] | list[Callable[..., Any]]\n    ) -&gt; None:\n        if isinstance(new_tools, list):\n            self.tools.extend([convert_to_tool(x) for x in new_tools])\n        elif isinstance(new_tools, StructuredTool) or isinstance(\n            new_tools, Callable\n        ):\n            self.tools.append(convert_to_tool(new_tools))\n        else:\n            raise TypeError(\"Expected a callable or a list of callables.\")\n        self.tool_node = ToolNode(self.tools)\n        self.llm = self.llm.bind_tools(self.tools)\n        self._action = self._build_graph()\n\n    def list_tools(self) -&gt; None:\n        print(\n            f\"Available tool names are: {', '.join([x.name for x in self.tools])}.\"\n        )\n\n    def remove_tool(self, cut_tools: str | list[str]) -&gt; None:\n        if isinstance(cut_tools, str):\n            self.remove_tool([cut_tools])\n        elif isinstance(cut_tools, list):\n            self.tools = [x for x in self.tools if x.name not in cut_tools]\n            self.tool_node = ToolNode(self.tools)\n            self.llm = self.llm.bind_tools(self.tools)\n            self._action = self._build_graph()\n        else:\n            raise TypeError(\n                \"Expected a string or a list of strings describing the tools to remove.\"\n            )\n\n    def _invoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 999_999, **_\n    ):\n        \"\"\"Invoke the compiled graph with inputs under a specified recursion limit.\n\n        This method builds a LangGraph config with the provided recursion limit\n        and a \"graph\" tag, then delegates to the compiled graph's invoke method.\n        \"\"\"\n        # Build invocation config with a generous recursion limit for long runs.\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n\n        # Delegate execution to the compiled graph.\n        return self._action.invoke(inputs, config)\n\n    def _ainvoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 999_999, **_\n    ):\n        \"\"\"Invoke the compiled graph with inputs under a specified recursion limit.\n\n        This method builds a LangGraph config with the provided recursion limit\n        and a \"graph\" tag, then delegates to the compiled graph's invoke method.\n        \"\"\"\n        # Build invocation config with a generous recursion limit for long runs.\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n\n        # Delegate execution to the compiled graph.\n        return self._action.ainvoke(inputs, config)\n\n    # This property is trying to stop people bypassing invoke\n    @property\n    def action(self):\n        \"\"\"Property used to affirm `action` attribute is unsupported.\"\"\"\n        raise AttributeError(\n            \"Use .stream(...) or .invoke(...); direct .action access is unsupported.\"\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.action","title":"<code>action</code>  <code>property</code>","text":"<p>Property used to affirm <code>action</code> attribute is unsupported.</p>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.__init__","title":"<code>__init__(llm, agent_memory=None, log_state=False, extra_tools=None, tokens_before_summarize=50000, messages_to_keep=20, safe_codes=None, **kwargs)</code>","text":"<p>ExecutionAgent class initialization.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def __init__(\n    self,\n    llm: BaseChatModel,\n    agent_memory: Optional[Any | AgentMemory] = None,\n    log_state: bool = False,\n    extra_tools: Optional[list[Callable[..., Any]]] = None,\n    tokens_before_summarize: int = 50000,\n    messages_to_keep: int = 20,\n    safe_codes: Optional[list[str]] = None,\n    **kwargs,\n):\n    \"\"\"ExecutionAgent class initialization.\"\"\"\n    super().__init__(llm, **kwargs)\n    self.agent_memory = agent_memory\n    self.safe_codes = safe_codes or [\"python\", \"julia\"]\n    self.get_safety_prompt = get_safety_prompt\n    self.executor_prompt = executor_prompt\n    self.summarize_prompt = summarize_prompt\n    self.tools = [run_cmd, write_code, edit_code, search_tool]\n    self.extra_tools = extra_tools\n    if self.extra_tools is not None:\n        self.tools.extend(self.extra_tools)\n    self.tool_node = ToolNode(self.tools)\n    self.llm = self.llm.bind_tools(self.tools)\n    self.log_state = log_state\n    self._action = self._build_graph()\n    self.context_summarizer = SummarizationMiddleware(\n        model=self.llm,\n        max_tokens_before_summary=tokens_before_summarize,\n        messages_to_keep=messages_to_keep,\n    )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.query_executor","title":"<code>query_executor(state)</code>","text":"<p>Prepare workspace, handle optional symlinks, and invoke the executor LLM.</p> <p>This method copies the incoming state, ensures a workspace directory exists (creating one with a random name when absent), optionally creates a symlink described by state[\"symlinkdir\"], sets or injects the executor system prompt as the first message, and invokes the bound LLM. When logging is enabled, it persists the pre-invocation state to disk.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The current execution state. Expected keys include: - \"messages\": Ordered list of System/Human/AI/Tool messages. - \"workspace\": Optional path to the working directory. - \"symlinkdir\": Optional dict with \"source\" and \"dest\" keys.</p> required <p>Returns:</p> Name Type Description <code>ExecutionState</code> <code>ExecutionState</code> <p>Partial state update containing: - \"messages\": A list with the model's response as the latest entry. - \"workspace\": The resolved workspace path.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def query_executor(self, state: ExecutionState) -&gt; ExecutionState:\n    \"\"\"Prepare workspace, handle optional symlinks, and invoke the executor LLM.\n\n    This method copies the incoming state, ensures a workspace directory exists\n    (creating one with a random name when absent), optionally creates a symlink\n    described by state[\"symlinkdir\"], sets or injects the executor system prompt\n    as the first message, and invokes the bound LLM. When logging is enabled,\n    it persists the pre-invocation state to disk.\n\n    Args:\n        state: The current execution state. Expected keys include:\n            - \"messages\": Ordered list of System/Human/AI/Tool messages.\n            - \"workspace\": Optional path to the working directory.\n            - \"symlinkdir\": Optional dict with \"source\" and \"dest\" keys.\n\n    Returns:\n        ExecutionState: Partial state update containing:\n            - \"messages\": A list with the model's response as the latest entry.\n            - \"workspace\": The resolved workspace path.\n    \"\"\"\n    new_state = state.copy()\n\n    # 1) Ensure a workspace directory exists, creating a named one if absent.\n    if \"workspace\" not in new_state.keys():\n        new_state[\"workspace\"] = randomname.get_name()\n        print(\n            f\"{RED}Creating the folder \"\n            f\"{BLUE}{BOLD}{new_state['workspace']}{RESET}{RED} \"\n            f\"for this project.{RESET}\"\n        )\n    os.makedirs(new_state[\"workspace\"], exist_ok=True)\n\n    # 1.5) Check message history length and summarize to shorten the token usage:\n    new_state = self._summarize_context(new_state)\n\n    # 2) Optionally create a symlink if symlinkdir is provided and not yet linked.\n    sd = new_state.get(\"symlinkdir\")\n    if isinstance(sd, dict) and \"is_linked\" not in sd:\n        # symlinkdir structure: {\"source\": \"/path/to/src\", \"dest\": \"link/name\"}\n        symlinkdir = sd\n\n        src = Path(symlinkdir[\"source\"]).expanduser().resolve()\n        workspace_root = Path(new_state[\"workspace\"]).expanduser().resolve()\n        dst = (\n            workspace_root / symlinkdir[\"dest\"]\n        )  # Link lives inside workspace.\n\n        # If a file/link already exists at the destination, replace it.\n        if dst.exists() or dst.is_symlink():\n            dst.unlink()\n\n        # Ensure parent directories for the link exist.\n        dst.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create the symlink (tell pathlib if the target is a directory).\n        dst.symlink_to(src, target_is_directory=src.is_dir())\n        print(f\"{RED}Symlinked {src} (source) --&gt; {dst} (dest)\")\n        new_state[\"symlinkdir\"][\"is_linked\"] = True\n\n    # 3) Ensure the executor prompt is the first SystemMessage.\n    if isinstance(new_state[\"messages\"][0], SystemMessage):\n        new_state[\"messages\"][0] = SystemMessage(\n            content=self.executor_prompt\n        )\n    else:\n        new_state[\"messages\"] = [\n            SystemMessage(content=self.executor_prompt)\n        ] + state[\"messages\"]\n\n    # 4) Invoke the LLM with the prepared message sequence.\n    try:\n        response = self.llm.invoke(\n            new_state[\"messages\"], self.build_config(tags=[\"agent\"])\n        )\n        new_state[\"messages\"].append(response)\n    except Exception as e:\n        print(\"Error: \", e, \" \", new_state[\"messages\"][-1].content)\n        new_state[\"messages\"].append(\n            AIMessage(content=f\"Response error {e}\")\n        )\n\n    # 5) Optionally persist the pre-invocation state for audit/debugging.\n    if self.log_state:\n        self.write_state(\"execution_agent.json\", new_state)\n\n    # Return the model's response and the workspace path as a partial state update.\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.safety_check","title":"<code>safety_check(state)</code>","text":"<p>Assess pending shell commands for safety and inject ToolMessages with results.</p> <p>This method inspects the most recent AI tool calls, evaluates any run_cmd queries against the safety prompt, and constructs ToolMessages that either flag unsafe commands with reasons or confirm safe execution. If any command is unsafe, the generated ToolMessages are appended to the state so the agent can react without executing the command.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>Current execution state.</p> required <p>Returns:</p> Name Type Description <code>ExecutionState</code> <code>ExecutionState</code> <p>Either the unchanged state (all safe) or a copy with one or more ToolMessages appended when unsafe commands are detected.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def safety_check(self, state: ExecutionState) -&gt; ExecutionState:\n    \"\"\"Assess pending shell commands for safety and inject ToolMessages with results.\n\n    This method inspects the most recent AI tool calls, evaluates any run_cmd\n    queries against the safety prompt, and constructs ToolMessages that either\n    flag unsafe commands with reasons or confirm safe execution. If any command\n    is unsafe, the generated ToolMessages are appended to the state so the agent\n    can react without executing the command.\n\n    Args:\n        state (ExecutionState): Current execution state.\n\n    Returns:\n        ExecutionState: Either the unchanged state (all safe) or a copy with one\n            or more ToolMessages appended when unsafe commands are detected.\n    \"\"\"\n    # 1) Work on a shallow copy; inspect the most recent model message.\n    new_state = state.copy()\n    last_msg = new_state[\"messages\"][-1]\n\n    # 1.5) Check message history length and summarize to shorten the token usage:\n    new_state = self._summarize_context(new_state)\n\n    # 2) Evaluate any pending run_cmd tool calls for safety.\n    tool_responses: list[ToolMessage] = []\n    any_unsafe = False\n    for tool_call in last_msg.tool_calls:\n        if tool_call[\"name\"] != \"run_cmd\":\n            continue\n\n        query = tool_call[\"args\"][\"query\"]\n        safety_result = self.llm.invoke(\n            self.get_safety_prompt(\n                query, self.safe_codes, new_state.get(\"code_files\", [])\n            ),\n            self.build_config(tags=[\"safety_check\"]),\n        )\n\n        if \"[NO]\" in safety_result.content:\n            any_unsafe = True\n            tool_response = (\n                \"[UNSAFE] That command `{q}` was deemed unsafe and cannot be run.\\n\"\n                \"For reason: {r}\"\n            ).format(q=query, r=safety_result.content)\n            console.print(\n                \"[bold red][WARNING][/bold red] Command deemed unsafe:\",\n                query,\n            )\n            # Also surface the model's rationale for transparency.\n            console.print(\n                \"[bold red][WARNING][/bold red] REASON:\", tool_response\n            )\n        else:\n            tool_response = f\"Command `{query}` passed safety check.\"\n            console.print(\n                f\"[green]Command passed safety check:[/green] {query}\"\n            )\n\n        tool_responses.append(\n            ToolMessage(\n                content=tool_response,\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n\n    # 3) If any command is unsafe, append all tool responses; otherwise keep state.\n    if any_unsafe:\n        new_state[\"messages\"].extend(tool_responses)\n\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.summarize","title":"<code>summarize(state)</code>","text":"<p>Produce a concise summary of the conversation and optionally persist memory.</p> <p>This method builds a summarization prompt, invokes the LLM to obtain a compact summary of recent interactions, optionally logs salient details to the agent memory backend, and writes debug state when logging is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The execution state containing message history.</p> required <p>Returns:</p> Name Type Description <code>ExecutionState</code> <code>ExecutionState</code> <p>A partial update with a single string message containing the summary.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def summarize(self, state: ExecutionState) -&gt; ExecutionState:\n    \"\"\"Produce a concise summary of the conversation and optionally persist memory.\n\n    This method builds a summarization prompt, invokes the LLM to obtain a compact\n    summary of recent interactions, optionally logs salient details to the agent\n    memory backend, and writes debug state when logging is enabled.\n\n    Args:\n        state (ExecutionState): The execution state containing message history.\n\n    Returns:\n        ExecutionState: A partial update with a single string message containing\n            the summary.\n    \"\"\"\n    new_state = state.copy()\n\n    # 0) Check message history length and summarize to shorten the token usage:\n    new_state = self._summarize_context(new_state)\n\n    # 1) Construct the summarization message list (system prompt + prior messages).\n    messages = (\n        new_state[\"messages\"]\n        if isinstance(new_state[\"messages\"][0], SystemMessage)\n        else [SystemMessage(content=summarize_prompt)]\n        + new_state[\"messages\"]\n    )\n\n    # 2) Invoke the LLM to generate a summary; capture content even on failure.\n    response_content = \"\"\n    try:\n        response = self.llm.invoke(\n            messages, self.build_config(tags=[\"summarize\"])\n        )\n        response_content = response.content\n        new_state[\"messages\"].append(response)\n    except Exception as e:\n        print(\"Error: \", e, \" \", messages[-1].content)\n        new_state[\"messages\"].append(\n            AIMessage(content=f\"Response error {e}\")\n        )\n\n    # 3) Optionally persist salient details to the memory backend.\n    if self.agent_memory:\n        memories: list[str] = []\n        # Collect human/system/tool message content; for AI tool calls, store args.\n        for msg in new_state[\"messages\"]:\n            if not isinstance(msg, AIMessage):\n                memories.append(msg.content)\n            elif not msg.tool_calls:\n                memories.append(msg.content)\n            else:\n                tool_strings = []\n                for tool in msg.tool_calls:\n                    tool_strings.append(\"Tool Name: \" + tool[\"name\"])\n                    for arg_name in tool[\"args\"]:\n                        tool_strings.append(\n                            f\"Arg: {str(arg_name)}\\nValue: \"\n                            f\"{str(tool['args'][arg_name])}\"\n                        )\n                memories.append(\"\\n\".join(tool_strings))\n        memories.append(response_content)\n        self.agent_memory.add_memories(memories)\n\n    # 4) Optionally write state to disk for debugging/auditing.\n    if self.log_state:\n        self.write_state(\"execution_agent.json\", new_state)\n\n    # 5) Return a partial state update with only the summary content.\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionState","title":"<code>ExecutionState</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict representing the execution agent's mutable run state used by nodes.</p> <p>Fields: - messages: list of messages (System/Human/AI/Tool) with add_messages metadata. - current_progress: short status string describing agent progress. - code_files: list of filenames created or edited in the workspace. - workspace: path to the working directory where files and commands run. - symlinkdir: optional dict describing a symlink operation (source, dest,   is_linked).</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>class ExecutionState(TypedDict):\n    \"\"\"TypedDict representing the execution agent's mutable run state used by nodes.\n\n    Fields:\n    - messages: list of messages (System/Human/AI/Tool) with add_messages metadata.\n    - current_progress: short status string describing agent progress.\n    - code_files: list of filenames created or edited in the workspace.\n    - workspace: path to the working directory where files and commands run.\n    - symlinkdir: optional dict describing a symlink operation (source, dest,\n      is_linked).\n    \"\"\"\n\n    messages: Annotated[list[AnyMessage], add_messages]\n    current_progress: str\n    code_files: list[str]\n    workspace: str\n    symlinkdir: dict\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.command_safe","title":"<code>command_safe(state)</code>","text":"<p>Return 'safe' if the last command was safe, otherwise 'unsafe'.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The current execution state containing messages and tool calls.</p> required <p>Returns:     A literal \"safe\" if no '[UNSAFE]' tags are in the last command,     otherwise \"unsafe\".</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def command_safe(state: ExecutionState) -&gt; Literal[\"safe\", \"unsafe\"]:\n    \"\"\"Return 'safe' if the last command was safe, otherwise 'unsafe'.\n\n    Args:\n        state: The current execution state containing messages and tool calls.\n    Returns:\n        A literal \"safe\" if no '[UNSAFE]' tags are in the last command,\n        otherwise \"unsafe\".\n    \"\"\"\n    index = -1\n    message = state[\"messages\"][index]\n    # Loop through all the consecutive tool messages in reverse order\n    while isinstance(message, ToolMessage):\n        if \"[UNSAFE]\" in message.content:\n            return \"unsafe\"\n\n        index -= 1\n        message = state[\"messages\"][index]\n\n    return \"safe\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.edit_code","title":"<code>edit_code(old_code, new_code, filename, state)</code>","text":"<p>Replace the first occurrence of old_code with new_code in filename.</p> <p>Parameters:</p> Name Type Description Default <code>old_code</code> <code>str</code> <p>Code fragment to search for.</p> required <code>new_code</code> <code>str</code> <p>Replacement fragment.</p> required <code>filename</code> <code>str</code> <p>Target file inside the workspace.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Success / failure message.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>@tool\ndef edit_code(\n    old_code: str,\n    new_code: str,\n    filename: str,\n    state: Annotated[dict, InjectedState],\n) -&gt; str:\n    \"\"\"Replace the **first** occurrence of *old_code* with *new_code* in *filename*.\n\n    Args:\n        old_code: Code fragment to search for.\n        new_code: Replacement fragment.\n        filename: Target file inside the workspace.\n\n    Returns:\n        Success / failure message.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    console.print(\"[cyan]Editing file:[/cyan]\", filename)\n\n    code_file = os.path.join(workspace_dir, filename)\n    try:\n        with open(code_file, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n    except FileNotFoundError:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]File not found:[/]\",\n            filename,\n        )\n        return f\"Failed: {filename} not found.\"\n\n    # Clean up markdown fences\n    old_code_clean = _strip_fences(old_code)\n    new_code_clean = _strip_fences(new_code)\n\n    if old_code_clean not in content:\n        console.print(\n            \"[yellow] \u26a0\ufe0f 'old_code' not found in file'; no changes made.[/]\"\n        )\n        return f\"No changes made to {filename}: 'old_code' not found in file.\"\n\n    updated = content.replace(old_code_clean, new_code_clean, 1)\n\n    console.print(\n        Panel(\n            DiffRenderer(content, updated, filename),\n            title=\"Diff Preview\",\n            border_style=\"cyan\",\n        )\n    )\n\n    try:\n        with open(code_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(updated)\n    except Exception as exc:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]Failed to write file:[/]\",\n            exc,\n        )\n        return f\"Failed to edit {filename}.\"\n\n    console.print(\n        f\"[bold bright_white on green] :heavy_check_mark: [/] \"\n        f\"[green]File updated:[/] {code_file}\"\n    )\n    file_list = state.get(\"code_files\", [])\n    if code_file not in file_list:\n        file_list.append(filename)\n    state[\"code_files\"] = file_list\n\n    return f\"File {filename} updated successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Execute a shell command in the workspace and return its combined output.</p> <p>Runs the specified command using subprocess.run in the given workspace directory, captures stdout and stderr, enforces a maximum character budget, and formats both streams into a single string. KeyboardInterrupt during execution is caught and reported.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The shell command to execute.</p> required <code>state</code> <code>Annotated[dict, InjectedState]</code> <p>A dict with injected state; must include the 'workspace' path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A formatted string with \"STDOUT:\" followed by the truncated stdout and</p> <code>str</code> <p>\"STDERR:\" followed by the truncated stderr.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"Execute a shell command in the workspace and return its combined output.\n\n    Runs the specified command using subprocess.run in the given workspace\n    directory, captures stdout and stderr, enforces a maximum character budget,\n    and formats both streams into a single string. KeyboardInterrupt during\n    execution is caught and reported.\n\n    Args:\n        query: The shell command to execute.\n        state: A dict with injected state; must include the 'workspace' path.\n\n    Returns:\n        A formatted string with \"STDOUT:\" followed by the truncated stdout and\n        \"STDERR:\" followed by the truncated stderr.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n\n    print(\"RUNNING: \", query)\n    try:\n        result = subprocess.run(\n            query,\n            text=True,\n            shell=True,\n            timeout=60000,\n            capture_output=True,\n            cwd=workspace_dir,\n        )\n        stdout, stderr = result.stdout, result.stderr\n    except KeyboardInterrupt:\n        print(\"Keyboard Interrupt of command: \", query)\n        stdout, stderr = \"\", \"KeyboardInterrupt:\"\n\n    # Fit BOTH streams under a single overall cap\n    stdout_fit, stderr_fit = _fit_streams_to_budget(\n        stdout or \"\", stderr or \"\", MAX_TOOL_MSG_CHARS\n    )\n\n    print(\"STDOUT: \", stdout_fit)\n    print(\"STDERR: \", stderr_fit)\n\n    return f\"STDOUT:\\n{stdout_fit}\\nSTDERR:\\n{stderr_fit}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.should_continue","title":"<code>should_continue(state)</code>","text":"<p>Return 'summarize' if no tool calls in the last message, else 'continue'.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The current execution state containing messages.</p> required <p>Returns:</p> Type Description <code>Literal['summarize', 'continue']</code> <p>A literal \"summarize\" if the last message has no tool calls,</p> <code>Literal['summarize', 'continue']</code> <p>otherwise \"continue\".</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def should_continue(state: ExecutionState) -&gt; Literal[\"summarize\", \"continue\"]:\n    \"\"\"Return 'summarize' if no tool calls in the last message, else 'continue'.\n\n    Args:\n        state: The current execution state containing messages.\n\n    Returns:\n        A literal \"summarize\" if the last message has no tool calls,\n        otherwise \"continue\".\n    \"\"\"\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no tool call, then we finish\n    if not last_message.tool_calls:\n        return \"summarize\"\n    # Otherwise if there is, we continue\n    else:\n        return \"continue\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.write_code","title":"<code>write_code(code, filename, tool_call_id, state)</code>","text":"<p>Write source code to a file and update the agent\u2019s workspace state.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The source code content to be written to disk.</p> required <code>filename</code> <code>str</code> <p>Name of the target file (including its extension).</p> required <code>tool_call_id</code> <code>Annotated[str, InjectedToolCallId]</code> <p>Identifier for this tool invocation.</p> required <code>state</code> <code>Annotated[dict, InjectedState]</code> <p>Agent state dict holding workspace path and file list.</p> required <p>Returns:</p> Name Type Description <code>Command</code> <code>Command</code> <p>Contains an updated state (including code_files) and</p> <code>Command</code> <p>a ToolMessage acknowledging success or failure.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>@tool\ndef write_code(\n    code: str,\n    filename: str,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n    state: Annotated[dict, InjectedState],\n) -&gt; Command:\n    \"\"\"Write source code to a file and update the agent\u2019s workspace state.\n\n    Args:\n        code: The source code content to be written to disk.\n        filename: Name of the target file (including its extension).\n        tool_call_id: Identifier for this tool invocation.\n        state: Agent state dict holding workspace path and file list.\n\n    Returns:\n        Command: Contains an updated state (including code_files) and\n        a ToolMessage acknowledging success or failure.\n    \"\"\"\n    # Determine the full path to the target file\n    workspace_dir = state[\"workspace\"]\n    console.print(\"[cyan]Writing file:[/]\", filename)\n\n    # Clean up markdown fences on submitted code.\n    code = _strip_fences(code)\n\n    # Show syntax-highlighted preview before writing to file\n    try:\n        lexer_name = Syntax.guess_lexer(filename, code)\n    except Exception:\n        lexer_name = \"text\"\n\n    console.print(\n        Panel(\n            Syntax(code, lexer_name, line_numbers=True),\n            title=\"File Preview\",\n            border_style=\"cyan\",\n        )\n    )\n\n    # Write cleaned code to disk\n    code_file = os.path.join(workspace_dir, filename)\n    try:\n        with open(code_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(code)\n    except Exception as exc:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]Failed to write file:[/]\",\n            exc,\n        )\n        return f\"Failed to write {filename}.\"\n\n    console.print(\n        f\"[bold bright_white on green] :heavy_check_mark: [/] \"\n        f\"[green]File written:[/] {code_file}\"\n    )\n\n    # Append the file to the list in agent's state for later reference\n    file_list = state.get(\"code_files\", [])\n    if filename not in file_list:\n        file_list.append(filename)\n\n    # Create a tool message to send back to acknowledge success.\n    msg = ToolMessage(\n        content=f\"File {filename} written successfully.\",\n        tool_call_id=tool_call_id,\n    )\n\n    # Return updated code files list &amp; the message\n    return Command(\n        update={\n            \"code_files\": file_list,\n            \"messages\": [msg],\n        }\n    )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent","title":"<code>hypothesizer_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent","title":"<code>HypothesizerAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>class HypothesizerAgent(BaseAgent):\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.hypothesizer_prompt = hypothesizer_prompt\n        self.critic_prompt = critic_prompt\n        self.competitor_prompt = competitor_prompt\n        self.search_tool = DDGS()\n        # self.search_tool = TavilySearchResults(\n        #     max_results=10, search_depth=\"advanced\", include_answer=False\n        # )\n\n        self._action = self._build_graph()\n\n    def agent1_generate_solution(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"Agent 1: Hypothesizer.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']}] Entering agent1_generate_solution. Iteration: {state['current_iteration']}\"\n        )\n\n        current_iter = state[\"current_iteration\"]\n        user_content = f\"Question: {state['question']}\\n\"\n\n        if current_iter &gt; 0:\n            user_content += (\n                f\"\\nPrevious solution: {state['agent1_solution'][-1]}\"\n            )\n            user_content += f\"\\nCritique: {state['agent2_critiques'][-1]}\"\n            user_content += (\n                f\"\\nCompetitor perspective: {state['agent3_perspectives'][-1]}\"\n            )\n            user_content += (\n                \"\\n\\n**You must explicitly list how this new solution differs from the previous solution,** \"\n                \"point by point, explaining what changes were made in response to the critique and competitor perspective.\"\n                \"\\nAfterward, provide your updated solution.\"\n            )\n        else:\n            user_content += \"Research this problem and generate a solution.\"\n\n        search_query = self.llm.invoke(\n            f\"Here is a problem description: {state['question']}. Turn it into a short query to be fed into a search engine.\"\n        ).content\n        if '\"' in search_query:\n            search_query = search_query.split('\"')[1]\n        raw_search_results = self.search_tool.text(\n            search_query, backend=\"duckduckgo\"\n        )\n\n        # Parse the results if possible, so we can collect URLs\n        new_state = state.copy()\n        new_state[\"question_search_query\"] = search_query\n        if \"visited_sites\" not in new_state:\n            new_state[\"visited_sites\"] = []\n\n        try:\n            if isinstance(raw_search_results, str):\n                results_list = ast.literal_eval(raw_search_results)\n            else:\n                results_list = raw_search_results\n            # Each item typically might have \"link\", \"title\", \"snippet\"\n            for item in results_list:\n                link = item.get(\"link\")\n                if link:\n                    # print(f\"[DEBUG] Appending visited link: {link}\")\n                    new_state[\"visited_sites\"].append(link)\n        except (ValueError, SyntaxError, TypeError):\n            # If it's not valid Python syntax or something else goes wrong\n            print(\"[DEBUG] Could not parse search results as Python list.\")\n            print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n        user_content += f\"\\nSearch results: {raw_search_results}\"\n\n        # Provide a system message to define this agent's role\n        messages = [\n            SystemMessage(content=self.hypothesizer_prompt),\n            HumanMessage(content=user_content),\n        ]\n        solution = self.llm.invoke(messages)\n\n        new_state[\"agent1_solution\"].append(solution.content)\n\n        # Print the entire solution in green\n        print(\n            f\"{GREEN}[Agent1 - Hypothesizer solution]\\n{solution.content}{RESET}\"\n        )\n        print(\n            f\"[iteration {state['current_iteration']}] Exiting agent1_generate_solution.\"\n        )\n        return new_state\n\n    def agent2_critique(self, state: HypothesizerState) -&gt; HypothesizerState:\n        \"\"\"Agent 2: Critic.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']}] Entering agent2_critique.\"\n        )\n\n        solution = state[\"agent1_solution\"][-1]\n        user_content = (\n            f\"Question: {state['question']}\\n\"\n            f\"Proposed solution: {solution}\\n\"\n            \"Provide a detailed critique of this solution. Identify potential flaws, assumptions, and areas for improvement.\"\n        )\n\n        fact_check_query = f\"fact check {state['question_search_query']} solution effectiveness\"\n\n        raw_search_results = self.search_tool.text(\n            fact_check_query, backend=\"duckduckgo\"\n        )\n\n        # Parse the results if possible, so we can collect URLs\n        new_state = state.copy()\n        if \"visited_sites\" not in new_state:\n            new_state[\"visited_sites\"] = []\n\n        try:\n            if isinstance(raw_search_results, str):\n                results_list = ast.literal_eval(raw_search_results)\n            else:\n                results_list = raw_search_results\n            # Each item typically might have \"link\", \"title\", \"snippet\"\n            for item in results_list:\n                link = item.get(\"link\")\n                if link:\n                    # print(f\"[DEBUG] Appending visited link: {link}\")\n                    new_state[\"visited_sites\"].append(link)\n        except (ValueError, SyntaxError, TypeError):\n            # If it's not valid Python syntax or something else goes wrong\n            print(\"[DEBUG] Could not parse search results as Python list.\")\n            print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n        fact_check_results = raw_search_results\n        user_content += f\"\\nFact check results: {fact_check_results}\"\n\n        messages = [\n            SystemMessage(content=self.critic_prompt),\n            HumanMessage(content=user_content),\n        ]\n        critique = self.llm.invoke(messages)\n\n        new_state[\"agent2_critiques\"].append(critique.content)\n\n        # Print the entire critique in blue\n        print(f\"{BLUE}[Agent2 - Critic]\\n{critique.content}{RESET}\")\n        print(\n            f\"[iteration {state['current_iteration']}] Exiting agent2_critique.\"\n        )\n        return new_state\n\n    def agent3_competitor_perspective(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"Agent 3: Competitor/Stakeholder Simulator.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']}] Entering agent3_competitor_perspective.\"\n        )\n\n        solution = state[\"agent1_solution\"][-1]\n        critique = state[\"agent2_critiques\"][-1]\n\n        user_content = (\n            f\"Question: {state['question']}\\n\"\n            f\"Proposed solution: {solution}\\n\"\n            f\"Critique: {critique}\\n\"\n            \"Simulate how a competitor, government agency, or other stakeholder might respond to this solution.\"\n        )\n\n        competitor_search_query = (\n            f\"competitor responses to {state['question_search_query']}\"\n        )\n\n        raw_search_results = self.search_tool.text(\n            competitor_search_query, backend=\"duckduckgo\"\n        )\n\n        # Parse the results if possible, so we can collect URLs\n        new_state = state.copy()\n        if \"visited_sites\" not in new_state:\n            new_state[\"visited_sites\"] = []\n\n        try:\n            if isinstance(raw_search_results, str):\n                results_list = ast.literal_eval(raw_search_results)\n            else:\n                results_list = raw_search_results\n            # Each item typically might have \"link\", \"title\", \"snippet\"\n            for item in results_list:\n                link = item.get(\"link\")\n                if link:\n                    # print(f\"[DEBUG] Appending visited link: {link}\")\n                    new_state[\"visited_sites\"].append(link)\n        except (ValueError, SyntaxError, TypeError):\n            # If it's not valid Python syntax or something else goes wrong\n            print(\"[DEBUG] Could not parse search results as Python list.\")\n            print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n        competitor_info = raw_search_results\n        user_content += f\"\\nCompetitor information: {competitor_info}\"\n\n        messages = [\n            SystemMessage(content=self.competitor_prompt),\n            HumanMessage(content=user_content),\n        ]\n        perspective = self.llm.invoke(messages)\n\n        new_state[\"agent3_perspectives\"].append(perspective.content)\n\n        # Print the entire perspective in red\n        print(\n            f\"{RED}[Agent3 - Competitor/Stakeholder Perspective]\\n{perspective.content}{RESET}\"\n        )\n        print(\n            f\"[iteration {state['current_iteration']}] Exiting agent3_competitor_perspective.\"\n        )\n        return new_state\n\n    def increment_iteration(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        new_state = state.copy()\n        new_state[\"current_iteration\"] += 1\n        print(\n            f\"[iteration {state['current_iteration']}] Iteration incremented to {new_state['current_iteration']}\"\n        )\n        return new_state\n\n    def generate_solution(self, state: HypothesizerState) -&gt; HypothesizerState:\n        \"\"\"Generate the overall, refined solution based on all iterations.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']}] Entering generate_solution.\"\n        )\n        prompt = f\"Original question: {state['question']}\\n\\n\"\n        prompt += \"Evolution of solutions:\\n\"\n\n        for i in range(state[\"max_iterations\"]):\n            prompt += f\"\\nIteration {i + 1}:\\n\"\n            prompt += f\"Solution: {state['agent1_solution'][i]}\\n\"\n            prompt += f\"Critique: {state['agent2_critiques'][i]}\\n\"\n            prompt += (\n                f\"Competitor perspective: {state['agent3_perspectives'][i]}\\n\"\n            )\n\n        prompt += \"\\nBased on this iterative process, provide the overall, refined solution.\"\n\n        print(\n            f\"[iteration {state['current_iteration']}] Generating overall solution with LLM...\"\n        )\n        solution = self.llm.invoke(prompt)\n        print(\n            f\"[iteration {state['current_iteration']}] Overall solution obtained. Preview:\",\n            solution.content[:200],\n            \"...\",\n        )\n\n        new_state = state.copy()\n        new_state[\"solution\"] = solution.content\n\n        print(\n            f\"[iteration {state['current_iteration']}] Exiting generate_solution.\"\n        )\n        return new_state\n\n    def print_visited_sites(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        new_state = state.copy()\n        # all_sites = new_state.get(\"visited_sites\", [])\n        # print(\"[DEBUG] Visited Sites:\")\n        # for s in all_sites:\n        #     print(\"  \", s)\n        return new_state\n\n    def summarize_process_as_latex(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"\n        Summarize how the solution changed over time, referencing\n        each iteration's critique and competitor perspective,\n        then produce a final LaTeX document.\n        \"\"\"\n        print(\"Entering summarize_process_as_latex.\")\n        llm_model = state.get(\"llm_model\", \"unknown_model\")\n\n        # Build a single string describing the entire iterative process\n        iteration_details = \"\"\n        for i, (sol, crit, comp) in enumerate(\n            zip(\n                state[\"agent1_solution\"],\n                state[\"agent2_critiques\"],\n                state[\"agent3_perspectives\"],\n            ),\n            start=1,\n        ):\n            iteration_details += (\n                f\"\\\\subsection*{{Iteration {i}}}\\n\\n\"\n                f\"\\\\textbf{{Solution:}}\\\\\\\\\\n{sol}\\n\\n\"\n                f\"\\\\textbf{{Critique:}}\\\\\\\\\\n{crit}\\n\\n\"\n                f\"\\\\textbf{{Competitor Perspective:}}\\\\\\\\\\n{comp}\\n\\n\"\n            )\n\n        # -----------------------------\n        # Write iteration_details to disk as .txt\n        # -----------------------------\n        timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        txt_filename = (\n            f\"iteration_details_{llm_model}_{timestamp_str}_chat_history.txt\"\n        )\n        with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(iteration_details)\n\n        print(f\"Wrote iteration details to {txt_filename}.\")\n\n        # Prompt the LLM to produce a LaTeX doc\n        # We'll just pass it as a single string to the LLM;\n        # you could also do system+human messages if you prefer.\n        prompt = f\"\"\"\\\n            You are a system that produces a FULL LaTeX document.\n            Here is information about a multi-iteration process:\n\n            Original question: {state[\"question\"]}\n\n            Below are the solutions, critiques, and competitor perspectives from each iteration:\n\n            {iteration_details}\n\n            The solution we arrived at was:\n\n            {state[\"solution\"]}\n\n            Now produce a valid LaTeX document.  Be sure to use a table of contents.\n            It must start with an Executive Summary (that may be multiple pages) which summarizes\n            the entire iterative process.  Following that, we should include the solution in full,\n            not summarized, but reformatted for appropriate LaTeX.  And then, finally (and this will be\n            quite long), we must take all the steps - solutions, critiques, and competitor perspectives\n            and *NOT SUMMARIZE THEM* but merely reformat them for the reader.  This will be in an Appendix\n            of the full content of the steps.  Finally, include a listing of all of the websites we\n            used in our research.\n\n            You must ONLY RETURN LaTeX, nothing else.  It must be valid LaTeX syntax!\n\n            Your output should start with:\n            \\\\documentclass{{article}}\n            \\\\usepackage[margin=1in]{{geometry}}\n            etc.\n\n            It must compile without errors under pdflatex. \n        \"\"\"\n\n        # Now produce a valid LaTeX document that nicely summarizes this entire iterative process.\n        # It must include the overall solution in full, not summarized, but reformatted for appropriate\n        # LaTeX. The summarization is for the other steps.\n\n        # all_visited_sites = state.get(\"visited_sites\", [])\n        # (Optional) remove duplicates by converting to a set, then back to a list\n        # visited_sites_unique = list(set(all_visited_sites))\n        # if visited_sites_unique:\n        #     websites_latex = \"\\\\section*{Websites Visited}\\\\begin{itemize}\\n\"\n        #     for url in visited_sites_unique:\n        #         print(f\"We visited: {url}\")\n        #         # Use \\url{} to handle special characters in URLs\n        #         websites_latex += f\"\\\\item \\\\url{{{url}}}\\n\"\n        #     websites_latex += \"\\\\end{itemize}\\n\\n\"\n        # else:\n        #     # If no sites visited, or the list is empty\n        #     websites_latex = (\n        #         \"\\\\section*{Websites Visited}\\nNo sites were visited.\\n\\n\"\n        #     )\n        # print(websites_latex)\n        websites_latex = \"\"\n\n        # Ask the LLM to produce *only* LaTeX content\n        latex_response = self.llm.invoke(prompt)\n\n        latex_doc = latex_response.content\n\n        def inject_into_latex(original_tex: str, injection: str) -&gt; str:\n            \"\"\"\n            Find the last occurrence of '\\\\end{document}' in 'original_tex'\n            and insert 'injection' right before it.\n            If '\\\\end{document}' is not found, just append the injection at the end.\n            \"\"\"\n            injection_index = original_tex.rfind(r\"\\end{document}\")\n            if injection_index == -1:\n                # If the LLM didn't include \\end{document}, just append\n                return original_tex + \"\\n\" + injection\n            else:\n                # Insert right before \\end{document}\n                return (\n                    original_tex[:injection_index]\n                    + \"\\n\"\n                    + injection\n                    + \"\\n\"\n                    + original_tex[injection_index:]\n                )\n\n        final_latex = inject_into_latex(latex_doc, websites_latex)\n\n        new_state = state.copy()\n        new_state[\"summary_report\"] = final_latex\n\n        print(\n            f\"[iteration {state['current_iteration']}] Received LaTeX from LLM. Preview:\"\n        )\n        print(latex_response.content[:300], \"...\")\n        print(\n            f\"[iteration {state['current_iteration']}] Exiting summarize_process_as_latex.\"\n        )\n        return new_state\n\n    def _build_graph(self):\n        # Initialize the graph\n        graph = StateGraph(HypothesizerState)\n\n        # Add nodes\n        self.add_node(graph, self.agent1_generate_solution, \"agent1\")\n        self.add_node(graph, self.agent2_critique, \"agent2\")\n        self.add_node(graph, self.agent3_competitor_perspective, \"agent3\")\n        self.add_node(graph, self.increment_iteration, \"increment_iteration\")\n        self.add_node(graph, self.generate_solution, \"finalize\")\n        self.add_node(graph, self.print_visited_sites, \"print_sites\")\n        self.add_node(\n            graph, self.summarize_process_as_latex, \"summarize_as_latex\"\n        )\n        # self.graph.add_node(\"compile_pdf\",                compile_summary_to_pdf)\n\n        # Add simple edges for the known flow\n        graph.add_edge(\"agent1\", \"agent2\")\n        graph.add_edge(\"agent2\", \"agent3\")\n        graph.add_edge(\"agent3\", \"increment_iteration\")\n\n        # Then from increment_iteration, we have a conditional:\n        # If we 'continue', we go back to agent1\n        # If we 'finish', we jump to the finalize node\n        graph.add_conditional_edges(\n            \"increment_iteration\",\n            should_continue,\n            {\"continue\": \"agent1\", \"finish\": \"finalize\"},\n        )\n\n        graph.add_edge(\"finalize\", \"summarize_as_latex\")\n        graph.add_edge(\"summarize_as_latex\", \"print_sites\")\n        # self.graph.add_edge(\"summarize_as_latex\", \"compile_pdf\")\n        # self.graph.add_edge(\"compile_pdf\", \"print_sites\")\n\n        # Set the entry point\n        graph.set_entry_point(\"agent1\")\n        graph.set_finish_point(\"print_sites\")\n\n        return graph.compile(checkpointer=self.checkpointer)\n        # self.action.get_graph().draw_mermaid_png(output_file_path=\"hypothesizer_agent_graph.png\", draw_method=MermaidDrawMethod.PYPPETEER)\n\n    def _invoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 100000, **_\n    ):\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n        if \"prompt\" not in inputs:\n            raise KeyError(\"'prompt' is a required arguments\")\n\n        inputs[\"question\"] = inputs[\"prompt\"]\n        inputs[\"max_iterations\"] = inputs.get(\"max_iterations\", 3)\n        inputs[\"current_iteration\"] = 0\n        inputs[\"agent1_solution\"] = []\n        inputs[\"agent2_critiques\"] = []\n        inputs[\"agent3_perspectives\"] = []\n        inputs[\"solution\"] = \"\"\n\n        return self._action.invoke(inputs, config)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent1_generate_solution","title":"<code>agent1_generate_solution(state)</code>","text":"<p>Agent 1: Hypothesizer.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent1_generate_solution(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"Agent 1: Hypothesizer.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']}] Entering agent1_generate_solution. Iteration: {state['current_iteration']}\"\n    )\n\n    current_iter = state[\"current_iteration\"]\n    user_content = f\"Question: {state['question']}\\n\"\n\n    if current_iter &gt; 0:\n        user_content += (\n            f\"\\nPrevious solution: {state['agent1_solution'][-1]}\"\n        )\n        user_content += f\"\\nCritique: {state['agent2_critiques'][-1]}\"\n        user_content += (\n            f\"\\nCompetitor perspective: {state['agent3_perspectives'][-1]}\"\n        )\n        user_content += (\n            \"\\n\\n**You must explicitly list how this new solution differs from the previous solution,** \"\n            \"point by point, explaining what changes were made in response to the critique and competitor perspective.\"\n            \"\\nAfterward, provide your updated solution.\"\n        )\n    else:\n        user_content += \"Research this problem and generate a solution.\"\n\n    search_query = self.llm.invoke(\n        f\"Here is a problem description: {state['question']}. Turn it into a short query to be fed into a search engine.\"\n    ).content\n    if '\"' in search_query:\n        search_query = search_query.split('\"')[1]\n    raw_search_results = self.search_tool.text(\n        search_query, backend=\"duckduckgo\"\n    )\n\n    # Parse the results if possible, so we can collect URLs\n    new_state = state.copy()\n    new_state[\"question_search_query\"] = search_query\n    if \"visited_sites\" not in new_state:\n        new_state[\"visited_sites\"] = []\n\n    try:\n        if isinstance(raw_search_results, str):\n            results_list = ast.literal_eval(raw_search_results)\n        else:\n            results_list = raw_search_results\n        # Each item typically might have \"link\", \"title\", \"snippet\"\n        for item in results_list:\n            link = item.get(\"link\")\n            if link:\n                # print(f\"[DEBUG] Appending visited link: {link}\")\n                new_state[\"visited_sites\"].append(link)\n    except (ValueError, SyntaxError, TypeError):\n        # If it's not valid Python syntax or something else goes wrong\n        print(\"[DEBUG] Could not parse search results as Python list.\")\n        print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n    user_content += f\"\\nSearch results: {raw_search_results}\"\n\n    # Provide a system message to define this agent's role\n    messages = [\n        SystemMessage(content=self.hypothesizer_prompt),\n        HumanMessage(content=user_content),\n    ]\n    solution = self.llm.invoke(messages)\n\n    new_state[\"agent1_solution\"].append(solution.content)\n\n    # Print the entire solution in green\n    print(\n        f\"{GREEN}[Agent1 - Hypothesizer solution]\\n{solution.content}{RESET}\"\n    )\n    print(\n        f\"[iteration {state['current_iteration']}] Exiting agent1_generate_solution.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent2_critique","title":"<code>agent2_critique(state)</code>","text":"<p>Agent 2: Critic.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent2_critique(self, state: HypothesizerState) -&gt; HypothesizerState:\n    \"\"\"Agent 2: Critic.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']}] Entering agent2_critique.\"\n    )\n\n    solution = state[\"agent1_solution\"][-1]\n    user_content = (\n        f\"Question: {state['question']}\\n\"\n        f\"Proposed solution: {solution}\\n\"\n        \"Provide a detailed critique of this solution. Identify potential flaws, assumptions, and areas for improvement.\"\n    )\n\n    fact_check_query = f\"fact check {state['question_search_query']} solution effectiveness\"\n\n    raw_search_results = self.search_tool.text(\n        fact_check_query, backend=\"duckduckgo\"\n    )\n\n    # Parse the results if possible, so we can collect URLs\n    new_state = state.copy()\n    if \"visited_sites\" not in new_state:\n        new_state[\"visited_sites\"] = []\n\n    try:\n        if isinstance(raw_search_results, str):\n            results_list = ast.literal_eval(raw_search_results)\n        else:\n            results_list = raw_search_results\n        # Each item typically might have \"link\", \"title\", \"snippet\"\n        for item in results_list:\n            link = item.get(\"link\")\n            if link:\n                # print(f\"[DEBUG] Appending visited link: {link}\")\n                new_state[\"visited_sites\"].append(link)\n    except (ValueError, SyntaxError, TypeError):\n        # If it's not valid Python syntax or something else goes wrong\n        print(\"[DEBUG] Could not parse search results as Python list.\")\n        print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n    fact_check_results = raw_search_results\n    user_content += f\"\\nFact check results: {fact_check_results}\"\n\n    messages = [\n        SystemMessage(content=self.critic_prompt),\n        HumanMessage(content=user_content),\n    ]\n    critique = self.llm.invoke(messages)\n\n    new_state[\"agent2_critiques\"].append(critique.content)\n\n    # Print the entire critique in blue\n    print(f\"{BLUE}[Agent2 - Critic]\\n{critique.content}{RESET}\")\n    print(\n        f\"[iteration {state['current_iteration']}] Exiting agent2_critique.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent3_competitor_perspective","title":"<code>agent3_competitor_perspective(state)</code>","text":"<p>Agent 3: Competitor/Stakeholder Simulator.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent3_competitor_perspective(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"Agent 3: Competitor/Stakeholder Simulator.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']}] Entering agent3_competitor_perspective.\"\n    )\n\n    solution = state[\"agent1_solution\"][-1]\n    critique = state[\"agent2_critiques\"][-1]\n\n    user_content = (\n        f\"Question: {state['question']}\\n\"\n        f\"Proposed solution: {solution}\\n\"\n        f\"Critique: {critique}\\n\"\n        \"Simulate how a competitor, government agency, or other stakeholder might respond to this solution.\"\n    )\n\n    competitor_search_query = (\n        f\"competitor responses to {state['question_search_query']}\"\n    )\n\n    raw_search_results = self.search_tool.text(\n        competitor_search_query, backend=\"duckduckgo\"\n    )\n\n    # Parse the results if possible, so we can collect URLs\n    new_state = state.copy()\n    if \"visited_sites\" not in new_state:\n        new_state[\"visited_sites\"] = []\n\n    try:\n        if isinstance(raw_search_results, str):\n            results_list = ast.literal_eval(raw_search_results)\n        else:\n            results_list = raw_search_results\n        # Each item typically might have \"link\", \"title\", \"snippet\"\n        for item in results_list:\n            link = item.get(\"link\")\n            if link:\n                # print(f\"[DEBUG] Appending visited link: {link}\")\n                new_state[\"visited_sites\"].append(link)\n    except (ValueError, SyntaxError, TypeError):\n        # If it's not valid Python syntax or something else goes wrong\n        print(\"[DEBUG] Could not parse search results as Python list.\")\n        print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n    competitor_info = raw_search_results\n    user_content += f\"\\nCompetitor information: {competitor_info}\"\n\n    messages = [\n        SystemMessage(content=self.competitor_prompt),\n        HumanMessage(content=user_content),\n    ]\n    perspective = self.llm.invoke(messages)\n\n    new_state[\"agent3_perspectives\"].append(perspective.content)\n\n    # Print the entire perspective in red\n    print(\n        f\"{RED}[Agent3 - Competitor/Stakeholder Perspective]\\n{perspective.content}{RESET}\"\n    )\n    print(\n        f\"[iteration {state['current_iteration']}] Exiting agent3_competitor_perspective.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.generate_solution","title":"<code>generate_solution(state)</code>","text":"<p>Generate the overall, refined solution based on all iterations.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def generate_solution(self, state: HypothesizerState) -&gt; HypothesizerState:\n    \"\"\"Generate the overall, refined solution based on all iterations.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']}] Entering generate_solution.\"\n    )\n    prompt = f\"Original question: {state['question']}\\n\\n\"\n    prompt += \"Evolution of solutions:\\n\"\n\n    for i in range(state[\"max_iterations\"]):\n        prompt += f\"\\nIteration {i + 1}:\\n\"\n        prompt += f\"Solution: {state['agent1_solution'][i]}\\n\"\n        prompt += f\"Critique: {state['agent2_critiques'][i]}\\n\"\n        prompt += (\n            f\"Competitor perspective: {state['agent3_perspectives'][i]}\\n\"\n        )\n\n    prompt += \"\\nBased on this iterative process, provide the overall, refined solution.\"\n\n    print(\n        f\"[iteration {state['current_iteration']}] Generating overall solution with LLM...\"\n    )\n    solution = self.llm.invoke(prompt)\n    print(\n        f\"[iteration {state['current_iteration']}] Overall solution obtained. Preview:\",\n        solution.content[:200],\n        \"...\",\n    )\n\n    new_state = state.copy()\n    new_state[\"solution\"] = solution.content\n\n    print(\n        f\"[iteration {state['current_iteration']}] Exiting generate_solution.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.summarize_process_as_latex","title":"<code>summarize_process_as_latex(state)</code>","text":"<p>Summarize how the solution changed over time, referencing each iteration's critique and competitor perspective, then produce a final LaTeX document.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def summarize_process_as_latex(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"\n    Summarize how the solution changed over time, referencing\n    each iteration's critique and competitor perspective,\n    then produce a final LaTeX document.\n    \"\"\"\n    print(\"Entering summarize_process_as_latex.\")\n    llm_model = state.get(\"llm_model\", \"unknown_model\")\n\n    # Build a single string describing the entire iterative process\n    iteration_details = \"\"\n    for i, (sol, crit, comp) in enumerate(\n        zip(\n            state[\"agent1_solution\"],\n            state[\"agent2_critiques\"],\n            state[\"agent3_perspectives\"],\n        ),\n        start=1,\n    ):\n        iteration_details += (\n            f\"\\\\subsection*{{Iteration {i}}}\\n\\n\"\n            f\"\\\\textbf{{Solution:}}\\\\\\\\\\n{sol}\\n\\n\"\n            f\"\\\\textbf{{Critique:}}\\\\\\\\\\n{crit}\\n\\n\"\n            f\"\\\\textbf{{Competitor Perspective:}}\\\\\\\\\\n{comp}\\n\\n\"\n        )\n\n    # -----------------------------\n    # Write iteration_details to disk as .txt\n    # -----------------------------\n    timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    txt_filename = (\n        f\"iteration_details_{llm_model}_{timestamp_str}_chat_history.txt\"\n    )\n    with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(iteration_details)\n\n    print(f\"Wrote iteration details to {txt_filename}.\")\n\n    # Prompt the LLM to produce a LaTeX doc\n    # We'll just pass it as a single string to the LLM;\n    # you could also do system+human messages if you prefer.\n    prompt = f\"\"\"\\\n        You are a system that produces a FULL LaTeX document.\n        Here is information about a multi-iteration process:\n\n        Original question: {state[\"question\"]}\n\n        Below are the solutions, critiques, and competitor perspectives from each iteration:\n\n        {iteration_details}\n\n        The solution we arrived at was:\n\n        {state[\"solution\"]}\n\n        Now produce a valid LaTeX document.  Be sure to use a table of contents.\n        It must start with an Executive Summary (that may be multiple pages) which summarizes\n        the entire iterative process.  Following that, we should include the solution in full,\n        not summarized, but reformatted for appropriate LaTeX.  And then, finally (and this will be\n        quite long), we must take all the steps - solutions, critiques, and competitor perspectives\n        and *NOT SUMMARIZE THEM* but merely reformat them for the reader.  This will be in an Appendix\n        of the full content of the steps.  Finally, include a listing of all of the websites we\n        used in our research.\n\n        You must ONLY RETURN LaTeX, nothing else.  It must be valid LaTeX syntax!\n\n        Your output should start with:\n        \\\\documentclass{{article}}\n        \\\\usepackage[margin=1in]{{geometry}}\n        etc.\n\n        It must compile without errors under pdflatex. \n    \"\"\"\n\n    # Now produce a valid LaTeX document that nicely summarizes this entire iterative process.\n    # It must include the overall solution in full, not summarized, but reformatted for appropriate\n    # LaTeX. The summarization is for the other steps.\n\n    # all_visited_sites = state.get(\"visited_sites\", [])\n    # (Optional) remove duplicates by converting to a set, then back to a list\n    # visited_sites_unique = list(set(all_visited_sites))\n    # if visited_sites_unique:\n    #     websites_latex = \"\\\\section*{Websites Visited}\\\\begin{itemize}\\n\"\n    #     for url in visited_sites_unique:\n    #         print(f\"We visited: {url}\")\n    #         # Use \\url{} to handle special characters in URLs\n    #         websites_latex += f\"\\\\item \\\\url{{{url}}}\\n\"\n    #     websites_latex += \"\\\\end{itemize}\\n\\n\"\n    # else:\n    #     # If no sites visited, or the list is empty\n    #     websites_latex = (\n    #         \"\\\\section*{Websites Visited}\\nNo sites were visited.\\n\\n\"\n    #     )\n    # print(websites_latex)\n    websites_latex = \"\"\n\n    # Ask the LLM to produce *only* LaTeX content\n    latex_response = self.llm.invoke(prompt)\n\n    latex_doc = latex_response.content\n\n    def inject_into_latex(original_tex: str, injection: str) -&gt; str:\n        \"\"\"\n        Find the last occurrence of '\\\\end{document}' in 'original_tex'\n        and insert 'injection' right before it.\n        If '\\\\end{document}' is not found, just append the injection at the end.\n        \"\"\"\n        injection_index = original_tex.rfind(r\"\\end{document}\")\n        if injection_index == -1:\n            # If the LLM didn't include \\end{document}, just append\n            return original_tex + \"\\n\" + injection\n        else:\n            # Insert right before \\end{document}\n            return (\n                original_tex[:injection_index]\n                + \"\\n\"\n                + injection\n                + \"\\n\"\n                + original_tex[injection_index:]\n            )\n\n    final_latex = inject_into_latex(latex_doc, websites_latex)\n\n    new_state = state.copy()\n    new_state[\"summary_report\"] = final_latex\n\n    print(\n        f\"[iteration {state['current_iteration']}] Received LaTeX from LLM. Preview:\"\n    )\n    print(latex_response.content[:300], \"...\")\n    print(\n        f\"[iteration {state['current_iteration']}] Exiting summarize_process_as_latex.\"\n    )\n    return new_state\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.mp_agent","title":"<code>mp_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.mp_agent.MaterialsProjectAgent","title":"<code>MaterialsProjectAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/mp_agent.py</code> <pre><code>class MaterialsProjectAgent(BaseAgent):\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        summarize: bool = True,\n        max_results: int = 3,\n        database_path: str = \"mp_database\",\n        summaries_path: str = \"mp_summaries\",\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.summarize = summarize\n        self.max_results = max_results\n        self.database_path = database_path\n        self.summaries_path = summaries_path\n\n        os.makedirs(self.database_path, exist_ok=True)\n        os.makedirs(self.summaries_path, exist_ok=True)\n\n        self._action = self._build_graph()\n\n    def _fetch_node(self, state: dict) -&gt; dict:\n        f = state[\"query\"]\n        els = f[\"elements\"]  # e.g. [\"Ga\",\"In\"]\n        bg = (f[\"band_gap_min\"], f[\"band_gap_max\"])\n        e_above_hull = (0, 0)  # only on-hull (stable)\n        mats = []\n        with MPRester() as mpr:\n            # get ALL matching materials\u2026\n            all_results = mpr.materials.summary.search(\n                elements=els,\n                band_gap=bg,\n                energy_above_hull=e_above_hull,\n                is_stable=True,  # equivalent filter\n            )\n            # \u2026then take only the first `max_results`\n            for doc in all_results[: self.max_results]:\n                mid = doc.material_id\n                data = doc.dict()\n                # cache to disk\n                path = os.path.join(self.database_path, f\"{mid}.json\")\n                if not os.path.exists(path):\n                    with open(path, \"w\") as f:\n                        json.dump(data, f, indent=2)\n                mats.append({\"material_id\": mid, \"metadata\": data})\n\n        return {**state, \"materials\": mats}\n\n    def _summarize_node(self, state: dict) -&gt; dict:\n        \"\"\"Summarize each material via LLM over its metadata.\"\"\"\n        # prompt template\n        prompt = ChatPromptTemplate.from_template(\"\"\"\nYou are a materials-science assistant. Given the following metadata about a material, produce a concise summary focusing on its key properties:\n\n{metadata}\n        \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n\n        summaries = [None] * len(state[\"materials\"])\n\n        def process(i, mat):\n            mid = mat[\"material_id\"]\n            meta = mat[\"metadata\"]\n            # flatten metadata to text\n            text = \"\\n\".join(f\"{k}: {v}\" for k, v in meta.items())\n            # build or load summary\n            summary_file = os.path.join(\n                self.summaries_path, f\"{mid}_summary.txt\"\n            )\n            if os.path.exists(summary_file):\n                with open(summary_file) as f:\n                    return i, f.read()\n            # optional: vectorize &amp; retrieve, but here we just summarize full text\n            result = chain.invoke({\"metadata\": text})\n            with open(summary_file, \"w\") as f:\n                f.write(result)\n            return i, result\n\n        with ThreadPoolExecutor(\n            max_workers=min(8, len(state[\"materials\"]))\n        ) as exe:\n            futures = [\n                exe.submit(process, i, m)\n                for i, m in enumerate(state[\"materials\"])\n            ]\n            for future in tqdm(futures, desc=\"Summarizing materials\"):\n                i, summ = future.result()\n                summaries[i] = summ\n\n        return {**state, \"summaries\": summaries}\n\n    def _aggregate_node(self, state: dict) -&gt; dict:\n        \"\"\"Combine all summaries into a single, coherent answer.\"\"\"\n        combined = \"\\n\\n----\\n\\n\".join(\n            f\"[{i + 1}] {m['material_id']}\\n\\n{summary}\"\n            for i, (m, summary) in enumerate(\n                zip(state[\"materials\"], state[\"summaries\"])\n            )\n        )\n\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a materials informatics assistant. Below are brief summaries of several materials:\n\n        {summaries}\n\n        Answer the user\u2019s question in context:\n\n        {context}\n                \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n        final = chain.invoke({\n            \"summaries\": combined,\n            \"context\": state[\"context\"],\n        })\n        return {**state, \"final_summary\": final}\n\n    def _build_graph(self):\n        graph = StateGraph(dict)  # using plain dict for state\n        self.add_node(graph, self._fetch_node)\n        if self.summarize:\n            self.add_node(graph, self._summarize_node)\n            self.add_node(graph, self._aggregate_node)\n\n            graph.set_entry_point(\"_fetch_node\")\n            graph.add_edge(\"_fetch_node\", \"_summarize_node\")\n            graph.add_edge(\"_summarize_node\", \"_aggregate_node\")\n            graph.set_finish_point(\"_aggregate_node\")\n        else:\n            graph.set_entry_point(\"_fetch_node\")\n            graph.set_finish_point(\"_fetch_node\")\n        return graph.compile(checkpointer=self.checkpointer)\n\n    def _invoke(\n        self,\n        inputs: Mapping[str, Any],\n        *,\n        summarize: bool | None = None,\n        recursion_limit: int = 1000,\n        **_,\n    ) -&gt; str:\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n\n        if \"query\" not in inputs:\n            if \"mp_query\" in inputs:\n                # make a shallow copy and rename the key\n                inputs = dict(inputs)\n                inputs[\"query\"] = inputs.pop(\"mp_query\")\n            else:\n                raise KeyError(\n                    \"Missing 'query' in inputs (alias 'mp_query' also accepted).\"\n                )\n\n        result = self._action.invoke(inputs, config)\n\n        use_summary = self.summarize if summarize is None else summarize\n        return (\n            result.get(\"final_summary\", \"No summary generated.\")\n            if use_summary\n            else \"\\n\\nFinished Fetching Materials Database Information!\"\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.optimization_agent","title":"<code>optimization_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.optimization_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Run a commandline command from using the subprocess package in python</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>commandline command to be run as a string given to the subprocess.run command.</p> required Source code in <code>src/ursa/agents/optimization_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"\n    Run a commandline command from using the subprocess package in python\n\n    Args:\n        query: commandline command to be run as a string given to the subprocess.run command.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    print(\"RUNNING: \", query)\n    try:\n        process = subprocess.Popen(\n            query.split(\" \"),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            cwd=workspace_dir,\n        )\n\n        stdout, stderr = process.communicate(timeout=60000)\n    except KeyboardInterrupt:\n        print(\"Keyboard Interrupt of command: \", query)\n        stdout, stderr = \"\", \"KeyboardInterrupt:\"\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.optimization_agent.write_code","title":"<code>write_code(code, filename, state)</code>","text":"<p>Writes python or Julia code to a file in the given workspace as requested.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to write</p> required <code>filename</code> <code>str</code> <p>the filename with an appropriate extension for programming language (.py for python, .jl for Julia, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Execution results</p> Source code in <code>src/ursa/agents/optimization_agent.py</code> <pre><code>@tool\ndef write_code(\n    code: str, filename: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Writes python or Julia code to a file in the given workspace as requested.\n\n    Args:\n        code: The code to write\n        filename: the filename with an appropriate extension for programming language (.py for python, .jl for Julia, etc.)\n\n    Returns:\n        Execution results\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    print(\"Writing filename \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.planning_agent","title":"<code>planning_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.planning_agent.PlanningAgent","title":"<code>PlanningAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/planning_agent.py</code> <pre><code>class PlanningAgent(BaseAgent):\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.planner_prompt = planner_prompt\n        self.reflection_prompt = reflection_prompt\n        self._action = self._build_graph()\n\n    def generation_node(self, state: PlanningState) -&gt; PlanningState:\n        \"\"\"\n        Plan generation with structured output. Produces a JSON string in messages\n        and a parsed list of steps in state[\"plan_steps\"].\n        \"\"\"\n\n        print(\"PlanningAgent: generating . . .\")\n\n        messages = state[\"messages\"]\n        if isinstance(messages[0], SystemMessage):\n            messages[0] = SystemMessage(content=self.planner_prompt)\n        else:\n            messages = [SystemMessage(content=self.planner_prompt)] + messages\n\n        structured_llm = self.llm.with_structured_output(Plan)\n        plan_obj: Plan = structured_llm.invoke(\n            messages, self.build_config(tags=[\"planner\"])\n        )\n\n        try:\n            json_text = plan_obj.model_dump_json(indent=2)\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to serialize Plan object with Pydantic v2: {e}\"\n            )\n\n        return {\n            \"messages\": [AIMessage(content=json_text)],\n            \"plan_steps\": plan_obj.steps,\n        }\n\n    def reflection_node(self, state: PlanningState) -&gt; PlanningState:\n        print(\"PlanningAgent: reflecting . . .\")\n\n        cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n        translated = [state[\"messages\"][0]] + [\n            cls_map[msg.type](content=msg.content)\n            for msg in state[\"messages\"][1:]\n        ]\n        translated = [SystemMessage(content=reflection_prompt)] + translated\n        res = self.llm.invoke(\n            translated,\n            self.build_config(tags=[\"planner\", \"reflect\"]),\n        )\n        return {\n            \"messages\": [HumanMessage(content=res.content)],\n            \"reflection_steps\": state[\"reflection_steps\"] - 1,\n        }\n\n    def _build_graph(self):\n        graph = StateGraph(PlanningState)\n        self.add_node(graph, self.generation_node, \"generate\")\n        self.add_node(graph, self.reflection_node, \"reflect\")\n        graph.set_entry_point(\"generate\")\n        graph.add_edge(\"generate\", \"reflect\")\n        graph.add_conditional_edges(\n            \"reflect\",\n            self._wrap_cond(\n                _should_continue, \"should_continue\", \"planning_agent\"\n            ),\n            {\"generate\": \"generate\", \"END\": END},\n        )\n        return graph.compile(checkpointer=self.checkpointer)\n\n    def _invoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 1000, **_\n    ):\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"planner\"]\n        )\n        inputs.setdefault(\"reflection_steps\", 1)\n        return self._action.invoke(inputs, config)\n\n    def _stream(\n        self,\n        inputs: Mapping[str, Any],\n        *,\n        config: dict | None = None,\n        recursion_limit: int = 1000,\n        **_,\n    ) -&gt; Iterator[dict]:\n        # If you have defaults, merge them here:\n        default = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"planner\"]\n        )\n        if config:\n            merged = {**default, **config}\n            if \"configurable\" in config:\n                merged[\"configurable\"] = {\n                    **default.get(\"configurable\", {}),\n                    **config[\"configurable\"],\n                }\n        else:\n            merged = default\n\n        inputs.setdefault(\"reflection_steps\", 1)\n        # Delegate to the compiled graph's stream\n        yield from self._action.stream(inputs, merged)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.planning_agent.PlanningAgent.generation_node","title":"<code>generation_node(state)</code>","text":"<p>Plan generation with structured output. Produces a JSON string in messages and a parsed list of steps in state[\"plan_steps\"].</p> Source code in <code>src/ursa/agents/planning_agent.py</code> <pre><code>def generation_node(self, state: PlanningState) -&gt; PlanningState:\n    \"\"\"\n    Plan generation with structured output. Produces a JSON string in messages\n    and a parsed list of steps in state[\"plan_steps\"].\n    \"\"\"\n\n    print(\"PlanningAgent: generating . . .\")\n\n    messages = state[\"messages\"]\n    if isinstance(messages[0], SystemMessage):\n        messages[0] = SystemMessage(content=self.planner_prompt)\n    else:\n        messages = [SystemMessage(content=self.planner_prompt)] + messages\n\n    structured_llm = self.llm.with_structured_output(Plan)\n    plan_obj: Plan = structured_llm.invoke(\n        messages, self.build_config(tags=[\"planner\"])\n    )\n\n    try:\n        json_text = plan_obj.model_dump_json(indent=2)\n    except Exception as e:\n        raise RuntimeError(\n            f\"Failed to serialize Plan object with Pydantic v2: {e}\"\n        )\n\n    return {\n        \"messages\": [AIMessage(content=json_text)],\n        \"plan_steps\": plan_obj.steps,\n    }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.websearch_agent","title":"<code>websearch_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.websearch_agent.WebSearchAgentLegacy","title":"<code>WebSearchAgentLegacy</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/websearch_agent.py</code> <pre><code>class WebSearchAgentLegacy(BaseAgent):\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.websearch_prompt = websearch_prompt\n        self.reflection_prompt = reflection_prompt\n        self.tools = [search_tool, process_content]  # + cb_tools\n        self.has_internet = self._check_for_internet(\n            kwargs.get(\"url\", \"http://www.lanl.gov\")\n        )\n        self._build_graph()\n\n    def _review_node(self, state: WebSearchState) -&gt; WebSearchState:\n        if not self.has_internet:\n            return {\n                \"messages\": [\n                    HumanMessage(\n                        content=\"No internet for WebSearch Agent so no research to review.\"\n                    )\n                ],\n                \"urls_visited\": [],\n            }\n\n        translated = [SystemMessage(content=reflection_prompt)] + state[\n            \"messages\"\n        ]\n        res = self.llm.invoke(\n            translated, {\"configurable\": {\"thread_id\": self.thread_id}}\n        )\n        return {\"messages\": [HumanMessage(content=res.content)]}\n\n    def _response_node(self, state: WebSearchState) -&gt; WebSearchState:\n        if not self.has_internet:\n            return {\n                \"messages\": [\n                    HumanMessage(\n                        content=\"No internet for WebSearch Agent. No research carried out.\"\n                    )\n                ],\n                \"urls_visited\": [],\n            }\n\n        messages = state[\"messages\"] + [SystemMessage(content=summarize_prompt)]\n        response = self.llm.invoke(\n            messages, {\"configurable\": {\"thread_id\": self.thread_id}}\n        )\n\n        urls_visited = []\n        for message in messages:\n            if message.model_dump().get(\"tool_calls\", []):\n                if \"url\" in message.tool_calls[0][\"args\"]:\n                    urls_visited.append(message.tool_calls[0][\"args\"][\"url\"])\n        return {\"messages\": [response.content], \"urls_visited\": urls_visited}\n\n    def _check_for_internet(self, url, timeout=2):\n        \"\"\"\n        Checks for internet connectivity by attempting an HTTP GET request.\n        \"\"\"\n        try:\n            requests.get(url, timeout=timeout)\n            return True\n        except (requests.ConnectionError, requests.Timeout):\n            return False\n\n    def _state_store_node(self, state: WebSearchState) -&gt; WebSearchState:\n        state[\"thread_id\"] = self.thread_id\n        return state\n        # return dict(**state, thread_id=self.thread_id)\n\n    def _create_react(self, state: WebSearchState) -&gt; WebSearchState:\n        react_agent = create_agent(\n            self.llm,\n            self.tools,\n            state_schema=WebSearchState,\n            system_prompt=self.websearch_prompt,\n        )\n        return react_agent.invoke(state)\n\n    def _build_graph(self):\n        graph = StateGraph(WebSearchState)\n        self.add_node(graph, self._state_store_node)\n        self.add_node(graph, self._create_react)\n        self.add_node(graph, self._review_node)\n        self.add_node(graph, self._response_node)\n\n        graph.set_entry_point(\"_state_store_node\")\n        graph.add_edge(\"_state_store_node\", \"_create_react\")\n        graph.add_edge(\"_create_react\", \"_review_node\")\n        graph.set_finish_point(\"_response_node\")\n\n        graph.add_conditional_edges(\n            \"_review_node\",\n            should_continue,\n            {\n                \"_create_react\": \"_create_react\",\n                \"_response_node\": \"_response_node\",\n            },\n        )\n        self._action = graph.compile(checkpointer=self.checkpointer)\n        # self._action.get_graph().draw_mermaid_png(output_file_path=\"./websearch_agent_graph.png\", draw_method=MermaidDrawMethod.PYPPETEER)\n\n    def _invoke(\n        self, inputs: Mapping[str, Any], recursion_limit: int = 1000, **_\n    ):\n        config = self.build_config(\n            recursion_limit=recursion_limit, tags=[\"graph\"]\n        )\n        return self._action.invoke(inputs, config)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.websearch_agent.process_content","title":"<code>process_content(url, context, state)</code>","text":"<p>Processes content from a given webpage.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>string with the url to obtain text content from.</p> required <code>context</code> <code>str</code> <p>string summary of the information the agent wants from the url for summarizing salient information.</p> required Source code in <code>src/ursa/agents/websearch_agent.py</code> <pre><code>def process_content(\n    url: str, context: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Processes content from a given webpage.\n\n    Args:\n        url: string with the url to obtain text content from.\n        context: string summary of the information the agent wants from the url for summarizing salient information.\n    \"\"\"\n    print(\"Parsing information from \", url)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    content_prompt = f\"\"\"\n    Here is the full content:\n    {soup.get_text()}\n\n    Carefully summarize the content in full detail, given the following context:\n    {context}\n    \"\"\"\n    summarized_information = (\n        state[\"model\"]\n        .invoke(\n            content_prompt, {\"configurable\": {\"thread_id\": state[\"thread_id\"]}}\n        )\n        .content\n    )\n    return summarized_information\n</code></pre>"},{"location":"api_reference/prompt_library/","title":"prompt_library","text":""},{"location":"api_reference/tools/","title":"tools","text":""},{"location":"api_reference/tools/#ursa.tools.feasibility_checker","title":"<code>feasibility_checker</code>","text":""},{"location":"api_reference/tools/#ursa.tools.feasibility_checker.heuristic_feasibility_check","title":"<code>heuristic_feasibility_check(constraints, variable_name, variable_type, variable_bounds, samples=10000)</code>","text":"<p>A tool for checking feasibility of the constraints.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Annotated[List[str], \"List of strings like 'x0+x1&lt;=5'\"]</code> <p>list of strings like 'x0 + x1 &lt;= 5', etc.</p> required <code>variable_name</code> <code>Annotated[List[str], \"List of strings like 'x0', 'x1', etc.\"]</code> <p>list of strings containing variable names used in constraint expressions.</p> required <code>variable_type</code> <code>Annotated[List[str], \"List of strings like 'real', 'integer', 'boolean', etc.\"]</code> <p>list of strings like 'real', 'integer', 'boolean', etc.</p> required <code>variable_bounds</code> <code>Annotated[List[List[float]], \"List of (lower bound, upper bound) tuples for x0, x1, ...'\"]</code> <p>list of (lower, upper) tuples for x0, x1, etc.</p> required <code>samples</code> <code>Annotated[int, 'Number of random sample. Default 10000']</code> <p>number of random samples, default value 10000</p> <code>10000</code> <p>Returns:</p> Type Description <code>Tuple[str]</code> <p>A string indicating whether a feasible solution was found.</p> Source code in <code>src/ursa/tools/feasibility_checker.py</code> <pre><code>@tool(parse_docstring=True)\ndef heuristic_feasibility_check(\n    constraints: Annotated[List[str], \"List of strings like 'x0+x1&lt;=5'\"],\n    variable_name: Annotated[\n        List[str], \"List of strings like 'x0', 'x1', etc.\"\n    ],\n    variable_type: Annotated[\n        List[str], \"List of strings like 'real', 'integer', 'boolean', etc.\"\n    ],\n    variable_bounds: Annotated[\n        List[List[float]],\n        \"List of (lower bound, upper bound) tuples for x0, x1, ...'\",\n    ],\n    samples: Annotated[int, \"Number of random sample. Default 10000\"] = 10000,\n) -&gt; Tuple[str]:\n    \"\"\"\n    A tool for checking feasibility of the constraints.\n\n    Args:\n        constraints: list of strings like 'x0 + x1 &lt;= 5', etc.\n        variable_name: list of strings containing variable names used in constraint expressions.\n        variable_type: list of strings like 'real', 'integer', 'boolean', etc.\n        variable_bounds: list of (lower, upper) tuples for x0, x1, etc.\n        samples: number of random samples, default value 10000\n\n    Returns:\n        A string indicating whether a feasible solution was found.\n    \"\"\"\n\n    symbols = sp.symbols(variable_name)\n\n    # Build a dict mapping each name to its Symbol, for parsing\n    locals_map = {name: sym for name, sym in zip(variable_name, symbols)}\n\n    # Parse constraints into Sympy Boolean expressions\n    parsed_constraints = []\n    try:\n        for expr in constraints:\n            parsed = parse_expr(\n                expr,\n                local_dict=locals_map,\n                transformations=standard_transformations,\n                evaluate=False,\n            )\n            parsed_constraints.append(parsed)\n    except Exception as e:\n        return f\"Error parsing constraints: {e}\"\n\n    # Sampling loop\n    n = len(parsed_constraints)\n    funcs = [\n        sp.lambdify(symbols, c, modules=[\"math\", \"numpy\"])\n        for c in parsed_constraints\n    ]\n    constraint_satisfied = np.zeros(n, dtype=int)\n    for _ in range(samples):\n        point = {}\n        for i, sym in enumerate(symbols):\n            typ = variable_type[i].lower()\n            low, high = variable_bounds[i]\n            if typ == \"integer\":\n                value = random.randint(int(low), int(high))\n            elif typ in (\"real\", \"continuous\"):\n                value = random.uniform(low, high)\n            elif typ in (\"boolean\", \"logical\"):\n                value = random.choice([False, True])\n            else:\n                raise ValueError(\n                    f\"Unknown type {variable_type[i]} for variable {variable_name[i]}\"\n                )\n            point[sym] = value\n\n        # Evaluate all constraints at this point\n        try:\n            vals = [point[s] for s in symbols]\n            cons_satisfaction = [\n                bool(np.asarray(f(*vals)).all()) for f in funcs\n            ]\n            if all(cons_satisfaction):\n                # Found a feasible point\n                readable = {str(k): round(v, 3) for k, v in point.items()}\n                return f\"Feasible solution found: {readable}\"\n            else:\n                constraint_satisfied += np.array(cons_satisfaction)\n        except Exception as e:\n            return f\"Error evaluating constraint at point {point}: {e}\"\n\n    rates = constraint_satisfied / samples  # fraction satisfied per constraint\n    order = np.argsort(rates)  # lowest (most violated) first\n\n    lines = []\n    for rank, idx in enumerate(order, start=1):\n        expr_text = constraints[\n            idx\n        ]  # use the original string; easier to read than str(sympy_expr)\n        sat = constraint_satisfied[idx]\n        lines.append(\n            f\"[C{idx + 1}] {expr_text} \u2014 satisfied {sat:,}/{samples:,} ({sat / samples:.1%}), \"\n            f\"violated {1 - sat / samples:.1%}\"\n        )\n\n    return (\n        f\"No feasible solution found after {samples:,} samples. Most violated constraints (low\u2192high satisfaction):\\n \"\n        + \"\\n  \".join(lines)\n    )\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.feasibility_tools","title":"<code>feasibility_tools</code>","text":"<p>Unified feasibility checker with heuristic pre-check and exact auto-routing.</p> <p>Backends (imported lazily and used only if available): - PySMT (cvc5/msat/yices/z3) for SMT-style logic, disjunctions, and nonlinear constructs. - OR-Tools CP-SAT for strictly linear integer/boolean instances with integer coefficients. - OR-Tools CBC (pywraplp) for linear MILP/LP (mixed real + integer, or pure LP). - SciPy HiGHS (linprog) for pure continuous LP feasibility.</p> Install any subset you need <p>pip install pysmt &amp;&amp; pysmt-install --cvc5        # or --z3/--msat/--yices pip install ortools pip install scipy pip install numpy</p> <p>This file exposes a single LangChain tool: <code>feasibility_check_auto</code>.</p>"},{"location":"api_reference/tools/#ursa.tools.feasibility_tools.feasibility_check_auto","title":"<code>feasibility_check_auto(constraints, variable_name, variable_type, variable_bounds, prefer_smt_solver='cvc5', heuristic_enabled=True, heuristic_first=True, heuristic_samples=2000, heuristic_seed=None, heuristic_unbounded_radius_real=1000.0, heuristic_unbounded_radius_int=10 ** 6, numeric_tolerance=1e-08)</code>","text":"<p>Unified feasibility checker with heuristic pre-check and exact auto-routing.</p> <p>Performs an optional randomized feasibility search. If no witness is found (or the heuristic is disabled), the function auto-routes to an exact backend based on the detected problem structure (PySMT for SMT/logic/nonlinear, OR-Tools CP-SAT for linear integer/boolean, OR-Tools CBC for MILP/LP, or SciPy HiGHS for pure LP).</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Annotated[List[str], \"Constraint strings like 'x0 + 2*x1 &lt;= 5' or '(x0&lt;=3) | (x1&gt;=2)'\"]</code> <p>Constraint strings such as \"x0 + 2*x1 &lt;= 5\" or \"(x0&lt;=3) | (x1&gt;=2)\".</p> required <code>variable_name</code> <code>Annotated[List[str], ['x0', 'x1', ...]]</code> <p>Variable names, e.g., [\"x0\", \"x1\"].</p> required <code>variable_type</code> <code>Annotated[List[str], ['real' | 'integer' | 'boolean', ...]]</code> <p>Variable types aligned with <code>variable_name</code>. Each must be one of \"real\", \"integer\", or \"boolean\".</p> required <code>variable_bounds</code> <code>Annotated[List[List[Optional[float]]], '[(low, high), ...] (use None for unbounded)']</code> <p>Per-variable [low, high] bounds aligned with <code>variable_name</code>. Use None to denote an unbounded side.</p> required <code>prefer_smt_solver</code> <code>Annotated[str, \"SMT backend if needed: 'cvc5'|'msat'|'yices'|'z3'\"]</code> <p>SMT backend name used by PySMT (\"cvc5\", \"msat\", \"yices\", or \"z3\").</p> <code>'cvc5'</code> <code>heuristic_enabled</code> <code>Annotated[bool, 'Run a fast randomized search first?']</code> <p>Whether to run the heuristic sampler.</p> <code>True</code> <code>heuristic_first</code> <code>Annotated[bool, 'Try heuristic before exact routing']</code> <p>If True, run the heuristic before exact routing; if False, run it after.</p> <code>True</code> <code>heuristic_samples</code> <code>Annotated[int, 'Samples for heuristic search']</code> <p>Number of heuristic samples.</p> <code>2000</code> <code>heuristic_seed</code> <code>Annotated[Optional[int], 'Seed for reproducibility']</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>heuristic_unbounded_radius_real</code> <code>Annotated[float, 'Sampling range for unbounded real vars']</code> <p>Sampling radius for unbounded real variables.</p> <code>1000.0</code> <code>heuristic_unbounded_radius_int</code> <code>Annotated[int, 'Sampling range for unbounded integer vars']</code> <p>Sampling radius for unbounded integer variables.</p> <code>10 ** 6</code> <code>numeric_tolerance</code> <code>Annotated[float, 'Tolerance for relational checks (Eq/Lt/Le/etc.)']</code> <p>Tolerance used in relational checks (e.g., Eq, Lt, Le).</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>str</code> <p>A message indicating the chosen backend and the feasibility result. On success,</p> <code>str</code> <p>includes an example model (assignment). On infeasibility, includes a short</p> <code>str</code> <p>diagnostic or solver status.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If constraints cannot be parsed or an unsupported variable type is provided.</p> Source code in <code>src/ursa/tools/feasibility_tools.py</code> <pre><code>@tool(parse_docstring=True)\ndef feasibility_check_auto(\n    constraints: Annotated[\n        List[str],\n        \"Constraint strings like 'x0 + 2*x1 &lt;= 5' or '(x0&lt;=3) | (x1&gt;=2)'\",\n    ],\n    variable_name: Annotated[List[str], \"['x0','x1',...]\"],\n    variable_type: Annotated[List[str], \"['real'|'integer'|'boolean', ...]\"],\n    variable_bounds: Annotated[\n        List[List[Optional[float]]],\n        \"[(low, high), ...] (use None for unbounded)\",\n    ],\n    prefer_smt_solver: Annotated[\n        str, \"SMT backend if needed: 'cvc5'|'msat'|'yices'|'z3'\"\n    ] = \"cvc5\",\n    heuristic_enabled: Annotated[\n        bool, \"Run a fast randomized search first?\"\n    ] = True,\n    heuristic_first: Annotated[\n        bool, \"Try heuristic before exact routing\"\n    ] = True,\n    heuristic_samples: Annotated[int, \"Samples for heuristic search\"] = 2000,\n    heuristic_seed: Annotated[Optional[int], \"Seed for reproducibility\"] = None,\n    heuristic_unbounded_radius_real: Annotated[\n        float, \"Sampling range for unbounded real vars\"\n    ] = 1e3,\n    heuristic_unbounded_radius_int: Annotated[\n        int, \"Sampling range for unbounded integer vars\"\n    ] = 10**6,\n    numeric_tolerance: Annotated[\n        float, \"Tolerance for relational checks (Eq/Lt/Le/etc.)\"\n    ] = 1e-8,\n) -&gt; str:\n    \"\"\"Unified feasibility checker with heuristic pre-check and exact auto-routing.\n\n    Performs an optional randomized feasibility search. If no witness is found (or the\n    heuristic is disabled), the function auto-routes to an exact backend based on the\n    detected problem structure (PySMT for SMT/logic/nonlinear, OR-Tools CP-SAT for\n    linear integer/boolean, OR-Tools CBC for MILP/LP, or SciPy HiGHS for pure LP).\n\n    Args:\n        constraints: Constraint strings such as \"x0 + 2*x1 &lt;= 5\" or \"(x0&lt;=3) | (x1&gt;=2)\".\n        variable_name: Variable names, e.g., [\"x0\", \"x1\"].\n        variable_type: Variable types aligned with `variable_name`. Each must be one of\n            \"real\", \"integer\", or \"boolean\".\n        variable_bounds: Per-variable [low, high] bounds aligned with `variable_name`.\n            Use None to denote an unbounded side.\n        prefer_smt_solver: SMT backend name used by PySMT (\"cvc5\", \"msat\", \"yices\", or \"z3\").\n        heuristic_enabled: Whether to run the heuristic sampler.\n        heuristic_first: If True, run the heuristic before exact routing; if False, run it after.\n        heuristic_samples: Number of heuristic samples.\n        heuristic_seed: Random seed for reproducibility.\n        heuristic_unbounded_radius_real: Sampling radius for unbounded real variables.\n        heuristic_unbounded_radius_int: Sampling radius for unbounded integer variables.\n        numeric_tolerance: Tolerance used in relational checks (e.g., Eq, Lt, Le).\n\n    Returns:\n        A message indicating the chosen backend and the feasibility result. On success,\n        includes an example model (assignment). On infeasibility, includes a short\n        diagnostic or solver status.\n\n    Raises:\n        ValueError: If constraints cannot be parsed or an unsupported variable type is provided.\n    \"\"\"\n    # 1) Parse\n    try:\n        symbols, sympy_cons = _parse_constraints(constraints, variable_name)\n    except Exception as e:\n        return f\"Parse error: {e}\"\n\n    # 2) Heuristic (optional)\n    if heuristic_enabled and heuristic_first:\n        try:\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        except Exception:\n            # Ignore heuristic issues and continue to exact route\n            pass\n\n    # 3) Classify &amp; route\n    info = _classify(sympy_cons, symbols, variable_type)\n\n    # SMT needed or nonlinear / non-conj\n    if info[\"requires_smt\"] or not info[\"all_linear\"]:\n        res = _solve_with_pysmt(\n            sympy_cons,\n            symbols,\n            variable_name,\n            variable_type,\n            variable_bounds,\n            solver_name=prefer_smt_solver,\n        )\n        # Optional heuristic after exact if requested\n        if (\n            heuristic_enabled\n            and not heuristic_first\n            and any(\n                kw in res.lower()\n                for kw in (\"unknown\", \"not installed\", \"unsupported\", \"failed\")\n            )\n        ):\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        return res\n\n    # Linear-only path: collect atomic conjuncts\n    conjuncts: List[sp.Expr] = []\n    for c in sympy_cons:\n        atoms, _ = _flatten_conjunction(c)\n        conjuncts.extend(atoms)\n\n    has_int, has_bool, has_real = (\n        info[\"has_int\"],\n        info[\"has_bool\"],\n        info[\"has_real\"],\n    )\n\n    # Pure LP (continuous only)\n    if not has_int and not has_bool and has_real:\n        res = _solve_with_highs_lp(\n            conjuncts, symbols, variable_name, variable_bounds\n        )\n        if \"not installed\" in res.lower():\n            res = _solve_with_cbc_milp(\n                conjuncts,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n            )\n        if (\n            heuristic_enabled\n            and not heuristic_first\n            and any(kw in res.lower() for kw in (\"failed\", \"unknown\"))\n        ):\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        return res\n\n    # All integer/boolean \u2192 CP-SAT first (if integer coefficients), else CBC MILP\n    if (has_int or has_bool) and not has_real:\n        res = _solve_with_cpsat_integer_boolean(\n            conjuncts, symbols, variable_name, variable_type, variable_bounds\n        )\n        if (\n            any(\n                kw in res\n                for kw in (\n                    \"routing to MILP/LP\",\n                    \"handles linear conjunctions only\",\n                )\n            )\n            or \"not installed\" in res.lower()\n        ):\n            res = _solve_with_cbc_milp(\n                conjuncts,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n            )\n        return res\n\n    # Mixed reals + integers \u2192 CBC MILP\n    res = _solve_with_cbc_milp(\n        conjuncts, symbols, variable_name, variable_type, variable_bounds\n    )\n\n    # Optional heuristic after exact (if backend missing/failing)\n    if (\n        heuristic_enabled\n        and not heuristic_first\n        and any(\n            kw in res.lower() for kw in (\"not installed\", \"failed\", \"status:\")\n        )\n    ):\n        h_model = _heuristic_feasible(\n            sympy_cons,\n            symbols,\n            variable_name,\n            variable_type,\n            variable_bounds,\n            samples=heuristic_samples,\n            seed=heuristic_seed,\n            tol=numeric_tolerance,\n            unbounded_radius_real=heuristic_unbounded_radius_real,\n            unbounded_radius_int=heuristic_unbounded_radius_int,\n        )\n        if h_model is not None:\n            return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n\n    return res\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.run_command","title":"<code>run_command</code>","text":""},{"location":"api_reference/tools/#ursa.tools.run_command.run_cmd","title":"<code>run_cmd(query, workspace_dir)</code>","text":"<p>Run command from commandline in the directory workspace_dir</p> Source code in <code>src/ursa/tools/run_command.py</code> <pre><code>@tool\ndef run_cmd(query: str, workspace_dir: str) -&gt; str:\n    \"\"\"Run command from commandline in the directory workspace_dir\"\"\"\n\n    print(\"RUNNING: \", query)\n    print(\n        \"DANGER DANGER DANGER - THERE IS NO GUARDRAIL FOR SAFETY IN THIS IMPLEMENTATION - DANGER DANGER DANGER\"\n    )\n    process = subprocess.Popen(\n        query.split(\" \"),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        cwd=workspace_dir,\n    )\n\n    stdout, stderr = process.communicate(timeout=600)\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.write_code","title":"<code>write_code</code>","text":""},{"location":"api_reference/tools/#ursa.tools.write_code.write_python","title":"<code>write_python(code, filename, workspace_dir)</code>","text":"<p>Writes code to a file in the given workspace.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to write</p> required <code>filename</code> <code>str</code> <p>the filename to write</p> required <p>Returns:</p> Type Description <code>str</code> <p>File writing status: string</p> Source code in <code>src/ursa/tools/write_code.py</code> <pre><code>@tool\ndef write_python(code: str, filename: str, workspace_dir: str) -&gt; str:\n    \"\"\"\n    Writes code to a file in the given workspace.\n\n    Args:\n        code: The code to write\n        filename: the filename to write\n\n    Returns:\n        File writing status: string\n    \"\"\"\n    print(\"Writing filename \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/util/","title":"util","text":""},{"location":"api_reference/util/#ursa.util.diff_renderer","title":"<code>diff_renderer</code>","text":""},{"location":"api_reference/util/#ursa.util.diff_renderer.DiffRenderer","title":"<code>DiffRenderer</code>","text":"<p>Renderable diff\u2014<code>console.print(DiffRenderer(...))</code></p> Source code in <code>src/ursa/util/diff_renderer.py</code> <pre><code>class DiffRenderer:\n    \"\"\"Renderable diff\u2014`console.print(DiffRenderer(...))`\"\"\"\n\n    def __init__(self, content: str, updated: str, filename: str):\n        # total lines in each version\n        self._old_total = len(content.splitlines())\n        self._new_total = len(updated.splitlines())\n\n        # number of digits in the largest count\n        self._num_width = len(str(max(self._old_total, self._new_total))) + 2\n\n        # get the diff\n        self._diff_lines = list(\n            difflib.unified_diff(\n                content.splitlines(),\n                updated.splitlines(),\n                fromfile=f\"{filename} (original)\",\n                tofile=f\"{filename} (modified)\",\n                lineterm=\"\",\n            )\n        )\n\n        # get syntax style\n        try:\n            self._lexer_name = Syntax.guess_lexer(filename, updated)\n        except Exception:\n            self._lexer_name = \"text\"\n\n    def __rich_console__(\n        self, console: Console, opts: ConsoleOptions\n    ) -&gt; RenderResult:\n        old_line = new_line = None\n        width = console.width\n\n        for raw in self._diff_lines:\n            # grab line numbers from hunk header\n            if m := _HUNK_RE.match(raw):\n                old_line, new_line = map(int, m.groups())\n                # build a marker\n                n = self._num_width\n                tick_col = \".\" * (n - 1)\n                indent_ticks = f\" {tick_col} {tick_col}\"\n                # pad to the indent width\n                full_indent = indent_ticks.ljust(2 * n + 3)\n                yield Text(\n                    f\"{full_indent}{raw}\".ljust(width), style=\"white on grey30\"\n                )\n                continue\n\n            # skip header lines\n            if raw.startswith((\"---\", \"+++\")):\n                continue\n\n            # split the line\n            if raw.startswith(\"+\"):\n                style = _STYLE[\"add\"]\n                code = raw[1:]\n            elif raw.startswith(\"-\"):\n                style = _STYLE[\"del\"]\n                code = raw[1:]\n            else:\n                style = _STYLE[\"ctx\"]\n                code = raw[1:] if raw.startswith(\" \") else raw\n\n            # compute line numbers\n            if raw.startswith(\"+\"):\n                old_num, new_num = None, new_line\n                new_line += 1\n            elif raw.startswith(\"-\"):\n                old_num, new_num = old_line, None\n                old_line += 1\n            else:\n                old_num, new_num = old_line, new_line\n                old_line += 1\n                new_line += 1\n\n            old_str = str(old_num) if old_num is not None else \" \"\n            new_str = str(new_num) if new_num is not None else \" \"\n\n            # Syntax-highlight the code part\n            syntax = Syntax(\n                code, self._lexer_name, line_numbers=False, word_wrap=False\n            )\n            text_code: Text = syntax.highlight(code)\n            if text_code.plain.endswith(\"\\n\"):\n                text_code = text_code[:-1]\n            # apply background\n            text_code.stylize(style.bg)\n\n            # line numbers + code\n            nums = Text(\n                f\"{old_str:&gt;{self._num_width}}{new_str:&gt;{self._num_width}} \",\n                style=f\"white {style.bg}\",\n            )\n            diff_mark = Text(style.prefix, style=f\"bright_white {style.bg}\")\n            line_text = nums + diff_mark + text_code\n\n            # pad to console width\n            pad_len = width - line_text.cell_len\n            if pad_len &gt; 0:\n                line_text.append(\" \" * pad_len, style=style.bg)\n\n            yield line_text\n</code></pre>"},{"location":"api_reference/util/#ursa.util.helperFunctions","title":"<code>helperFunctions</code>","text":""},{"location":"api_reference/util/#ursa.util.helperFunctions.run_tool_calls","title":"<code>run_tool_calls(ai_msg, tools)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ai_msg</code> <code>AIMessage</code> <p>The LLM's AIMessage containing tool calls.</p> required <code>tools</code> <code>Union[ToolRegistry, Iterable[Union[Runnable, Callable[..., Any]]]]</code> <p>Either a dict {name: tool} or an iterable of tools (must have <code>.name</code>    for mapping). Each tool can be a Runnable or a plain callable.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>List[BaseMessage]</code> <p>list[BaseMessage] to feed back to the model</p> Source code in <code>src/ursa/util/helperFunctions.py</code> <pre><code>def run_tool_calls(\n    ai_msg: AIMessage,\n    tools: Union[ToolRegistry, Iterable[Union[Runnable, Callable[..., Any]]]],\n) -&gt; List[BaseMessage]:\n    \"\"\"\n    Args:\n        ai_msg: The LLM's AIMessage containing tool calls.\n        tools: Either a dict {name: tool} or an iterable of tools (must have `.name`\n               for mapping). Each tool can be a Runnable or a plain callable.\n\n    Returns:\n        out: list[BaseMessage] to feed back to the model\n    \"\"\"\n    # Build a name-&gt;tool map\n    if isinstance(tools, dict):\n        registry: ToolRegistry = tools  # type: ignore\n    else:\n        registry = {}\n        for t in tools:\n            name = getattr(t, \"name\", None) or getattr(t, \"__name__\", None)\n            if not name:\n                raise ValueError(f\"Tool {t!r} has no discoverable name.\")\n            registry[name] = t  # type: ignore\n\n    calls = extract_tool_calls(ai_msg)\n\n    if not calls:\n        return []\n\n    out: List[BaseMessage] = []\n    for call in calls:\n        name = call.get(\"name\")\n        args = call.get(\"args\", {}) or {}\n        call_id = call.get(\"id\") or f\"call_{uuid.uuid4().hex}\"\n\n        # 1) the AIMessage that generated the call\n        out.append(ai_msg)\n\n        # 2) the ToolMessage with the execution result (or error)\n        if name not in registry:\n            content = f\"ERROR: unknown tool '{name}'.\"\n        else:\n            try:\n                result = _invoke_tool(registry[name], args)\n                content = _stringify_output(result)\n            except Exception as e:\n                content = f\"ERROR: {type(e).__name__}: {e}\"\n\n        out.append(\n            ToolMessage(content=content, tool_call_id=call_id, name=name)\n        )\n\n    return out\n</code></pre>"},{"location":"api_reference/util/#ursa.util.logo_generator","title":"<code>logo_generator</code>","text":""},{"location":"api_reference/util/#ursa.util.logo_generator.generate_logo_sync","title":"<code>generate_logo_sync(*, problem_text, workspace, out_dir, filename=None, model='gpt-image-1', size=None, background='opaque', quality='high', n=1, overwrite=False, style='sticker', allow_text=False, palette=None, mode='logo', aspect='square', style_intensity='overt', console=None)</code>","text":"<p>Generate an image. Default behavior matches previous versions (logo/sticker). To create a cinematic illustration, set mode='scene' and consider aspect='wide'.</p> Source code in <code>src/ursa/util/logo_generator.py</code> <pre><code>def generate_logo_sync(\n    *,\n    problem_text: str,\n    workspace: str,\n    out_dir: str | Path,\n    filename: str | None = None,\n    model: str = \"gpt-image-1\",\n    size: str | None = None,  # NEW: allow None \u2192 computed from aspect/mode\n    background: str = \"opaque\",\n    quality: str = \"high\",\n    n: int = 1,\n    overwrite: bool = False,\n    style: str = \"sticker\",\n    allow_text: bool = False,\n    palette: str | None = None,\n    mode: str = \"logo\",  # NEW\n    aspect: str = \"square\",  # NEW: \"wide\" | \"tall\" | \"square\"\n    style_intensity: str = \"overt\",  # NEW\n    console: Optional[Console] = None,\n) -&gt; Path:\n    \"\"\"\n    Generate an image. Default behavior matches previous versions (logo/sticker).\n    To create a cinematic illustration, set mode='scene' and consider aspect='wide'.\n    \"\"\"\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    prompt, style_slug = _craft_logo_prompt(\n        problem_text,\n        workspace,\n        style=style,\n        allow_text=allow_text,\n        palette=palette,\n        mode=mode,\n        style_intensity=style_intensity,\n    )\n\n    # Pretty console output\n    extra_title = f\"[bold magenta]mode: {mode}[/bold magenta] [dim]\u2022[/dim] aspect: {aspect}\"\n    _render_prompt_panel(\n        console=console,\n        style_slug=style_slug,\n        workspace=workspace,\n        prompt=prompt,\n        extra_title=extra_title,\n    )\n\n    main_path, alt_paths = _compose_filenames(out_dir, style_slug, filename, n)\n    if main_path.exists() and not overwrite:\n        return main_path\n\n    client = OpenAI()\n\n    final_size = _normalize_size(size, aspect, mode)\n    # Scenes tend to look odd with transparent backgrounds; force opaque.\n    final_background = \"opaque\" if mode == \"scene\" else background\n\n    kwargs = dict(\n        model=model,\n        prompt=prompt,\n        size=final_size,\n        n=n,\n        quality=quality,\n        background=final_background,\n    )\n    try:\n        resp = client.images.generate(**kwargs)\n    except Exception:\n        # Some models ignore/forbid background=; retry without it\n        kwargs.pop(\"background\", None)\n        resp = client.images.generate(**kwargs)\n\n    main_path.write_bytes(base64.b64decode(resp.data[0].b64_json))\n    for i, item in enumerate(resp.data[1:], start=0):\n        if i &lt; len(alt_paths):\n            alt_paths[i].write_bytes(base64.b64decode(item.b64_json))\n    return main_path\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger","title":"<code>memory_logger</code>","text":""},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory","title":"<code>AgentMemory</code>","text":"<p>Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory--parameters","title":"Parameters","text":"<p>path : str | Path | None     Where to keep the on-disk Chroma DB.  If None, a folder called     <code>agent_memory_db</code> is created in the package\u2019s base directory. collection_name : str     Name of the Chroma collection. embedding_model :  | None     the embedding model"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory--notes","title":"Notes","text":"<ul> <li>Requires <code>langchain-chroma</code>, and <code>chromadb</code>.</li> </ul> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>class AgentMemory:\n    \"\"\"\n    Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.\n\n    Parameters\n    ----------\n    path : str | Path | None\n        Where to keep the on-disk Chroma DB.  If *None*, a folder called\n        ``agent_memory_db`` is created in the package\u2019s base directory.\n    collection_name : str\n        Name of the Chroma collection.\n    embedding_model : &lt;TODO&gt; | None\n        the embedding model\n\n    Notes\n    -----\n    * Requires `langchain-chroma`, and `chromadb`.\n    \"\"\"\n\n    @classmethod\n    def get_db_path(cls, path: Optional[str | Path]) -&gt; Path:\n        match path:\n            case None:\n                return Path.home() / \".cache\" / \"ursa\" / \"rag\" / \"db\"\n            case str():\n                return Path(path)\n            case Path():\n                return path\n            case _:\n                raise TypeError(\n                    f\"Type of path is `{type(path)}` \"\n                    \"but `Optional[str | Path]` was expected.\"\n                )\n\n    def __init__(\n        self,\n        embedding_model,\n        path: Optional[str | Path] = None,\n        collection_name: str = \"agent_memory\",\n    ) -&gt; None:\n        self.path = self.get_db_path(path)\n        self.collection_name = collection_name\n        self.path.mkdir(parents=True, exist_ok=True)\n        self.embeddings = embedding_model\n\n        # If a DB already exists, load it; otherwise defer creation until `build_index`.\n        self.vectorstore: Optional[Chroma] = None\n        if any(self.path.iterdir()):\n            self.vectorstore = Chroma(\n                collection_name=self.collection_name,\n                embedding_function=self.embeddings,\n                persist_directory=str(self.path),\n            )\n\n    # --------------------------------------------------------------------- #\n    # \u2776 Build &amp; index a brand-new database                                   #\n    # --------------------------------------------------------------------- #\n    def build_index(\n        self,\n        chunks: Sequence[str],\n        metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Create a fresh vector store from ``chunks``.  Existing data (if any)\n        are overwritten.\n\n        Parameters\n        ----------\n        chunks : Sequence[str]\n            Text snippets (already chunked) to embed.\n        metadatas : Sequence[dict] | None\n            Optional metadata dict for each chunk, same length as ``chunks``.\n        \"\"\"\n        docs = [\n            Document(\n                page_content=text, metadata=metadatas[i] if metadatas else {}\n            )\n            for i, text in enumerate(chunks)\n        ]\n\n        # Create (or overwrite) the collection\n        self.vectorstore = Chroma.from_documents(\n            documents=docs,\n            embedding=self.embeddings,\n            collection_name=self.collection_name,\n            persist_directory=str(self.path),\n        )\n\n    # --------------------------------------------------------------------- #\n    # \u2777 Add new chunks and re-index                                          #\n    # --------------------------------------------------------------------- #\n    def add_memories(\n        self,\n        new_chunks: Sequence[str],\n        metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Append new text chunks to the existing store (must call `build_index`\n        first if the DB is empty).\n\n        Raises\n        ------\n        RuntimeError\n            If the vector store is not yet initialised.\n        \"\"\"\n        if self.vectorstore is None:\n            self.build_index(new_chunks, metadatas)\n            print(\"----- Vector store initialised -----\")\n\n        docs = []\n        for i, text in enumerate(new_chunks):\n            if len(text) &gt; 0:  # only add non-empty documents\n                docs.append(\n                    Document(\n                        page_content=text,\n                        metadata=metadatas[i] if metadatas else {},\n                    )\n                )\n        self.vectorstore.add_documents(docs)\n\n    # --------------------------------------------------------------------- #\n    # \u2778 Retrieve relevant chunks (RAG query)                                 #\n    # --------------------------------------------------------------------- #\n    def retrieve(\n        self,\n        query: str,\n        k: int = 4,\n        with_scores: bool = False,\n        **search_kwargs,\n    ):\n        \"\"\"\n        Return the *k* most similar chunks for `query`.\n\n        Parameters\n        ----------\n        query : str\n            Natural-language question or statement.\n        k : int\n            How many results to return.\n        with_scores : bool\n            If True, also return similarity scores.\n        **search_kwargs\n            Extra kwargs forwarded to Chroma\u2019s ``similarity_search*`` helpers.\n\n        Returns\n        -------\n        list[Document] | list[tuple[Document, float]]\n        \"\"\"\n        if self.vectorstore is None:\n            return [\"None\"]\n\n        if with_scores:\n            return self.vectorstore.similarity_search_with_score(\n                query, k=k, **search_kwargs\n            )\n        return self.vectorstore.similarity_search(query, k=k, **search_kwargs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.add_memories","title":"<code>add_memories(new_chunks, metadatas=None)</code>","text":"<p>Append new text chunks to the existing store (must call <code>build_index</code> first if the DB is empty).</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.add_memories--raises","title":"Raises","text":"<p>RuntimeError     If the vector store is not yet initialised.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def add_memories(\n    self,\n    new_chunks: Sequence[str],\n    metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Append new text chunks to the existing store (must call `build_index`\n    first if the DB is empty).\n\n    Raises\n    ------\n    RuntimeError\n        If the vector store is not yet initialised.\n    \"\"\"\n    if self.vectorstore is None:\n        self.build_index(new_chunks, metadatas)\n        print(\"----- Vector store initialised -----\")\n\n    docs = []\n    for i, text in enumerate(new_chunks):\n        if len(text) &gt; 0:  # only add non-empty documents\n            docs.append(\n                Document(\n                    page_content=text,\n                    metadata=metadatas[i] if metadatas else {},\n                )\n            )\n    self.vectorstore.add_documents(docs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.build_index","title":"<code>build_index(chunks, metadatas=None)</code>","text":"<p>Create a fresh vector store from <code>chunks</code>.  Existing data (if any) are overwritten.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.build_index--parameters","title":"Parameters","text":"<p>chunks : Sequence[str]     Text snippets (already chunked) to embed. metadatas : Sequence[dict] | None     Optional metadata dict for each chunk, same length as <code>chunks</code>.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def build_index(\n    self,\n    chunks: Sequence[str],\n    metadatas: Optional[Sequence[Dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Create a fresh vector store from ``chunks``.  Existing data (if any)\n    are overwritten.\n\n    Parameters\n    ----------\n    chunks : Sequence[str]\n        Text snippets (already chunked) to embed.\n    metadatas : Sequence[dict] | None\n        Optional metadata dict for each chunk, same length as ``chunks``.\n    \"\"\"\n    docs = [\n        Document(\n            page_content=text, metadata=metadatas[i] if metadatas else {}\n        )\n        for i, text in enumerate(chunks)\n    ]\n\n    # Create (or overwrite) the collection\n    self.vectorstore = Chroma.from_documents(\n        documents=docs,\n        embedding=self.embeddings,\n        collection_name=self.collection_name,\n        persist_directory=str(self.path),\n    )\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve","title":"<code>retrieve(query, k=4, with_scores=False, **search_kwargs)</code>","text":"<p>Return the k most similar chunks for <code>query</code>.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve--parameters","title":"Parameters","text":"<p>query : str     Natural-language question or statement. k : int     How many results to return. with_scores : bool     If True, also return similarity scores. **search_kwargs     Extra kwargs forwarded to Chroma\u2019s <code>similarity_search*</code> helpers.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve--returns","title":"Returns","text":"<p>list[Document] | list[tuple[Document, float]]</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def retrieve(\n    self,\n    query: str,\n    k: int = 4,\n    with_scores: bool = False,\n    **search_kwargs,\n):\n    \"\"\"\n    Return the *k* most similar chunks for `query`.\n\n    Parameters\n    ----------\n    query : str\n        Natural-language question or statement.\n    k : int\n        How many results to return.\n    with_scores : bool\n        If True, also return similarity scores.\n    **search_kwargs\n        Extra kwargs forwarded to Chroma\u2019s ``similarity_search*`` helpers.\n\n    Returns\n    -------\n    list[Document] | list[tuple[Document, float]]\n    \"\"\"\n    if self.vectorstore is None:\n        return [\"None\"]\n\n    if with_scores:\n        return self.vectorstore.similarity_search_with_score(\n            query, k=k, **search_kwargs\n        )\n    return self.vectorstore.similarity_search(query, k=k, **search_kwargs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.delete_database","title":"<code>delete_database(path=None)</code>","text":"<p>Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.delete_database--parameters","title":"Parameters","text":"<p>path : str | Path | None     Where the on-disk Chroma DB is for deleting.  If None, a folder called     <code>agent_memory_db</code> is created in the package\u2019s base directory.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def delete_database(path: Optional[str | Path] = None):\n    \"\"\"\n    Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.\n\n    Parameters\n    ----------\n    path : str | Path | None\n        Where the on-disk Chroma DB is for deleting.  If *None*, a folder called\n        ``agent_memory_db`` is created in the package\u2019s base directory.\n    \"\"\"\n    db_path = AgentMemory.get_db_path(path)\n    if os.path.exists(db_path):\n        shutil.rmtree(db_path)\n        print(f\"Database: {db_path} has been deleted.\")\n    else:\n        print(\"No database found to delete.\")\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse","title":"<code>parse</code>","text":""},{"location":"api_reference/util/#ursa.util.parse.extract_json","title":"<code>extract_json(text)</code>","text":"<p>Extract a JSON object or array from text that might contain markdown or other content.</p> The function attempts three strategies <ol> <li>Extract JSON from a markdown code block labeled as JSON.</li> <li>Extract JSON from any markdown code block.</li> <li>Use bracket matching to extract a JSON substring starting with '{' or '['.</li> </ol> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A Python object parsed from the JSON string (dict or list).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid JSON is found.</p> Source code in <code>src/ursa/util/parse.py</code> <pre><code>def extract_json(text: str) -&gt; list[dict]:\n    \"\"\"\n    Extract a JSON object or array from text that might contain markdown or other content.\n\n    The function attempts three strategies:\n        1. Extract JSON from a markdown code block labeled as JSON.\n        2. Extract JSON from any markdown code block.\n        3. Use bracket matching to extract a JSON substring starting with '{' or '['.\n\n    Returns:\n        A Python object parsed from the JSON string (dict or list).\n\n    Raises:\n        ValueError: If no valid JSON is found.\n    \"\"\"\n    # Approach 1: Look for a markdown code block specifically labeled as JSON.\n    labeled_block = re.search(\n        r\"```json\\s*([\\[{].*?[\\]}])\\s*```\", text, re.DOTALL\n    )\n    if labeled_block:\n        json_str = labeled_block.group(1).strip()\n        try:\n            return json.loads(json_str)\n        except json.JSONDecodeError:\n            # Fall back to the next approach if parsing fails.\n            pass\n\n    # Approach 2: Look for any code block delimited by triple backticks.\n    generic_block = re.search(r\"```(.*?)```\", text, re.DOTALL)\n    if generic_block:\n        json_str = generic_block.group(1).strip()\n        if json_str.startswith(\"{\") or json_str.startswith(\"[\"):\n            try:\n                return json.loads(json_str)\n            except json.JSONDecodeError:\n                pass\n\n    # Approach 3: Attempt to extract JSON using bracket matching.\n    # Find the first occurrence of either '{' or '['.\n    first_obj = text.find(\"{\")\n    first_arr = text.find(\"[\")\n    if first_obj == -1 and first_arr == -1:\n        raise ValueError(\"No JSON object or array found in the text.\")\n\n    # Determine which bracket comes first.\n    if first_obj == -1:\n        start = first_arr\n        open_bracket = \"[\"\n        close_bracket = \"]\"\n    elif first_arr == -1:\n        start = first_obj\n        open_bracket = \"{\"\n        close_bracket = \"}\"\n    else:\n        if first_obj &lt; first_arr:\n            start = first_obj\n            open_bracket = \"{\"\n            close_bracket = \"}\"\n        else:\n            start = first_arr\n            open_bracket = \"[\"\n            close_bracket = \"]\"\n\n    # Bracket matching: find the matching closing bracket.\n    depth = 0\n    end = None\n    for i in range(start, len(text)):\n        if text[i] == open_bracket:\n            depth += 1\n        elif text[i] == close_bracket:\n            depth -= 1\n            if depth == 0:\n                end = i\n                break\n\n    if end is None:\n        raise ValueError(\n            \"Could not find matching closing bracket for JSON content.\"\n        )\n\n    json_str = text[start : end + 1]\n    try:\n        return json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Extracted content is not valid JSON.\") from e\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse.extract_main_text_only","title":"<code>extract_main_text_only(html, *, max_chars=250000)</code>","text":"<p>Returns plain text with navigation/ads/scripts removed. Prefers trafilatura -&gt; jusText -&gt; BS4 paragraphs.</p> Source code in <code>src/ursa/util/parse.py</code> <pre><code>def extract_main_text_only(html: str, *, max_chars: int = 250_000) -&gt; str:\n    \"\"\"\n    Returns plain text with navigation/ads/scripts removed.\n    Prefers trafilatura -&gt; jusText -&gt; BS4 paragraphs.\n    \"\"\"\n    # 1) Trafilatura\n    # You can tune config: with_metadata, include_comments, include_images, favor_recall, etc.\n    cfg = trafilatura.settings.use_config()\n    cfg.set(\"DEFAULT\", \"include_comments\", \"false\")\n    cfg.set(\"DEFAULT\", \"include_tables\", \"false\")\n    cfg.set(\"DEFAULT\", \"favor_recall\", \"false\")  # be stricter; less noise\n    try:\n        # If you fetched HTML already, use extract() on string; otherwise, fetch_url(url)\n        txt = trafilatura.extract(\n            html,\n            config=cfg,\n            include_comments=False,\n            include_tables=False,\n            favor_recall=False,\n        )\n        if txt and txt.strip():\n            txt = _normalize_ws(txt)\n            txt = _dedupe_lines(txt)\n            return txt[:max_chars]\n    except Exception:\n        pass\n\n    # 2) jusText\n    try:\n        paragraphs = justext.justext(html, justext.get_stoplist(\"English\"))\n        body_paras = [p.text for p in paragraphs if not p.is_boilerplate]\n        if body_paras:\n            txt = _normalize_ws(\"\\n\\n\".join(body_paras))\n            txt = _dedupe_lines(txt)\n            return txt[:max_chars]\n    except Exception:\n        pass\n\n    # 4) last-resort: BS4 paragraphs/headings only\n    from bs4 import BeautifulSoup\n\n    soup = BeautifulSoup(html, \"html.parser\")\n    for tag in soup([\n        \"script\",\n        \"style\",\n        \"noscript\",\n        \"header\",\n        \"footer\",\n        \"nav\",\n        \"form\",\n        \"aside\",\n    ]):\n        tag.decompose()\n    chunks = []\n    for el in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\", \"figcaption\"]):\n        t = el.get_text(\" \", strip=True)\n        if t:\n            chunks.append(t)\n    txt = _normalize_ws(\"\\n\\n\".join(chunks))\n    txt = _dedupe_lines(txt)\n    return txt[:max_chars]\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse.resolve_pdf_from_osti_record","title":"<code>resolve_pdf_from_osti_record(rec, *, headers=None, unpaywall_email=None, timeout=25)</code>","text":"<p>Returns (pdf_url, landing_used, note)   - pdf_url: direct downloadable PDF URL if found (or a strong candidate)   - landing_used: landing page URL we parsed (if any)   - note: brief trace of how we found it</p> Source code in <code>src/ursa/util/parse.py</code> <pre><code>def resolve_pdf_from_osti_record(\n    rec: dict[str, Any],\n    *,\n    headers: Optional[dict[str, str]] = None,\n    unpaywall_email: Optional[str] = None,\n    timeout: int = 25,\n) -&gt; Tuple[Optional[str], Optional[str], str]:\n    \"\"\"\n    Returns (pdf_url, landing_used, note)\n      - pdf_url: direct downloadable PDF URL if found (or a strong candidate)\n      - landing_used: landing page URL we parsed (if any)\n      - note: brief trace of how we found it\n    \"\"\"\n    headers = headers or {\"User-Agent\": \"Mozilla/5.0\"}\n    note_parts: list[str] = []\n\n    links = rec.get(\"links\", []) or []\n    # doi = rec.get(\"doi\")\n\n    # 1) Try 'fulltext' first (OSTI purl)\n    fulltext = None\n    for link in links:\n        if link.get(\"rel\") == \"fulltext\":\n            fulltext = link.get(\"href\")\n            break\n\n    if fulltext:\n        note_parts.append(\"Tried links[fulltext] purl\")\n        try:\n            # Follow redirects; stream to peek headers without loading whole body\n            r = requests.get(\n                fulltext,\n                headers=headers,\n                timeout=timeout,\n                allow_redirects=True,\n                stream=True,\n            )\n            r.raise_for_status()\n\n            if _is_pdf_response(r):\n                note_parts.append(\"fulltext resolved directly to PDF\")\n                return (r.url, None, \" | \".join(note_parts))\n\n            # Not a PDF: parse page HTML for meta or obvious PDF anchors\n            # (If server sent binary but CT lied, _is_pdf_response would have caught via CD or ext)\n            r.close()\n            soup = _get_soup(fulltext, timeout=timeout, headers=headers)\n            candidate = _find_pdf_on_landing(soup, fulltext)\n            if candidate:\n                note_parts.append(\n                    \"found PDF via meta/anchor on fulltext landing\"\n                )\n                return (candidate, fulltext, \" | \".join(note_parts))\n        except Exception as e:\n            note_parts.append(f\"fulltext failed: {e}\")\n\n    # 2) Try DOE PAGES landing (citation_doe_pages)\n    doe_pages = None\n    for link in links:\n        if link.get(\"rel\") == \"citation_doe_pages\":\n            doe_pages = link.get(\"href\")\n            break\n\n    if doe_pages:\n        note_parts.append(\"Tried links[citation_doe_pages] landing\")\n        try:\n            soup = _get_soup(doe_pages, timeout=timeout, headers=headers)\n            candidate = _find_pdf_on_landing(soup, doe_pages)\n            if candidate:\n                # Candidate may itself be a landing\u2014check if it serves PDF\n                try:\n                    r2 = requests.get(\n                        candidate,\n                        headers=headers,\n                        timeout=timeout,\n                        allow_redirects=True,\n                        stream=True,\n                    )\n                    r2.raise_for_status()\n                    if _is_pdf_response(r2):\n                        note_parts.append(\"citation_doe_pages \u2192 direct PDF\")\n                        return (r2.url, doe_pages, \" | \".join(note_parts))\n                    r2.close()\n                except Exception:\n                    pass\n                # If not clearly PDF, still return as a candidate (agent will fetch &amp; parse)\n                note_parts.append(\n                    \"citation_doe_pages \u2192 PDF-like candidate (not confirmed by headers)\"\n                )\n                return (candidate, doe_pages, \" | \".join(note_parts))\n        except Exception as e:\n            note_parts.append(f\"citation_doe_pages failed: {e}\")\n\n    # # 3) Optional: DOI \u2192 Unpaywall OA\n    # if doi and unpaywall_email:\n    #     note_parts.append(\"Tried Unpaywall via DOI\")\n    #     pdf_from_ua = _resolve_pdf_via_unpaywall(doi, unpaywall_email)\n    #     if pdf_from_ua:\n    #         # May be direct PDF or landing; the caller will validate headers during download\n    #         note_parts.append(\"Unpaywall returned candidate\")\n    #         return (pdf_from_ua, None, \" | \".join(note_parts))\n\n    # 4) Give up\n    note_parts.append(\"No PDF found\")\n    return (None, None, \" | \".join(note_parts))\n</code></pre>"}]}